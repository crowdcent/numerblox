[
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Model",
    "section": "",
    "text": "The BaseModel is an abstract base class that handles directory logic and naming conventions. All models should inherit from BaseModel and be sure to implement the .predict method.\nIn general, models are loaded in from disk. However, if no model files are involved in your model you should pass an empty string (\"\") as the model_directory argument.\nNote that a new prediction column will have the column name prediction_{MODEL_NAME}.\n\nsource\n\n\n\n\n BaseModel (model_directory:str, model_name:str=None)\n\nSetup for model prediction on a Dataset.\n:param model_directory: Main directory from which to read in models.\n:param model_name: Name that will be used to create column names and for display purposes.\n\n\n\nA DirectoryModel assumes that you have a directory of models and you want to load + predict for all models with a certain file_suffix (for example, .joblib, .cbm or .lgb). This base class handles prediction logic for this situation.\nIf you are thinking of implementing your own model and your use case involves reading multiple models from a directory, then you should inherit from DirectoryModel and be sure to implement .load_models. You then don’t have to implement any prediction logic in the .predict method.\nWhen inheriting from DirectoryModel the only mandatory method implementation is for .load_models. It should instantiate all models and return them as a list.\n\nsource\n\n\n\n\n DirectoryModel (model_directory:str, file_suffix:str,\n                 model_name:str=None, feature_cols:list=None,\n                 combine_preds=True)\n\nBase class implementation where predictions are averaged out from a directory of models. Walks through every file with given file_suffix in a directory.\n:param model_directory: Main directory from which to read in models.\n:param file_suffix: File format to load (For example, .joblib, .pkl, .cbm or .lgb)\n:param model_name: Name that will be used to create column names and for display purposes.\n:param feature_cols: optional list of features to use for prediction. Selects all feature columns (i.e. column names with prefix ‘feature’) by default.\n:param combine_preds: Whether to average predictions along column axis. Only relevant for multi target models.\nConvenient when you want to predict the main target by averaging a multi-target model."
  },
  {
    "objectID": "model.html#base",
    "href": "model.html#base",
    "title": "Model",
    "section": "",
    "text": "The BaseModel is an abstract base class that handles directory logic and naming conventions. All models should inherit from BaseModel and be sure to implement the .predict method.\nIn general, models are loaded in from disk. However, if no model files are involved in your model you should pass an empty string (\"\") as the model_directory argument.\nNote that a new prediction column will have the column name prediction_{MODEL_NAME}.\n\nsource\n\n\n\n\n BaseModel (model_directory:str, model_name:str=None)\n\nSetup for model prediction on a Dataset.\n:param model_directory: Main directory from which to read in models.\n:param model_name: Name that will be used to create column names and for display purposes.\n\n\n\nA DirectoryModel assumes that you have a directory of models and you want to load + predict for all models with a certain file_suffix (for example, .joblib, .cbm or .lgb). This base class handles prediction logic for this situation.\nIf you are thinking of implementing your own model and your use case involves reading multiple models from a directory, then you should inherit from DirectoryModel and be sure to implement .load_models. You then don’t have to implement any prediction logic in the .predict method.\nWhen inheriting from DirectoryModel the only mandatory method implementation is for .load_models. It should instantiate all models and return them as a list.\n\nsource\n\n\n\n\n DirectoryModel (model_directory:str, file_suffix:str,\n                 model_name:str=None, feature_cols:list=None,\n                 combine_preds=True)\n\nBase class implementation where predictions are averaged out from a directory of models. Walks through every file with given file_suffix in a directory.\n:param model_directory: Main directory from which to read in models.\n:param file_suffix: File format to load (For example, .joblib, .pkl, .cbm or .lgb)\n:param model_name: Name that will be used to create column names and for display purposes.\n:param feature_cols: optional list of features to use for prediction. Selects all feature columns (i.e. column names with prefix ‘feature’) by default.\n:param combine_preds: Whether to average predictions along column axis. Only relevant for multi target models.\nConvenient when you want to predict the main target by averaging a multi-target model."
  },
  {
    "objectID": "model.html#single-model-formats",
    "href": "model.html#single-model-formats",
    "title": "Model",
    "section": "1. Single model formats",
    "text": "1. Single model formats\nImplementations for common Numerai model prediction situations.\n\n1.1. SingleModel\nIn many cases you just want to load a single model file and create predictions for that model. SingleModel supports this.\nThis class supports multiple model formats for easy use. All models should have a .predict method. Currently, .joblib, .cbm, .pkl, .pickle and .h5 (keras) format are supported.\nThings to keep in mind - This model will use all available features in the NumerFrame and use them for prediction by default. Define feature_cols in SingleModel or implement a FeatureSelectionPreProcessor as part of your ModelPipeline if you are using a subset of features. - If you have XGBoost models we recommend saving them as .joblib. - The added prediction column will have the column name prediction_{MODEL_NAME} if 1 target is predicted. For multiple targets the new column names will be prediction_{MODEL_NAME}_{i} for each target number i (starting with 0). - We welcome the Numerai community to extend SingleModel for more file formats. See the Contributing section in README.md for more information on contributing.\n\nsource\n\n\nSingleModel\n\n SingleModel (model_file_path:str, model_name:str=None,\n              combine_preds=False, autoencoder_mlp=False,\n              feature_cols:list=None)\n\nLoad single model from file and perform prediction logic.\n:param model_file_path: Full path to model file.\nSupport loading directly from GCS if path starts with ‘gs://’.\n:param model_name: Name that will be used to create column names and for display purposes.\n:param combine_preds: Whether to average predictions along column axis. Only relevant for multi target models. Convenient when you want to predict the main target by averaging a multi-target model.\n:param autoencoder_mlp: Whether your model is an autoencoder + MLP model. Will take the 3rd of tuple output in this case. Only relevant for NN models. More info on autoencoders: https://forum.numer.ai/t/autoencoder-and-multitask-mlp-on-new-dataset-from-kaggle-jane-street/4338\n:param feature_cols: optional list of features to use for prediction. Selects all feature columns (i.e. column names with prefix ‘feature’) by default.\n\ndataf = create_numerframe(\"test_assets/mini_numerai_version_2_data.parquet\")\ntest_paths = [\"test_assets/joblib_v2_example_model.joblib\"]\nfor path in test_paths:\n    model = SingleModel(path, model_name=\"test\")\n    print(model.predict(dataf).get_prediction_data.head(2))\n\n2023-09-20 18:31:08.629005: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-09-20 18:31:08.698026: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-09-20 18:31:08.699488: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-09-20 18:31:09.914738: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n                  prediction_test\nid                               \nn559bd06a8861222         0.506948\nn9d39dea58c9e3cf         0.492578\n\n\n\nmodel = SingleModel(test_paths[0], model_name=\"test\")\nmodel.suffix_to_model_mapping\n\n{'.joblib': &lt;function joblib.numpy_pickle.load(filename, mmap_mode=None)&gt;,\n '.cbm': &lt;bound method CatBoost.load_model of &lt;catboost.core.CatBoost object&gt;&gt;,\n '.pkl': &lt;function _pickle.load(file, *, fix_imports=True, encoding='ASCII', errors='strict', buffers=())&gt;,\n '.pickle': &lt;function _pickle.load(file, *, fix_imports=True, encoding='ASCII', errors='strict', buffers=())&gt;,\n '.h5': functools.partial(&lt;function load_model&gt;, compile=False)}\n\n\nYou can also load directly from GCS if the path is prefixed by gs://. For example, gs://my_bucket_name/my_model.joblib.\n\n# my_model_path = \"gs://my_bucket_name/my_model.joblib\"\n# model = SingleModel(my_model_path, model_name=\"test\")\n# df = create_numerframe(\"test_assets/train_int8_5_eras.parquet\")\n# preds = model.predict(df.get_feature_data).get_prediction_data\n\n\n\n1.2. WandbKerasModel\nThis model is for a specific case. Namely, if you are logging Keras model using Weights & Biases and want to download the best model for a specific run. WandbKerasModel wraps SingleModel and only adds additional logic for downloading models from Weights & Biases.\nTo authenticate your W&B account you are given several options: 1. Run wandb login in terminal and follow instructions (docs). 2. Configure global environment variable \"WANDB_API_KEY\". 3. Run wandb.init(project=PROJECT_NAME, entity=ENTITY_NAME) and pass API key from https://wandb.ai/authorize.\n\nsource\n\n\nWandbKerasModel\n\n WandbKerasModel (run_path:str, file_name:str='model-best.h5',\n                  combine_preds=False, autoencoder_mlp=False,\n                  replace=False, feature_cols:list=None)\n\nDownload best .h5 model from Weights & Biases (W&B) run in local directory and make predictions. More info on W&B: https://wandb.ai/site\n:param run_path: W&B path structured as entity/project/run_id. Can be copied from the Overview tab of a W&B run. For more info: https://docs.wandb.ai/ref/app/pages/run-page#overview-tab\n:param file_name: Name of .h5 file as saved in W&B run. ‘model-best.h5’ by default. File name can be found under files tab of W&B run.\n:param combine_preds: Whether to average predictions along column axis. Convenient when you want to predict the main target by averaging a multi-target model.\n:param autoencoder_mlp: Whether your model is an autoencoder + MLP model. Will take the 3rd of tuple output in this case. Only relevant for NN models.\nMore info on autoencoders: https://forum.numer.ai/t/autoencoder-and-multitask-mlp-on-new-dataset-from-kaggle-jane-street/4338\n:param replace: Replace any model files saved under the same file name with downloaded W&B run model. WARNING: Setting to True may overwrite models in your local environment.\n:param feature_cols: optional list of features to use for prediction. Selects all feature columns (i.e. column names with prefix ‘feature’) by default.\n\n\n1.3. CSVSub\nThis model is a wrapper for if you want to add predictions from external CSVs in a directory.\n\nsource\n\n\nExternalCSVs\n\n ExternalCSVs (data_directory:str='external_submissions')\n\nLoad external submissions and add to NumerFrame.\nAll csv files in this directory will be added to NumerFrame. Make sure all external predictions are prepared and ready for submission. i.e. IDs lining up and one column named ‘prediction’.\n:param data_directory: Directory path for retrieving external submission.\nFor testing, the external_submissions directory contains test_predictions.csv with values from feature target_thomas_20.\n\ndataf = create_numerframe(\"test_assets/mini_numerai_version_2_data.parquet\")\nexternal = ExternalCSVs(data_directory=\"test_assets/external_submissions\")\nnew_dataf = external.predict(dataf)\nnew_dataf.get_prediction_data.head(2)\n\n\n\n\n\n\n\n\n\n\n\nprediction_test_predictions.csv\n\n\nid\n\n\n\n\n\nn559bd06a8861222\n0.333333\n\n\nn9d39dea58c9e3cf\n0.500000\n\n\n\n\n\n\n\nIf no submissions are found in the given data_directory you should recieve a warning.\n\nExternalCSVs(data_directory=\"Some_nonexisting_directory_12354321\");\n\n⚠ WARNING: No csvs found in directory 'Some_nonexisting_directory_12354321'. ⚠\n\n\n\n\n\n1.4. NumerBay\nThis model is a wrapper for if you want to add predictions from NumerBay purchases.\nCurrently only Numerai Classic submissions are supported. Numerai Signals will be supported in a future version.\n\nsource\n\n\nNumerBayCSVs\n\n NumerBayCSVs (data_directory:str='numerbay_submissions',\n               numerbay_product_full_names:list=None,\n               numerbay_username:str=None, numerbay_password:str=None,\n               numerbay_key_path:str=None,\n               ticker_col:str='bloomberg_ticker')\n\nLoad NumerBay submissions and add to NumerFrame.\nMake sure to provide correct NumerBay credentials and that your purchases have been confirmed and artifacts are available for download.\n:param data_directory: Directory path for caching submission. Files not already present in the directory will be downloaded from NumerBay. :param numerbay_product_full_names: List of product full names (in the format of [category]-[product name]) to download from NumerBay. E.g. [‘numerai-predictions-numerbay’] :param numerbay_username: NumerBay username :param numerbay_password: NumerBay password :param numerbay_key_path: NumerBay encryption key json file path (exported from the profile page)\nExample usage\n\n# nb_model = NumerBayCSVs(data_directory='/app/notebooks/tmp',\n#                         numerbay_product_full_names=['numerai-predictions-someproduct'],\n#                         numerbay_username=\"myusername\",\n#                         numerbay_password=\"mypassword\",\n#                         numerbay_key_path=\"/app/notebooks/tmp/numerbay.json\")\n# preds = nb_model.predict(dataf)"
  },
  {
    "objectID": "model.html#loading-all-models-in-directory",
    "href": "model.html#loading-all-models-in-directory",
    "title": "Model",
    "section": "2. Loading all models in directory",
    "text": "2. Loading all models in directory\n\n2.1. Joblib directory\nMany models, like scikit-learn, can conveniently be saved as .joblib files. This class automatically loads all .joblib files in a given folder and generates (averaged out) predictions.\n\nsource\n\n\nJoblibModel\n\n JoblibModel (model_directory:str, model_name:str=None,\n              feature_cols:list=None)\n\nLoad and predict for arbitrary models in directory saved as .joblib.\nAll loaded models should have a .predict method and accept the features present in the data.\n:param model_directory: Main directory from which to read in models.\n:param model_name: Name that will be used to create column names and for display purposes.\n:param feature_cols: optional list of features to use for prediction. Selects all feature columns (i.e. column names with prefix ‘feature’) by default.\n\ndataf = create_numerframe(\"test_assets/mini_numerai_version_2_data.parquet\")\nmodel = JoblibModel(\"test_assets\", model_name=\"Joblib_LGB\")\npredictions = model.predict(dataf).get_prediction_data\nassert predictions['prediction_Joblib_LGB'].between(0, 1).all()\npredictions.head(2)\n\n\n\n\n✅ Finished step DirectoryModel. Output shape=(10, 1074). Time taken for step: 0:00:00.376394. ✅\n\n\n\n\n\n\n\n\n\n\nprediction_Joblib_LGB\n\n\nid\n\n\n\n\n\nn559bd06a8861222\n0.506948\n\n\nn9d39dea58c9e3cf\n0.492578\n\n\n\n\n\n\n\n\n\n2.2. Catboost directory (.cbm)\nThis model setup loads all CatBoost (.cbm) models present in a given directory and makes (averaged out) predictions.\n\nsource\n\n\nCatBoostModel\n\n CatBoostModel (model_directory:str, model_name:str=None,\n                feature_cols:list=None)\n\nLoad and predict with all .cbm models (CatBoostRegressor) in directory.\n:param model_directory: Main directory from which to read in models.\n:param model_name: Name that will be used to define column names and for display purposes.\n:param feature_cols: optional list of features to use for prediction. Selects all feature columns (i.e. column names with prefix ‘feature’) by default.\n\n# Example on NumerFrame\n# !pip install catboost\n# dataf = create_numerframe(\"test_assets/mini_numerai_version_2_data.parquet\")\n# model = CatBoostModel(\"test_assets\", model_name=\"CB\")\n# predictions = model.predict(dataf).get_prediction_data\n# assert predictions['prediction_CB'].between(0, 1).all()\n# predictions.head(2)\n\n\n\n2.3. LightGBM directory (.lgb)\nThis model setup loads all LightGBM (.lgb) models present in a given directory and makes (averaged out) predictions.\n\nsource\n\n\nLGBMModel\n\n LGBMModel (model_directory:str, model_name:str=None,\n            feature_cols:list=None)\n\nLoad and predict with all .lgb models (LightGBM) in directory.\n:param model_directory: Main directory from which to read in models.\n:param model_name: Name that will be used to define column names and for display purposes.\n:param feature_cols: optional list of features to use for prediction. Selects all feature columns (i.e. column names with prefix ‘feature’) by default.\n\n# Example on NumerFrame\n# !pip install lightgbm\n# dataf = create_numerframe(\"test_assets/mini_numerai_version_2_data.parquet\")\n# model = LGBMModel(\"test_assets\", model_name=\"LGB\")\n# predictions = model.predict(dataf).get_prediction_data\n# assert predictions['prediction_LGB'].between(0, 1).all()\n# predictions.head(2)"
  },
  {
    "objectID": "model.html#baseline-models",
    "href": "model.html#baseline-models",
    "title": "Model",
    "section": "3. Baseline models",
    "text": "3. Baseline models\nSetting a baseline is always an important step for data science problems. This section introduces models that should only be used a baselines.\n\n3.1. ConstantModel\nThis model simply outputs a constant of your choice. Convenient for setting classification baselines.\n\nsource\n\n\nConstantModel\n\n ConstantModel (constant:float=0.5, model_name:str=None)\n\nWARNING: Only use this Model for testing purposes.\nCreate constant prediction.\n:param constant: Value for constant prediction.\n:param model_name: Name that will be used to create column names and for display purposes.\n\nconstant = 0.85\ndataf = create_numerframe(\"test_assets/mini_numerai_version_2_data.parquet\")\nconstant_model = ConstantModel(constant=constant)\npredictions = constant_model.predict(dataf).get_prediction_data\nassert (predictions.to_numpy() == constant).all()\npredictions.head(2)\n\n\n\n\n\n\n\n\nprediction_constant_0.85\n\n\nid\n\n\n\n\n\nn559bd06a8861222\n0.85\n\n\nn9d39dea58c9e3cf\n0.85\n\n\n\n\n\n\n\n\n\n3.2. RandomModel\nThis model returns uniformly distributed predictions in range \\([0...1)\\). Solid naive baseline for regression models.\n\nsource\n\n\nRandomModel\n\n RandomModel (model_name:str=None)\n\nWARNING: Only use this Model for testing purposes.\nCreate uniformly distributed predictions.\n:param model_name: Name that will be used to create column names and for display purposes.\n\ndataf = create_numerframe(\"test_assets/mini_numerai_version_2_data.parquet\")\nrandom_model = RandomModel()\npredictions = random_model.predict(dataf).get_prediction_data\nassert predictions['prediction_random'].between(0, 1).all()\npredictions.head(2)\n\n\n\n\n\n\n\n\nprediction_random\n\n\nid\n\n\n\n\n\nn559bd06a8861222\n0.478581\n\n\nn9d39dea58c9e3cf\n0.600459\n\n\n\n\n\n\n\n\n\n3.3. Example (validation) predictions\nThis Model performs downloading and adding of example predictions for Numerai Classic. Convenient when you are constructing a ModelPipeline and want to include example predictions.\n\nsource\n\n\nExamplePredictionsModel\n\n ExamplePredictionsModel\n                          (file_name:str='v4.2/validation_example_preds.pa\n                          rquet',\n                          data_directory:str='example_predictions_model',\n                          round_num:int=None)\n\nLoad example predictions and add to NumerFrame.\n:param file_name: File to download from NumerAPI. By default this is example predictions for v4.2 data. Other example of example predictions in previous version: - v4.1. -&gt; “v4.1/validation_example_preds.parquet” - v4. -&gt; “v4/validation_example_preds.parquet” ‘example_validation_predictions.parquet’ by default.\n:param data_directory: Directory path to download example predictions to or directory where example data already exists.\n:param round_num: Optional round number. Downloads most recent round by default.\n\n# # Download validation data\n# downloader = NumeraiClassicDownloader(\"example_predictions_model\")\n# val_file = \"v4.2/validation_int8.parquet\"\n# val_save_path = f\"{str(downloader.dir)}/{val_file}\"\n# downloader.download_single_dataset(filename=val_file,\n#                                    dest_path=val_save_path)\n\n# # Load validation data and add example predictions\n# dataf = create_numerframe(val_save_path)\n# example_model = ExamplePredictionsModel()\n# predictions = example_model.predict(dataf).get_prediction_data\n# assert predictions['prediction_example'].between(0, 1).all()\n# predictions.head(2)"
  },
  {
    "objectID": "model.html#custom-model",
    "href": "model.html#custom-model",
    "title": "Model",
    "section": "4. Custom Model",
    "text": "4. Custom Model\nThere are two different ways to implement new models. Both have their own conveniences and use cases.\n4.1. Inherit from BaseModel (custom prediction logic).\n4.2. Inherit from DirectoryModel (make predictions for all models in directory with given file suffix. Prediction logic will already be implemented. Only implement model loading logic).\n4.1. (From BaseModel) works well when you have no or only a single file that you use for generating predictions.\nExamples: 1. Loading a model is not relevant or your model is already loaded in memory. 2. You would like predictions for one model loaded from disk. 3. The object you are loading already aggregates multiple models and transformation steps (such as scikit-learn FeatureUnion).\n4.2. (From DirectoryModel) is convenient when you have a lot of similar models in a directory and want to generate predictions for all of them.\nExamples: 1. You have multiple similar models saved through a cross validation process. 2. You have a bagging strategy where a lot of models trained on slightly different data or with different initializations should are averaged.\n\n4.1. From BaseModel\nArbitrary models can be instantiated and used for prediction generation by inheriting from BaseModel. Arbitrary logic (model loading, prediction, etc.) can be defined in .predict as long as the method takes a NumerFrame as input and outputs a NumerFrame.\nFor clear console output we recommend adding the @display_processor_info decorator to the .predict method.\nIf your model does not involve reading files from disk specify model_directory=\"\".\n\nsource\n\n\nAwesomeModel\n\n AwesomeModel (model_directory:str, model_name:str=None,\n               feature_cols:list=None)\n\nTEMPLATE - Predict with arbitrary prediction logic and model formats.\n:param model_directory: Main directory from which to read in models.\n:param model_name: Name that will be used to define column names and for display purposes.\n:param feature_cols: optional list of features to use for prediction. Selects all feature columns (i.e. column names with prefix ‘feature’) by default.\n\n\n4.2. From DirectoryModel\nYou may want to implement a setup similar to JoblibModel and CatBoostModel. Namely, load in all models of a certain type from a directory, predict for all and take the average. If this is your use case, inherit from DirectoryModel and be sure to implement the .load_models method.\nFor a DirectoryModel you should specify a file_suffix (like .joblib or .cbm) which will be used to store all available models in self.model_paths.\nThe .predict method will in this case already be implemented, but can be overridden if the prediction logic is more complex. For example, if you want to apply weighted averaging or a geometric mean for models within a given directory.\n\nsource\n\n\nAwesomeDirectoryModel\n\n AwesomeDirectoryModel (model_directory:str, model_name:str=None,\n                        feature_cols:list=None)\n\nTEMPLATE - Load in all models of arbitrary file format and predict for all.\n:param model_directory: Main directory from which to read in models.\n:param model_name: Name that will be used to define column names and for display purposes.\n:param feature_cols: optional list of features to use for prediction. Selects all feature columns (i.e. column names with prefix ‘feature’) by default."
  },
  {
    "objectID": "download.html",
    "href": "download.html",
    "title": "Download",
    "section": "",
    "text": "There are common methods needed for Downloaders and Submittors. BaseIO implements this functionality and allows us to make abstract base classes. Namely, BaseDownloader and BaseSubmitter (implemented in submission section).\n\nsource\n\n\n\n BaseIO (directory_path:str)\n\nBasic functionality for IO (downloading and uploading).\n:param directory_path: Base folder for IO. Will be created if it does not exist."
  },
  {
    "objectID": "download.html#baseio",
    "href": "download.html#baseio",
    "title": "Download",
    "section": "",
    "text": "There are common methods needed for Downloaders and Submittors. BaseIO implements this functionality and allows us to make abstract base classes. Namely, BaseDownloader and BaseSubmitter (implemented in submission section).\n\nsource\n\n\n\n BaseIO (directory_path:str)\n\nBasic functionality for IO (downloading and uploading).\n:param directory_path: Base folder for IO. Will be created if it does not exist."
  },
  {
    "objectID": "download.html#basedownloader",
    "href": "download.html#basedownloader",
    "title": "Download",
    "section": "1. BaseDownloader",
    "text": "1. BaseDownloader\nBaseDownloader is an object which implements logic common to all downloaders.\nTo implement a new Downloader, you should inherit from BaseDownloader and be sure to implement at least methods for .download_training_data and .download_inference_data.\n\nsource\n\nBaseDownloader\n\n BaseDownloader (directory_path:str)\n\nAbstract base class for downloaders.\n:param directory_path: Base folder to download files to."
  },
  {
    "objectID": "download.html#numerai-classic",
    "href": "download.html#numerai-classic",
    "title": "Download",
    "section": "2. Numerai Classic",
    "text": "2. Numerai Classic\n\nsource\n\nNumeraiClassicDownloader\n\n NumeraiClassicDownloader (directory_path:str, *args, **kwargs)\n\nWARNING: Versions 1 and 2 (legacy data) are deprecated. Only supporting version 3+.\nDownloading from NumerAPI for Numerai Classic data.\n:param directory_path: Base folder to download files to.\nAll *args, **kwargs will be passed to NumerAPI initialization.\n\ntest_dir_classic = \"test_numclassic_general\"\nnumer_classic_downloader = NumeraiClassicDownloader(test_dir_classic)\n\n# Test building class\nassert isinstance(numer_classic_downloader.dir, PosixPath)\nassert numer_classic_downloader.dir.is_dir()\n\n# Test is_empty\n(numer_classic_downloader.dir / \"test.txt\").write_text(\"test\")\nrich_print(f\"Directory contents:\\n{numer_classic_downloader.get_all_files}\")\nassert not numer_classic_downloader.is_empty\n\n# Downloading example data\nnumer_classic_downloader.download_example_data(\"test/\", version=\"4.2\")\n\n# Features\nfeature_stats_test = numer_classic_downloader.get_classic_features()\nassert isinstance(feature_stats_test, dict)\nassert len(feature_stats_test[\"feature_sets\"][\"medium\"]) == 583\n\n# Remove contents\nnumer_classic_downloader.remove_base_directory()\nassert not os.path.exists(test_dir_classic)\n\nNo existing directory found at 'test_numclassic_general'. Creating directory...\n\n\n\nDirectory contents:\n[Path('test_numclassic_general/test.txt')]\n\n\n\n📁 Downloading 'v4.2/live_example_preds.parquet' 📁\n\n\n\n2023-09-04 12:11:02,108 INFO numerapi.utils: starting download\ntest_numclassic_general/test/live_example_preds.parquet: 132kB [00:00, 701kB/s]                            \n2023-09-04 12:11:02,982 INFO numerapi.utils: starting download\ntest_numclassic_general/test/validation_example_preds.parquet: 60.6MB [00:03, 19.1MB/s]                            \n2023-09-04 12:11:06,717 INFO numerapi.utils: starting download\ntest_numclassic_general/features.json: 1.03MB [00:00, 3.32MB/s]                           \n\n\n📁 Downloading 'v4.2/validation_example_preds.parquet' 📁\n\n\n\n📁 Downloading 'v4.2/features.json' 📁\n\n\n\n⚠ Deleting directory for 'NumeraiClassicDownloader' ⚠\nPath: '/home/clepelaars/numerblox/nbs/test_numclassic_general'\n\n\n\n\n\n2.1. Example usage\nThis section will explain how to quickly get started with NumeraiClassicDownloader.\nThe more advanced use case of working with GCS (Google Cloud Storage) is discussed in edu_nbs/google_cloud_storage.ipynb.\n\n2.1.1. Training data\nTraining + validation data for Numerai Classic can be downloaded with effectively 2 lines of code. Feature stats and overview can be downloaded with .get_classic_features().\n\n# Initialization\ntrain_base_directory = \"test_numclassic_train\"\nnumer_classic_downloader = NumeraiClassicDownloader(train_base_directory)\n\n# Uncomment line below to download training and validation data\n# numer_classic_downloader.download_training_data(\"train_val\", int8=True)\n\n# 4.2 training data download should fail with int8=False\ntry:\n    numer_classic_downloader.download_training_data(\"train_val\", version=\"4.2\", int8=False)\nexcept NotImplementedError:\n    pass\nelse:\n    assert False, f\"Downloading 4.2 data with int8=False should raise an error.\"\n\n# Get feature overview (dict) for 4.2 data\nnumer_classic_downloader.get_classic_features()\n\n# Remove contents (To clean up environment)\nnumer_classic_downloader.remove_base_directory()\n\nNo existing directory found at 'test_numclassic_train'. Creating directory...\n\n\n\n📁 Downloading 'v4.2/features.json' 📁\n\n\n\n2023-09-04 12:11:07,784 INFO numerapi.utils: starting download\ntest_numclassic_train/features.json: 1.03MB [00:00, 3.17MB/s]                           \n\n\n⚠ Deleting directory for 'NumeraiClassicDownloader' ⚠\nPath: '/home/clepelaars/numerblox/nbs/test_numclassic_train'\n\n\n\nFor the training example the directory structure will be:\n\n\n📁 test_numclassic_train (base_directory)\n┣━━ 📄 features.json\n┗━━ 📁 train_val\n    ┣━━ 📄 numerai_training_data.parquet\n    ┗━━ 📄 numerai_validation_data.parquet\n\n\n\n\n\n2.1.2. Inference data\nInference data for the most recent round of Numerai Classic can be downloaded with effectively 2 lines of code. It can also easily be deleted after you are done with inference by calling .remove_base_directory.\n\n# Initialization\ninference_base_dir = \"test_numclassic_inference\"\nnumer_classic_downloader = NumeraiClassicDownloader(directory_path=inference_base_dir)\n\n# Download tournament (inference) data\nnumer_classic_downloader.download_inference_data(\"inference\", version=\"4.2\", int8=True)\n\n# Download meta model predictions\nnumer_classic_downloader.download_meta_model_preds(\"inference\")\n\n# Remove folder when done with inference\nnumer_classic_downloader.remove_base_directory()\n\nNo existing directory found at 'test_numclassic_inference'. Creating directory...\n\n\n\n📁 Downloading 'v4.2/live_int8.parquet' 📁\n\n\n\n2023-09-04 12:11:09,182 INFO numerapi.utils: starting download\ntest_numclassic_inference/inference/live_int8.parquet: 5.96MB [00:00, 12.3MB/s]                            \n2023-09-04 12:11:10,486 INFO numerapi.utils: starting download\ntest_numclassic_inference/inference/meta_model.parquet: 22.6MB [00:01, 18.2MB/s]                            \n\n\n📁 Downloading 'v4.2/meta_model.parquet' 📁\n\n\n\n⚠ Deleting directory for 'NumeraiClassicDownloader' ⚠\nPath: '/home/clepelaars/numerblox/nbs/test_numclassic_inference'\n\n\n\nFor the inference example the directory structure will be:\n\n\n📁 test_numclassic_inference (base_directory)\n┗━━ 📁 inference\n    ┣━━ 📄 meta_model.parquet\n    ┗━━ 📄 numerai_tournament_data.parquet"
  },
  {
    "objectID": "download.html#kaggledownloader-numerai-signals",
    "href": "download.html#kaggledownloader-numerai-signals",
    "title": "Download",
    "section": "3. KaggleDownloader (Numerai Signals)",
    "text": "3. KaggleDownloader (Numerai Signals)\nThe Numerai community maintains some excellent datasets on Kaggle for Numerai Signals.\nFor example, Katsu1110 maintains a dataset with yfinance price data on Kaggle that is updated daily. KaggleDownloader allows you to easily pull data through the Kaggle API. We will be using this dataset in an example below.\nIn this case, download_inference_data and download_training_data have the same functionality as we can’t make the distinction beforehand for an arbitrary dataset on Kaggle.\n\nsource\n\nKaggleDownloader\n\n KaggleDownloader (directory_path:str)\n\nDownload awesome financial data from Kaggle.\nFor authentication, make sure you have a directory called .kaggle in your home directory with therein a kaggle.json file. kaggle.json should have the following structure:\n{\"username\": USERNAME, \"key\": KAGGLE_API_KEY}\nMore info on authentication: github.com/Kaggle/kaggle-api#api-credentials\nMore info on the Kaggle Python API: kaggle.com/donkeys/kaggle-python-api\n:param directory_path: Base folder to download files to.\nThe link to Katsu1110’s yfinance price dataset is https://www.kaggle.com/code1110/yfinance-stock-price-data-for-numerai-signals. In .download_training_data we define the slug after kaggle.com (code1110/yfinance-stock-price-data-for-numerai-signals) as an argument. The full Kaggle dataset is downloaded and unzipped.\n\nhome_directory = \"test_kaggle_downloader\"\nkd = KaggleDownloader(home_directory)\nkd.download_training_data(\"code1110/yfinance-stock-price-data-for-numerai-signals\")\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/clepelaars/.kaggle/kaggle.json'\n\n\nNo existing directory found at 'test_kaggle_downloader'. Creating directory...\n\n\n\nThis Kaggle dataset contains one file called \"full_data.parquet\".\n\nlist(kd.dir.iterdir())\n\n[Path('test_kaggle_downloader/full_data.parquet')]\n\n\n\ndf = pd.read_parquet(f\"{home_directory}/full_data.parquet\")\ndf.head(2)\n\n\n\n\n\n\n\n\nticker\ndate\nclose\nraw_close\nhigh\nlow\nopen\nvolume\n\n\n\n\n0\n000060 KS\n20020103\n534.924377\n1248.795166\n1248.795166\n1248.795166\n1248.795166\n0.0\n\n\n1\n000060 KS\n20020104\n566.944519\n1323.546997\n1363.121460\n1213.617798\n1275.178223\n3937763.0\n\n\n\n\n\n\n\nFolder can be cleaned up when done with inference.\n\nkd.remove_base_directory()\n\n⚠ Deleting directory for 'KaggleDownloader' ⚠\nPath: '/home/clepelaars/numerblox/nbs/test_kaggle_downloader'"
  },
  {
    "objectID": "download.html#eoddownloader",
    "href": "download.html#eoddownloader",
    "title": "Download",
    "section": "4. EODDownloader",
    "text": "4. EODDownloader\nEOD Historical data is an affordable Financial data APIs that offers a large range of global stock tickers. Very convenient for Numerai Signals modeling. We will use a Python API build on top of EOD Historical data to download stock ticker data for training and inference.\n\nsource\n\nEODDownloader\n\n EODDownloader (directory_path:str, key:str, tickers:list,\n                frequency:str='d')\n\nDownload data from EOD historical data.\nMore info: https://eodhistoricaldata.com/\n:param directory_path: Base folder to download files to.\n:param key: Valid EOD client key.\n:param tickers: List of valid EOD tickers (Bloomberg ticker format).\n:param frequency: Choose from [d, w, m].\nDaily data by default.\n\nos.listdir(\"test_assets\")\n\n['test_credentials.json',\n 'mini_numerai_version_2_data.parquet',\n 'lgb_v2_example_model.lgb',\n 'keys.json',\n 'external_submissions',\n 'joblib_v2_example_model.joblib',\n 'eodhd-map.csv']\n\n\n\nkey = BaseDownloader._load_json(\"test_assets/keys.json\")['eod_key'] # YOUR_EOD_KEY_HERE\neodd = EODDownloader(directory_path=\"eod_test\", key=key, tickers=['AAPL.US', 'MSFT.US', 'COIN.US', 'NOT_A_TICKER'])\n\nIf no starting date is passed in download_training_data this downloader will take the earliest date available. That is why the starting date in the filename is the 1st Unix timestamp (January 1st 1970).\n\neodd.download_inference_data()\neodd.download_training_data()\n\n\n\n\n⚠ WARNING: Date pull failed on ticker: 'NOT_A_TICKER'. ⚠ Exception: 404 Client Error: Not Found for url: \nhttps://eodhistoricaldata.com/api/eod/NOT_A_TICKER?period=d&to=2023-09-04&fmt=json&api_token=621661e8653533.2141337\n4&from=2022-09-04\n\n\n\n\n\n\n⚠ WARNING: Date pull failed on ticker: 'NOT_A_TICKER'. ⚠ Exception: 404 Client Error: Not Found for url: \nhttps://eodhistoricaldata.com/api/eod/NOT_A_TICKER?period=d&to=2023-09-04&fmt=json&api_token=621661e8653533.2141337\n4&from=1970-01-01\n\n\n\n\ntoday = dt.now().strftime(\"%Y%m%d\")\ndf = pd.read_parquet(f\"eod_test/eod_19700101_{today}.parquet\")\ndf.head(2)\n\n\n\n\n\n\n\n\nopen\nhigh\nlow\nclose\nadjusted_close\nvolume\nticker\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n2021-04-09\n381.0\n381.0\n381.0\n381.0\n250.0\n0\nCOIN.US\n\n\n2021-04-12\n381.0\n381.0\n381.0\n381.0\n250.0\n0\nCOIN.US\n\n\n\n\n\n\n\nLive data with a custom starting date can be retrieved as a NumerFrame directly with get_live_data. The starting date can be either in datetime, pd.Timestamp or string format.\n\nlive_dataf = eodd.get_live_data(start=pd.Timestamp(year=2021, month=1, day=1))\nlive_dataf.head(2)\n\n\n\n\n⚠ WARNING: Date pull failed on ticker: 'NOT_A_TICKER'. ⚠ Exception: 404 Client Error: Not Found for url: \nhttps://eodhistoricaldata.com/api/eod/NOT_A_TICKER?period=d&to=2023-09-04&fmt=json&api_token=621661e8653533.2141337\n4&from=2021-01-01+00%3A00%3A00\n\n\n\n\n\n\n\n\n\n\nopen\nhigh\nlow\nclose\nadjusted_close\nvolume\nticker\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n2021-01-04\n133.52\n133.61\n126.76\n129.41\n127.3317\n143301900\nAAPL.US\n\n\n2021-01-05\n128.89\n131.74\n128.43\n131.01\n128.9060\n97664900\nAAPL.US\n\n\n\n\n\n\n\n\nlive_dataf[live_dataf['ticker'] == \"AAPL.US\"]['close'].plot(figsize=(15, 6), title=\"AAPL from January 2021\");\n\n\n\n\n\neodd.remove_base_directory()\n\n⚠ Deleting directory for 'EODDownloader' ⚠\nPath: '/home/clepelaars/numerblox/nbs/eod_test'"
  },
  {
    "objectID": "download.html#custom-downloader",
    "href": "download.html#custom-downloader",
    "title": "Download",
    "section": "5. Custom Downloader",
    "text": "5. Custom Downloader\nWe invite the Numerai Community to implement new downloaders for this project using interesting APIs.\nThese are especially important for creating innovative Numerai Signals models.\nA new Downloader can be created by inheriting from BaseDownloader. You should implement methods for .download_inference_data and .download_training_data so every downloader has a common interface. Below you will find a template for a new downloader.\n\nsource\n\nAwesomeCustomDownloader\n\n AwesomeCustomDownloader (directory_path:str)\n\nTEMPLATE - Download awesome financial data from who knows where.\n:param directory_path: Base folder to download files to."
  },
  {
    "objectID": "postprocessing.html",
    "href": "postprocessing.html",
    "title": "Postprocessing",
    "section": "",
    "text": "The postprocessing procedure is similar to preprocessing. Preprocessors manipulate and/or add feature columns, while postprocessors manipulate and/or add prediction columns.\nEvery postprocessor should inherit from BasePostProcessor. A postprocessor should take a NumerFrame as input and output a NumerFrame. One or more new prediction column(s) with prefix prediction are added or manipulated in a postprocessor."
  },
  {
    "objectID": "postprocessing.html#overview",
    "href": "postprocessing.html#overview",
    "title": "Postprocessing",
    "section": "",
    "text": "The postprocessing procedure is similar to preprocessing. Preprocessors manipulate and/or add feature columns, while postprocessors manipulate and/or add prediction columns.\nEvery postprocessor should inherit from BasePostProcessor. A postprocessor should take a NumerFrame as input and output a NumerFrame. One or more new prediction column(s) with prefix prediction are added or manipulated in a postprocessor."
  },
  {
    "objectID": "postprocessing.html#basepostprocessor",
    "href": "postprocessing.html#basepostprocessor",
    "title": "Postprocessing",
    "section": "0. BasePostProcessor",
    "text": "0. BasePostProcessor\nSome characteristics are particular to Postprocessors, but not suitable to put in the Processor base class. This functionality is implemented in BasePostProcessor.\n\nsource\n\nBasePostProcessor\n\n BasePostProcessor (final_col_name:str)\n\nBase class for postprocessing objects.\nPostprocessors manipulate or introduce new prediction columns in a NumerFrame."
  },
  {
    "objectID": "postprocessing.html#common-postprocessing-steps",
    "href": "postprocessing.html#common-postprocessing-steps",
    "title": "Postprocessing",
    "section": "1. Common postprocessing steps",
    "text": "1. Common postprocessing steps\nWe invite the Numerai community to develop new postprocessors so that everyone can benefit from new insights and research. This section implements commonly used postprocessing for Numerai."
  },
  {
    "objectID": "postprocessing.html#tournament-agnostic",
    "href": "postprocessing.html#tournament-agnostic",
    "title": "Postprocessing",
    "section": "1.0. Tournament agnostic",
    "text": "1.0. Tournament agnostic\nPostprocessing that works for both Numerai Classic and Numerai Signals.\n\n1.0.1. Standardization\nStandardizing is an essential step in order to reliably combine Numerai predictions. It is a default postprocessor for ModelPipeline.\n\nsource\n\n\nStandardizer\n\n Standardizer (cols:list=None)\n\nUniform standardization of prediction columns. All values should only contain values in the range [0…1].\n:param cols: All prediction columns that should be standardized. Use all prediction columns by default.\n\n# Random DataFrame\ntest_features = [f\"prediction_{l}\" for l in \"ABCDE\"]\ndf = pd.DataFrame(np.random.uniform(size=(100, 5)), columns=test_features)\ndf[\"target\"] = np.random.normal(size=100)\ndf[\"date\"] = [0, 1, 2, 3] * 25\ntest_dataf = NumerFrame(df)\n\n\nstd = Standardizer()\nstd.transform(test_dataf).get_prediction_data.head(2)\n\n✅ Finished step Standardizer. Output shape=(100, 7). Time taken for step: 0:00:00.004771. ✅\n\n\n\n\n\n\n\n\n\n\nprediction_A\nprediction_B\nprediction_C\nprediction_D\nprediction_E\n\n\n\n\n0\n0.12\n0.40\n0.04\n0.72\n0.04\n\n\n1\n0.92\n0.48\n0.96\n0.92\n0.72\n\n\n\n\n\n\n\n\n\n1.0.2. Ensembling\nMultiple prediction results can be ensembled in multiple ways. We provide the most common use cases here.\n\n1.0.2.1. Simple Mean\n\nsource\n\n\n\nMeanEnsembler\n\n MeanEnsembler (final_col_name:str, cols:list=None,\n                standardize:bool=False)\n\nTake simple mean of multiple cols and store in new col.\n:param final_col_name: Name of new averaged column. final_col_name should start with “prediction”.\n:param cols: Column names to average.\n:param standardize: Whether to standardize by era before averaging. Highly recommended as columns that are averaged may have different distributions.\n\n1.0.2.2. Donate’s formula\nThis method for weighted averaging is mostly suitable if you have multiple models trained on a time series cross validation scheme. The first models will be trained on less data so we want to give them a lower weighting compared to the later models.\nSource: Yirun Zhang in his winning solution for the Jane Street 2021 Kaggle competition. Based on a paper by Donate et al.\n\nsource\n\n\n\nDonateWeightedEnsembler\n\n DonateWeightedEnsembler (final_col_name:str, cols:list=None)\n\nWeighted average as per Donate et al.’s formula Paper Link: https://doi.org/10.1016/j.neucom.2012.02.053 Code source: https://www.kaggle.com/gogo827jz/jane-street-supervised-autoencoder-mlp\nWeightings for 5 folds: [0.0625, 0.0625, 0.125, 0.25, 0.5]\n:param cols: Prediction columns to ensemble. Uses all prediction columns by default.\n:param final_col_name: New column name for ensembled values.\n\n# Random DataFrame\n#| include: false\ntest_features = [f\"prediction_{l}\" for l in \"ABCDE\"]\ndf = pd.DataFrame(np.random.uniform(size=(100, 5)), columns=test_features)\ndf[\"target\"] = np.random.normal(size=100)\ndf[\"era\"] = range(100)\ntest_dataf = NumerFrame(df)\n\nFor 5 folds, the weightings are [0.0625, 0.0625, 0.125, 0.25, 0.5].\n\nw_5_fold = [0.0625, 0.0625, 0.125, 0.25, 0.5]\ndonate = DonateWeightedEnsembler(\n    cols=test_dataf.prediction_cols, final_col_name=\"prediction\"\n)\nensembled = donate(test_dataf).get_prediction_data\nassert ensembled[\"prediction\"][0] == np.sum(\n    [w * elem for w, elem in zip(w_5_fold, ensembled[test_features].iloc[0])]\n)\nensembled.head(2)\n\n🍲 Ensembled '['prediction_A', 'prediction_B', 'prediction_C', 'prediction_D', 'prediction_E']' with \nDonateWeightedEnsembler and saved in 'prediction' 🍲\n\n\n\n✅ Finished step DonateWeightedEnsembler. Output shape=(100, 8). Time taken for step: 0:00:00.004307. ✅\n\n\n\n\n\n\n\n\n\n\nprediction_A\nprediction_B\nprediction_C\nprediction_D\nprediction_E\nprediction\n\n\n\n\n0\n0.809680\n0.821740\n0.673158\n0.130708\n0.946340\n0.691955\n\n\n1\n0.665325\n0.402088\n0.454365\n0.820944\n0.091936\n0.374713\n\n\n\n\n\n\n\n\n1.0.2.3. Geometric Mean\nTake the mean of multiple prediction columns using the product of values.\nMore info on Geometric mean: - Wikipedia - Investopedia\n\nsource\n\n\n\nGeometricMeanEnsembler\n\n GeometricMeanEnsembler (final_col_name:str, cols:list=None)\n\nCalculate the weighted Geometric mean.\n:param cols: Prediction columns to ensemble. Uses all prediction columns by default.\n:param final_col_name: New column name for ensembled values.\n\ngeo_mean = GeometricMeanEnsembler(final_col_name=\"prediction_geo\")\nensembled = geo_mean(test_dataf).get_prediction_data\nensembled.head(2)\n\n🍲 Ensembled '['prediction_A', 'prediction_B', 'prediction_C', 'prediction_D', 'prediction_E']' with \nGeometricMeanEnsembler and saved in 'prediction_geo' 🍲\n\n\n\n✅ Finished step GeometricMeanEnsembler. Output shape=(100, 9). Time taken for step: 0:00:00.031692. ✅\n\n\n\n\n\n\n\n\n\n\nprediction_A\nprediction_B\nprediction_C\nprediction_D\nprediction_E\nprediction\nprediction_geo\n\n\n\n\n0\n0.809680\n0.821740\n0.673158\n0.130708\n0.946340\n0.691955\n0.560664\n\n\n1\n0.665325\n0.402088\n0.454365\n0.820944\n0.091936\n0.374713\n0.391302\n\n\n\n\n\n\n\n\n\n1.0.3. Neutralization and penalization\n\n1.0.3.1. Feature Neutralization\nClassic feature neutralization (subtracting linear model from scores).\nNew column name for neutralized values will be {pred_name}_neutralized_{PROPORTION}. pred_name should start with 'prediction'.\nOptionally, you can run feature neutralization on the GPU using cupy by setting cuda=True. Make sure you have cupy installed with the correct CUDA Toolkit version. More information: docs.cupy.dev/en/stable/install.html\nDetailed explanation of Feature Neutralization by Katsu1110\n\nsource\n\n\n\nFeatureNeutralizer\n\n FeatureNeutralizer (feature_names:list=None, pred_name:str='prediction',\n                     proportion:float=0.5, suffix:str=None, cuda=False)\n\nClassic feature neutralization by subtracting linear model.\n:param feature_names: List of column names to neutralize against. Uses all feature columns by default.\n:param pred_name: Prediction column to neutralize.\n:param proportion: Number in range [0…1] indicating how much to neutralize.\n:param suffix: Optional suffix that is added to new column name.\n:param cuda: Do neutralization on the GPU\nMake sure you have CuPy installed when setting cuda to True.\nInstallation docs: docs.cupy.dev/en/stable/install.html\n\ntestv1_dataf = create_numerframe(\"test_assets/mini_numerai_version_2_data.parquet\")\ntestv1_dataf.loc[:, \"prediction\"] = np.random.uniform(size=len(testv1_dataf))\n\n\ntestv1_dataf.head(2)\n\n\n\n\n\n\n\n\nid\nera\ndata_type\nfeature_intelligence1\nfeature_intelligence2\nfeature_intelligence3\nfeature_intelligence4\nfeature_intelligence5\nfeature_intelligence6\nfeature_intelligence7\n...\nfeature_wisdom39\nfeature_wisdom40\nfeature_wisdom41\nfeature_wisdom42\nfeature_wisdom43\nfeature_wisdom44\nfeature_wisdom45\nfeature_wisdom46\ntarget\nprediction\n\n\n\n\n0\nn000315175b67977\nera1\ntrain\n0.0\n0.5\n0.25\n0.00\n0.5\n0.25\n0.25\n...\n1.0\n0.75\n0.5\n0.75\n0.50\n1.0\n0.50\n0.75\n0.50\n0.612091\n\n\n1\nn0014af834a96cdd\nera1\ntrain\n0.0\n0.0\n0.00\n0.25\n0.5\n0.00\n0.00\n...\n1.0\n0.00\n0.0\n0.75\n0.25\n0.0\n0.25\n1.00\n0.25\n0.636917\n\n\n\n\n2 rows × 315 columns\n\n\n\n\nft = FeatureNeutralizer(\n    feature_names=test_dataf.feature_cols, pred_name=\"prediction\", proportion=0.8\n)\nnew_dataf = ft.transform(test_dataf)\n\n🤖 Neutralized 'prediction' with proportion '0.8' 🤖\n\n\n\nNew neutralized column = 'prediction_neutralized_0.8'.\n\n\n\n✅ Finished step FeatureNeutralizer. Output shape=(100, 10). Time taken for step: 0:00:00.455949. ✅\n\n\n\n\nassert \"prediction_neutralized_0.8\" in new_dataf.prediction_cols\nassert 0.0 in new_dataf.get_prediction_data[\"prediction_neutralized_0.8\"]\nassert 1.0 in new_dataf.get_prediction_data[\"prediction_neutralized_0.8\"]\n\nGenerated columns and data can be easily retrieved for the NumerFrame.\n\nnew_dataf.prediction_cols\n\n['prediction_A',\n 'prediction_B',\n 'prediction_C',\n 'prediction_D',\n 'prediction_E',\n 'prediction',\n 'prediction_geo',\n 'prediction_neutralized_0.8']\n\n\n\nnew_dataf.get_prediction_data.head(3)\n\n\n\n\n\n\n\n\nprediction_A\nprediction_B\nprediction_C\nprediction_D\nprediction_E\nprediction\nprediction_geo\nprediction_neutralized_0.8\n\n\n\n\n0\n0.809680\n0.821740\n0.673158\n0.130708\n0.946340\n0.691955\n0.560664\nNaN\n\n\n1\n0.665325\n0.402088\n0.454365\n0.820944\n0.091936\n0.374713\n0.391302\nNaN\n\n\n2\n0.404700\n0.519101\n0.104269\n0.781825\n0.263947\n0.398201\n0.339651\nNaN\n\n\n\n\n\n\n\n\n1.0.3.2. Feature Penalization\n\nsource\n\n\n\nFeaturePenalizer\n\n FeaturePenalizer (max_exposure:float, feature_names:list=None,\n                   pred_name:str='prediction', suffix:str=None)\n\nFeature penalization with TensorFlow.\nSource (by jrb): https://github.com/jonrtaylor/twitch/blob/master/FE_Clipping_Script.ipynb\nSource of first PyTorch implementation (by Michael Oliver / mdo): https://forum.numer.ai/t/model-diagnostics-feature-exposure/899/12\n:param feature_names: List of column names to reduce feature exposure. Uses all feature columns by default.\n:param pred_name: Prediction column to neutralize.\n:param max_exposure: Number in range [0…1] indicating how much to reduce max feature exposure to."
  },
  {
    "objectID": "postprocessing.html#numerai-classic",
    "href": "postprocessing.html#numerai-classic",
    "title": "Postprocessing",
    "section": "1.1. Numerai Classic",
    "text": "1.1. Numerai Classic\nPostprocessing steps that are specific to Numerai Classic\n\n# 1.1.\n# No Numerai Classic specific postprocessors implemented yet."
  },
  {
    "objectID": "postprocessing.html#numerai-signals",
    "href": "postprocessing.html#numerai-signals",
    "title": "Postprocessing",
    "section": "1.2. Numerai Signals",
    "text": "1.2. Numerai Signals\nPostprocessors that are specific to Numerai Signals.\n\n# 1.2.\n# No Numerai Signals specific postprocessors implemented yet."
  },
  {
    "objectID": "postprocessing.html#custom-postprocessors",
    "href": "postprocessing.html#custom-postprocessors",
    "title": "Postprocessing",
    "section": "2. Custom PostProcessors",
    "text": "2. Custom PostProcessors\nAs with preprocessors, there are an almost unlimited number of ways to postprocess data. We (once again) invite the Numerai community to develop Numerai Classic and Signals postprocessors.\nA new Postprocessor should inherit from BasePostProcessor and implement a transform method. The transform method should take a NumerFrame as input and return a NumerFrame object as output. A template for this is given below.\nTo enable fancy logging output. Add the @display_processor_info decorator to the transform method.\n\nsource\n\nAwesomePostProcessor\n\n AwesomePostProcessor (final_col_name:str, *args, **kwargs)\n\nTEMPLATE - Do some awesome postprocessing.\n:param final_col_name: Column name to store manipulated or ensembled predictions in."
  },
  {
    "objectID": "key.html",
    "href": "key.html",
    "title": "Key",
    "section": "",
    "text": "Numerai authentication is done by passing a valid pub_id and secret_key for the numerapi API.\nAs stakes get larger we might want additional protection and confidence that credentials are parsed correctly. We might also like to load credentials from a (JSON) file that is stored safely somewhere. This section offers tools to safely load and use Numerai credentials.\nA Key object is needed to initialize submission objects.\nThe Key object allows you to either initialize from Python variables (strings) or load from a JSON file.\nIn order to load_key_from_json, the key JSON file must have the following format:\n{\"pub_id\": \"PUBLIC_ID\", \"secret_key\": \"SECRET_KEY\"}\n\nsource\n\nload_key_from_json\n\n load_key_from_json (file_path:str, *args, **kwargs)\n\nInitialize Key object from JSON file.\nCredentials file must have the following format:\n{\"pub_id\": \"PUBLIC_ID\", \"secret_key\": \"SECRET_KEY\"}\n\nsource\n\n\nKey\n\n Key (pub_id:str, secret_key:str)\n\nNumerai credentials.\n\n\nExample usage 1: direct initialization\n\n# Random credentials\npub_id, secret_key = \"QVdFU09NRV9QVUJMSUNfSUQ=\", \"VkVSWV9FVkVOX01PUkVfU0VDUkVUX0tFWQ==\"\nexample_key = Key(pub_id=pub_id, secret_key=secret_key)\n\n\n\n\n\nassert (example_key.pub_id, example_key.secret_key) == (pub_id, secret_key)\n\n\n\n\n\n\nExample usage 2: loading from JSON\n\nexample_key2 = load_key_from_json(\"test_assets/test_credentials.json\")\n\n\n\n\nThis Key contains the credentials defined in test_assets/test_credentials.json.\n\nassert (example_key2.pub_id, example_key2.secret_key) == (\n    \"UFVCTElDX0lE\",\n    \"U1VQRVJfU0VDUkVUX0tFWQ==\",\n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NumerBlox",
    "section": "",
    "text": "numerblox offers Numerai specific functionality, so you can worry less about software/data engineering and focus more on building great Numerai models!\nMost of the components in this library are designed for solid weekly inference pipelines, but tools like NumerFrame, preprocessors and evaluators also greatly simplify the training process.\nDocumentation: crowdcent.github.io/numerblox"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "NumerBlox",
    "section": "1. Install",
    "text": "1. Install"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "NumerBlox",
    "section": "1. Getting Started",
    "text": "1. Getting Started\nThis document has been generated by NBDev. Please edit nbs/index.ipynb instead of this README.md. Read CONTRIBUTING.MD for more information on the contribution process and how to change files. Thank you!\n\n1.1 Installation\nInstall numerblox from PyPi by running:\npip install numerblox\nAlternatively you can clone this repository and install it in development mode running the following from the root of the repository:\npip install -e .\n\n\n1.2 Running Notebooks\nStart by spinning up your favorite Jupyter Notebook environment. Here we’ll use:\njupyter notebook\nTest your installation using one of the education notebooks in nbs/edu_nbs. A good example is numerframe_tutorial. Run it in your Notebook environment to quickly test if your installation has succeeded\n\n\n2.1. Contents\n\n2.1.1. Core functionality\nnumerblox features the following functionality:\n\nDownloading data\nA custom data structure extending Pandas DataFrame (NumerFrame)\nA suite of preprocessors for Numerai Classic and Signals (feature selection, engineering and manipulation)\nModel objects for easy inference.\nA suite of postprocessors for Numerai Classic and Signals (standardization, ensembling, neutralization and penalization)\nPipelines handling processing and prediction (ModelPipeline and ModelPipelineCollection)\nEvaluation (NumeraiClassicEvaluator and NumeraiSignalsEvaluator)\nAuthentication (Key and load_key_from_json)\nSubmitting (NumeraiClassicSubmitter, NumeraiSignalsSubmitter and NumerBaySubmitter)\n\n\n\n2.1.2. Educational notebooks\nExample notebooks can be found in the nbs/edu_nbs directory.\nnbs/edu_nbs currently contains the following examples: - numerframe_tutorial.ipynb: A deep dive into what NumerFrame has to offer. - submitting.ipynb: How to use Submitters for safe and easy Numerai submissions. - google_cloud_storage.ipynb: How to use Downloaders and Submitters to interact with Google Cloud Storage (GCS). - load_model_from_wandb.ipynb: For Weights & Biases users. Easily pull a model from W&B for inference. - numerbay_integration.ipynb: How to use NumerBlox to download and upload predictions listed on NumerBay. - synthetic_data_generation.ipynb: Tutorial for generating synthetic data for training Numerai models.\nDevelopment notebooks are also in the nbs directory. These notebooks are also used to generate the documentation.\nFull documentation: crowdcent.github.io/numerblox\n\n\n\n2.2. Examples\nBelow we will illustrate a common use case for inference pipelines. To learn more in-depth about the features of this library, check out notebooks in nbs/edu_nbs.\n\n2.2.1. Numerai Classic\n# --- 0. Numerblox dependencies ---\nfrom numerblox.download import NumeraiClassicDownloader\nfrom numerblox.numerframe import create_numerframe\nfrom numerblox.postprocessing import FeatureNeutralizer\nfrom numerblox.model import SingleModel\nfrom numerblox.model_pipeline import ModelPipeline\nfrom numerblox.key import load_key_from_json\nfrom numerblox.submission import NumeraiClassicSubmitter\n\n# --- 1. Download version 4 data ---\ndownloader = NumeraiClassicDownloader(\"data\")\ndownloader.download_inference_data(\"current_round\")\n\n# --- 2. Initialize NumerFrame ---\ndataf = create_numerframe(file_path=\"data/current_round/live.parquet\")\n\n# --- 3. Define and run pipeline ---\nmodels = [SingleModel(\"test_assets/joblib_v2_example_model.joblib\",\n                      model_name=\"test\")]\n# No preprocessing and 0.5 feature neutralization\npostprocessors = [FeatureNeutralizer(pred_name=f\"prediction_test\",\n                                     proportion=0.5)]\npipeline = ModelPipeline(preprocessors=[],\n                         models=models,\n                         postprocessors=postprocessors)\ndataf = pipeline(dataf)\n\n# --- 4. Submit ---\n# Load credentials from .json (random credentials in this example)\nkey = load_key_from_json(\"test_assets/test_credentials.json\")\nsubmitter = NumeraiClassicSubmitter(directory_path=\"sub_current_round\", key=key)\n# full_submission checks contents, saves as csv and submits.\nsubmitter.full_submission(dataf=dataf,\n                          cols=f\"prediction_test_neutralized_0.5\",\n                          model_name=\"test\")\n\n# --- 5. Clean up environment (optional) ---\ndownloader.remove_base_directory()\nsubmitter.remove_base_directory()\n\n\n💻 Directory structure before starting\n┗━━ 📁 test_assets\n    ┣━━ 📄 joblib_v2_example_model.joblib\n    ┗━━ 📄 test_credentials.json\n\n\n\n💻 Directory structure after submitting\n┣━━ 📁 data\n┃   ┗━━ 📁 current_round\n┃       ┗━━ 📄 numerai_tournament_data.parquet\n┗━━ 📁 sub_current_round\n    ┗━━ 📄 test_model1.csv\n\n\n\n\n\n2.2.2. Numerai Signals\n# --- 0. Numerblox dependencies ---\nfrom numerblox.download import KaggleDownloader\nfrom numerblox.numerframe import create_numerframe\nfrom numerblox.preprocessing import KatsuFeatureGenerator\nfrom numerblox.model import SingleModel\nfrom numerblox.model_pipeline import ModelPipeline\nfrom numerblox.key import load_key_from_json\nfrom numerblox.submission import NumeraiSignalsSubmitter\n\n# --- 1. Download Katsu1110 yfinance dataset from Kaggle ---\nkd = KaggleDownloader(\"data\")\nkd.download_inference_data(\"code1110/yfinance-stock-price-data-for-numerai-signals\")\n\n# --- 2. Initialize NumerFrame ---\ndataf = create_numerframe(\"data/full_data.parquet\")\n\n# --- 3. Define and run pipeline ---\nmodels = [SingleModel(\"models/signals_model.cbm\", model_name=\"cb\")]\n# Simple and fast feature generator based on Katsu Signals starter notebook\n# https://www.kaggle.com/code1110/numeraisignals-starter-for-beginners\npipeline = ModelPipeline(preprocessors=[KatsuFeatureGenerator(windows=[20, 40, 60])],\n                         models=models,\n                         postprocessors=[])\ndataf = pipeline(dataf)\n\n# --- 4. Submit ---\n# Load credentials from .json (random credentials in this example)\nkey = load_key_from_json(\"test_assets/test_credentials.json\")\nsubmitter = NumeraiSignalsSubmitter(directory_path=\"sub_current_round\", key=key)\n# full_submission checks contents, saves as csv and submits.\n# cols selection must at least contain 1 ticker column and a signal column.\ndataf['signal'] = dataf['prediction_cb']\nsubmitter.full_submission(dataf=dataf,\n                          cols=['bloomberg_ticker', 'signal'],\n                          model_name=\"test_model1\")\n\n# --- 5. Clean up environment (optional) ---\nkd.remove_base_directory()\nsubmitter.remove_base_directory()\n\n\n💻 Directory structure before starting\n┣━━ 📁 test_assets\n┃   ┗━━ 📄 test_credentials.json\n┗━━ 📁 models\n    ┗━━ 📄 signals_model.cbm\n\n\n\n💻 Directory structure after submitting\n┣━━ 📁 data\n┃   ┗━━ 📄 full_data.parquet\n┗━━ 📁 sub_current_round\n    ┗━━ 📄 submission.csv"
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "NumerBlox",
    "section": "3. Contributing",
    "text": "3. Contributing\nBe sure to read CONTRIBUTING.md for detailed instructions on contributing.\nIf you have questions or want to discuss new ideas for numerblox, please create a Github issue first."
  },
  {
    "objectID": "index.html#branch-structure",
    "href": "index.html#branch-structure",
    "title": "NumerBlox",
    "section": "4. Branch structure",
    "text": "4. Branch structure\nEvery new feature should be implemented in a branch that branches from dev and has the naming convention feature/{FEATURE_DESCRIPTION}. Explicit bugfixes should be named bugfix/{FIX_DESCRIPTION}. An example structure is given below.\n\n\nBranch structure\n┗━━ 📦 master (release)\n    ┗━━ 👨‍💻 dev\n        ┣━━ ✨ feature/ta-signals-features\n        ┣━━ ✨ feature/news-api-downloader\n        ┣━━ ✨ feature/staking-portfolio-management\n        ┗━━ ✨ bugfix/evaluator-metrics-fix"
  },
  {
    "objectID": "index.html#crediting-sources",
    "href": "index.html#crediting-sources",
    "title": "NumerBlox",
    "section": "5. Crediting sources",
    "text": "5. Crediting sources\nSome of the components in this library may be based on forum posts, notebooks or ideas made public by the Numerai community. We have done our best to ask all parties who posted a specific piece of code for their permission and credit their work in the documentation. If your code is used in this library without credits, please let us know, so we can add a link to your article/code.\nIf you are contributing to numerblox and are using ideas posted earlier by someone else, make sure to credit them by posting a link to their article/code in documentation."
  },
  {
    "objectID": "submission.html",
    "href": "submission.html",
    "title": "Submission",
    "section": "",
    "text": "BaseSubmitter handles all submission logic common to Numerai Classic and Numerai Signals. Under the hood directory logic is handled by BaseIO. Each submittor should inherit from BaseSubmitter and implement the .save_csv method.\n\nsource\n\n\n\n BaseSubmitter (directory_path:str,\n                api:Union[numerapi.numerapi.NumerAPI,numerapi.signalsapi.S\n                ignalsAPI])\n\nBasic functionality for submitting to Numerai.\nUses numerapi under the hood. More info: https://numerapi.readthedocs.io/\n:param directory_path: Directory to store and read submissions from.\n:param api: NumerAPI or SignalsAPI"
  },
  {
    "objectID": "submission.html#basesubmitter",
    "href": "submission.html#basesubmitter",
    "title": "Submission",
    "section": "",
    "text": "BaseSubmitter handles all submission logic common to Numerai Classic and Numerai Signals. Under the hood directory logic is handled by BaseIO. Each submittor should inherit from BaseSubmitter and implement the .save_csv method.\n\nsource\n\n\n\n BaseSubmitter (directory_path:str,\n                api:Union[numerapi.numerapi.NumerAPI,numerapi.signalsapi.S\n                ignalsAPI])\n\nBasic functionality for submitting to Numerai.\nUses numerapi under the hood. More info: https://numerapi.readthedocs.io/\n:param directory_path: Directory to store and read submissions from.\n:param api: NumerAPI or SignalsAPI"
  },
  {
    "objectID": "submission.html#numeraiclassicsubmitter",
    "href": "submission.html#numeraiclassicsubmitter",
    "title": "Submission",
    "section": "1. NumeraiClassicSubmitter",
    "text": "1. NumeraiClassicSubmitter\nFor Numerai Classic submissions. Uses NumerAPI under the hood.\nNote that using submitters requires a Key object.\n\nsource\n\nNumeraiClassicSubmitter\n\n NumeraiClassicSubmitter (directory_path:str, key:numerblox.key.Key,\n                          *args, **kwargs)\n\nSubmit for Numerai Classic.\n:param directory_path: Base directory to save and read prediction files from.\n:param key: Key object containing valid credentials for Numerai Classic.\n*args, **kwargs will be passed to NumerAPI initialization.\n\n\nExample usage 1: NumeraiClassicSubmitter\n\n# example 1\n# Initialization (Random credentials)\ntest_dir = \"test_sub\"\nclassic_key = Key(pub_id=\"UFVCTElDX0lE\", secret_key=\"U1VQRVJfU0VDUkVUX0tFWQ==\")\nnum_sub = NumeraiClassicSubmitter(directory_path=test_dir, key=classic_key)\nassert num_sub.dir.is_dir()\n\n# Create random dataframe\nn_rows = 100\ntargets = \"prediction_mymodel\"\ntest_dataf = pd.DataFrame(np.random.uniform(size=n_rows), columns=[targets])\ntest_dataf[\"id\"] = [uuid.uuid4() for _ in range(n_rows)]\ntest_dataf = test_dataf.set_index(\"id\")\ntest_dataf.head(2)\n\nNo existing directory found at 'test_sub'. Creating directory...\n\n\n\n\n\n\n\n\n\n\nprediction_mymodel\n\n\nid\n\n\n\n\n\n3c5a588a-a4e1-4b4d-976a-e86079ea5053\n0.704015\n\n\nd9ea0000-47a6-4e68-985f-36b3f9c51be4\n0.667652\n\n\n\n\n\n\n\nCSVs can be saved with .save_csv. NumeraiClassicSubmitter will automatically provide checks to make sure that data is saved correctly.\n\nfile_name = \"test.csv\"\nnum_sub.save_csv(dataf=test_dataf, file_name=file_name, cols=targets)\nnum_sub.save_csv(dataf=test_dataf, file_name=\"test2.csv\", cols=targets)\npd.read_csv(f\"{test_dir}/{file_name}\").head(2)\n\n📄 Saving predictions CSV to 'test_sub/test.csv'. 📄\n\n\n\n📄 Saving predictions CSV to 'test_sub/test2.csv'. 📄\n\n\n\n\n\n\n\n\n\n\nid\nprediction\n\n\n\n\n0\n3c5a588a-a4e1-4b4d-976a-e86079ea5053\n0.704015\n\n\n1\nd9ea0000-47a6-4e68-985f-36b3f9c51be4\n0.667652\n\n\n\n\n\n\n\nNumeraiClassicSubmitter also gives you the option to combine multiple predictions csvs that you already created. Prediction will be standardized by default.\n\nsource\n\n\nBaseSubmitter.combine_csvs\n\n BaseSubmitter.combine_csvs (csv_paths:list, aux_cols:list,\n                             era_col:str=None, pred_col:str='prediction')\n\nRead in csv files and combine all predictions with a rank mean.\nMulti-target predictions will be averaged out.\n:param csv_paths: List of full paths to .csv prediction files.\n:param aux_cols: [‘id’] for Numerai Classic.\n[‘ticker’, ‘last_friday’, ‘data_type’], for example, with Numerai Signals.\n:param era_col: Column indicating era (‘era’ or ‘last_friday’).\nWill be used for Grouping the rank mean if given. Skip groupby if no era_col provided.\n:param pred_col: ‘prediction’ for Numerai Classic and ‘signal’ for Numerai Signals.\n\ncombined = num_sub.combine_csvs([\"test_sub/test.csv\", \"test_sub/test2.csv\"], aux_cols=['id'])\nassert combined.columns == ['prediction']\ncombined.head(2)\n\n\n\n\n\n\n\n\n\n\n\nprediction\n\n\nid\n\n\n\n\n\n3c5a588a-a4e1-4b4d-976a-e86079ea5053\n0.75\n\n\nd9ea0000-47a6-4e68-985f-36b3f9c51be4\n0.71\n\n\n\n\n\n\n\nUncomment to save CSV and upload predictions in one go.\n\n# Full submission\n# num_sub.full_submission(dataf=test_dataf, file_name='test.csv', cols=targets, model_name=\"test\")\n\nAfter a successful submission, contents can be removed to keep a clean environment.\n\nnum_sub.remove_base_directory()\nassert not os.path.exists(test_dir)\n\n⚠ Deleting directory for 'NumeraiClassicSubmitter' ⚠\nPath: '/home/clepelaars/numerblox/nbs/test_sub'"
  },
  {
    "objectID": "submission.html#numeraisignalssubmitter",
    "href": "submission.html#numeraisignalssubmitter",
    "title": "Submission",
    "section": "2. NumeraiSignalsSubmitter",
    "text": "2. NumeraiSignalsSubmitter\nNumerai Signals submissions. Uses SignalsAPI under the hood.\n\nsource\n\nNumeraiSignalsSubmitter\n\n NumeraiSignalsSubmitter (directory_path:str, key:numerblox.key.Key,\n                          *args, **kwargs)\n\nSubmit for Numerai Signals.\n:param directory_path: Base directory to save and read prediction files from.\n:param key: Key object containing valid credentials for Numerai Signals.\n*args, **kwargs will be passed to SignalsAPI initialization.\n\n\nExample usage 2: NumeraiSignalsSubmitter\nInitialization (Random credentials)\n\ntest_dir_signals = \"test_sub_signals\"\nsignals_key = Key(pub_id=\"UFVCTElDX0lE\", secret_key=\"U1VQRVJfU0VDUkVUX0tFWQ==\")\nsignals_sub = NumeraiSignalsSubmitter(directory_path=test_dir_signals, key=signals_key)\nassert signals_sub.dir.is_dir()\n\nNo existing directory found at 'test_sub_signals'. Creating directory...\n\n\n\n\ndef create_random_signals_dataf(n_rows=5000):\n    signals_test_dataf = pd.DataFrame(\n        np.random.uniform(size=(n_rows, 1)), columns=[\"signal\"]\n    )\n    signals_test_dataf[\"ticker\"] = [\n        \"\".join(choices(ascii_uppercase, k=4)) for _ in range(n_rows)\n    ]\n    last_friday = str((datetime.now() + relativedelta(weekday=FR(-1))).date()).replace(\"-\", \"\")\n    signals_test_dataf['last_friday'] = last_friday\n    signals_test_dataf['data_type'] = 'live'\n    return signals_test_dataf\n\nsignals_test_dataf = create_random_signals_dataf()\nsignals_test_dataf.head(2)\n\n\n\n\n\n\n\n\nsignal\nticker\nlast_friday\ndata_type\n\n\n\n\n0\n0.907471\nXONJ\n20230901\nlive\n\n\n1\n0.149298\nKDHF\n20230901\nlive\n\n\n\n\n\n\n\nSaving Signals CSV should fail if there is no valid ticker column or if signal has values outside the range \\([0...1]\\).\nUncomment to save CSV and upload predictions in one go.\n\n# Full Signals submission\n# signals_sub.full_submission(dataf=signals_test_dataf, file_name='signals_test.csv', cols=signals_cols, model_name=\"test\")\n\nAfter a successful submission, contents can be removed to keep a clean environment.\n\nsignals_sub.remove_base_directory()\nassert not os.path.exists(test_dir_signals)\n\n⚠ Deleting directory for 'NumeraiSignalsSubmitter' ⚠\nPath: '/home/clepelaars/numerblox/nbs/test_sub_signals'"
  },
  {
    "objectID": "submission.html#numerbaysubmitter",
    "href": "submission.html#numerbaysubmitter",
    "title": "Submission",
    "section": "3. NumerBaySubmitter",
    "text": "3. NumerBaySubmitter\nWrapper on top of the tournament submitters, submits to NumerBay to fulfill sale orders. Make sure you have numerbay installed (pip install numerbay).\n\nsource\n\nNumerBaySubmitter\n\n NumerBaySubmitter (tournament_submitter:Union[__main__.NumeraiClassicSubm\n                    itter,__main__.NumeraiSignalsSubmitter],\n                    upload_to_numerai:bool=True,\n                    numerbay_username:str=None,\n                    numerbay_password:str=None)\n\nSubmit to NumerBay to fulfill sale orders, in addition to submission to Numerai.\n:param tournament_submitter: Base tournament submitter (NumeraiClassicSubmitter or NumeraiSignalsSubmitter). This submitter will use the same directory path. :param upload_to_numerai: Whether to also submit to Numerai using the tournament submitter. Defaults to True, set to False to only upload to NumerBay. :param numerbay_username: NumerBay username :param numerbay_password: NumerBay password\n\n\nExample usage 3: NumerBaySubmitter\nNumerai submissions (existing file)\n\n# numerai_submitter = NumeraiClassicSubmitter(directory_path=\"/app/notebooks/tmp\", key=key)\n# nb_submitter = NumerBaySubmitter(tournament_submitter=numerai_submitter, upload_to_numerai=False,\n#                                  numerbay_username=\"someusername\", numerbay_password=\"somepassword\")\n# nb_submitter.upload_predictions(file_name='upload.csv', model_name='mymodel',\n#                                 numerbay_product_full_name='numerai-predictions-myproduct')\n\nNumerai submissions (from NumerFrame)\n\n# numerai_submitter = NumeraiClassicSubmitter(directory_path=\"/app/notebooks/tmp\", key=key)\n# nb_submitter = NumerBaySubmitter(tournament_submitter=numerai_submitter, upload_to_numerai=False,\n#                                  numerbay_username=\"someusername\", numerbay_password=\"somepassword\")\n# nb_submitter.full_submission(dataf, file_name='upload.csv', model_name='mymodel',\n#                              numerbay_product_full_name='numerai-predictions-myproduct',\n#                              cols='some_pred_col')\n\nSignals submissions (existing file)\n\n# signals_submitter = NumeraiSignalsSubmitter(directory_path=\"/app/notebooks/tmp\", key=key)\n# nb_submitter = NumerBaySubmitter(tournament_submitter=signals_submitter, upload_to_numerai=False,\n#                                  numerbay_username=\"someusername\", numerbay_password=\"somepassword\")\n# nb_submitter.upload_predictions(file_name='upload-signals.csv', model_name='mymodel',\n#                                 numerbay_product_full_name='signals-predictions-myproduct')\n\nSignals submissions (from NumerFrame)\n\n# signals_submitter = NumeraiSignalsSubmitter(directory_path=\"/app/notebooks/tmp\", key=key)\n# nb_submitter = NumerBaySubmitter(tournament_submitter=signals_submitter, upload_to_numerai=False,\n#                                  numerbay_username=\"someusername\", numerbay_password=\"somepassword\")\n# nb_submitter.full_submission(dataf, file_name='upload-signals.csv', model_name='mymodel',\n#                              numerbay_product_full_name='signals-predictions-myproduct',\n#                              cols=['bloomberg_ticker', 'friday_date', 'data_type', 'signal'])"
  },
  {
    "objectID": "preprocessing.html",
    "href": "preprocessing.html",
    "title": "Preprocessing",
    "section": "",
    "text": "These objects will provide a base for all pre- and post-processing functionality and log relevant information."
  },
  {
    "objectID": "preprocessing.html#base",
    "href": "preprocessing.html#base",
    "title": "Preprocessing",
    "section": "",
    "text": "These objects will provide a base for all pre- and post-processing functionality and log relevant information."
  },
  {
    "objectID": "preprocessing.html#baseprocessor",
    "href": "preprocessing.html#baseprocessor",
    "title": "Preprocessing",
    "section": "0.1. BaseProcessor",
    "text": "0.1. BaseProcessor\nBaseProcessor defines common functionality for preprocessing and postprocessing (Section 5).\nEvery Preprocessor should inherit from BaseProcessor and implement the .transform method.\n\nsource\n\nBaseProcessor\n\n BaseProcessor ()\n\nCommon functionality for preprocessors and postprocessors."
  },
  {
    "objectID": "preprocessing.html#logging",
    "href": "preprocessing.html#logging",
    "title": "Preprocessing",
    "section": "0.2. Logging",
    "text": "0.2. Logging\nWe would like to keep an overview of which steps are done in a data pipeline and where processing bottlenecks occur. The decorator below will display for a given function/method: 1. When it has finished. 2. What the output shape of the data is. 3. How long it took to finish.\nTo use this functionality, simply add @display_processor_info as a decorator to the function/method you want to track.\nWe will use this decorator throughout the pipeline (preprocessing, model and postprocessing).\nInspiration for this decorator: Calmcode Pandas Pipe Logs\n\nsource\n\ndisplay_processor_info\n\n display_processor_info (func)\n\nFancy console output for data processing.\n\n\n✅ Finished step TestDisplay. Output shape=(11471, 2183). Time taken for step: 0:00:02.002134. ✅\n\n\n\n\n\n\n\n\n\n\nera\ndata_type\nfeature_honoured_observational_balaamite\nfeature_polaroid_vadose_quinze\nfeature_untidy_withdrawn_bargeman\nfeature_genuine_kyphotic_trehala\nfeature_unenthralled_sportful_schoolhouse\nfeature_divulsive_explanatory_ideologue\nfeature_ichthyotic_roofed_yeshiva\nfeature_waggly_outlandish_carbonisation\n...\ntarget_bravo_v4_20\ntarget_bravo_v4_60\ntarget_charlie_v4_20\ntarget_charlie_v4_60\ntarget_delta_v4_20\ntarget_delta_v4_60\ntarget_echo_v4_20\ntarget_echo_v4_60\ntarget_jeremy_v4_20\ntarget_jeremy_v4_60\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn003bba8a98662e4\n0001\ntrain\n4\n2\n4\n4\n0\n0\n4\n4\n...\n0.25\n0.00\n0.50\n0.25\n0.25\n0.00\n0.25\n0.00\n0.25\n0.25\n\n\nn003bee128c2fcfc\n0001\ntrain\n2\n4\n1\n3\n0\n3\n2\n3\n...\n0.75\n1.00\n0.75\n0.75\n0.75\n0.75\n0.75\n0.75\n0.75\n1.00\n\n\nn0048ac83aff7194\n0001\ntrain\n2\n1\n3\n0\n3\n0\n3\n3\n...\n0.50\n0.25\n0.50\n0.25\n0.50\n0.25\n0.50\n0.25\n0.50\n0.25\n\n\nn00691bec80d3e02\n0001\ntrain\n4\n2\n2\n3\n0\n4\n1\n4\n...\n0.75\n0.50\n0.75\n0.75\n0.50\n0.50\n0.75\n0.50\n0.50\n0.50\n\n\nn00b8720a2fdc4f2\n0001\ntrain\n4\n3\n4\n4\n0\n0\n4\n2\n...\n0.75\n0.50\n0.75\n0.50\n0.75\n0.50\n0.75\n0.50\n0.50\n0.50\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nnff8c4db7e8c26ab\n0005\ntrain\n4\n3\n1\n2\n1\n1\n1\n3\n...\n0.50\n0.50\n1.00\n0.50\n0.75\n0.50\n0.50\n0.50\n0.75\n0.75\n\n\nnffaef40da241883\n0005\ntrain\n3\n2\n4\n3\n1\n4\n3\n0\n...\n0.50\n0.50\n0.75\n0.50\n0.50\n0.50\n0.50\n0.50\n0.50\nNaN\n\n\nnffd795bbc001447\n0005\ntrain\n3\n2\n2\n1\n1\n1\n1\n4\n...\n0.50\n0.50\n0.50\n0.50\n0.50\n0.50\n0.50\n0.50\n0.50\n0.50\n\n\nnfff8f6044464a9e\n0005\ntrain\n2\n0\n2\n3\n2\n4\n1\n4\n...\n0.50\n0.50\n0.50\n0.50\n0.50\n0.50\n0.50\n0.50\n0.50\n0.50\n\n\nnffff520391f69ad\n0005\ntrain\n0\n0\n0\n2\n1\n4\n0\n1\n...\n0.50\n0.50\n0.50\n0.50\n0.25\n0.50\n0.50\n0.50\n0.00\n0.50\n\n\n\n\n11471 rows × 2183 columns"
  },
  {
    "objectID": "preprocessing.html#common-preprocessing-steps",
    "href": "preprocessing.html#common-preprocessing-steps",
    "title": "Preprocessing",
    "section": "1. Common preprocessing steps",
    "text": "1. Common preprocessing steps\nThis section implements commonly used preprocessing for Numerai. We invite the Numerai community to develop new preprocessors."
  },
  {
    "objectID": "preprocessing.html#tournament-agnostic",
    "href": "preprocessing.html#tournament-agnostic",
    "title": "Preprocessing",
    "section": "1.0 Tournament agnostic",
    "text": "1.0 Tournament agnostic\nPreprocessors that can be applied for both Numerai Classic and Numerai Signals.\n\n1.0.1. CopyPreProcessor\nThe first and obvious preprocessor is copying, which is implemented as a default in ModelPipeline (Section 4) to avoid manipulation of the original DataFrame or NumerFrame that you load in.\n\nsource\n\n\nCopyPreProcessor\n\n CopyPreProcessor ()\n\nCopy DataFrame to avoid manipulation of original DataFrame.\n\ndataset = create_numerframe(\n    \"test_assets/train_int8_5_eras.parquet\"\n)\ncopied_dataset = CopyPreProcessor().transform(dataset)\nassert copied_dataset.equals(dataset)\nassert dataset.meta == copied_dataset.meta\n\n✅ Finished step CopyPreProcessor. Output shape=(11471, 2183). Time taken for step: 0:00:00.081558. ✅\n\n\n\n\n\n1.0.2. FeatureSelectionPreProcessor\nFeatureSelectionPreProcessor will keep all features that you pass + keeps all other columns that are not features.\n\nsource\n\n\nFeatureSelectionPreProcessor\n\n FeatureSelectionPreProcessor (feature_cols:Union[str,list])\n\nKeep only features given + all target, predictions and aux columns.\n\nselected_dataset = FeatureSelectionPreProcessor(\n    feature_cols=[\"feature_byzantine_festinate_mannose\"]\n).transform(dataset)\n\nassert selected_dataset.get_feature_data.shape[1] == 1\nassert dataset.meta == selected_dataset.meta\n\n✅ Finished step FeatureSelectionPreProcessor. Output shape=(11471, 52). Time taken for step: 0:00:00.003162. ✅\n\n\n\n\nselected_dataset.head(2)\n\n\n\n\n\n\n\n\nfeature_byzantine_festinate_mannose\ntarget\ntarget_nomi_v4_20\ntarget_nomi_v4_60\ntarget_tyler_v4_20\ntarget_tyler_v4_60\ntarget_victor_v4_20\ntarget_victor_v4_60\ntarget_ralph_v4_20\ntarget_ralph_v4_60\n...\ntarget_charlie_v4_20\ntarget_charlie_v4_60\ntarget_delta_v4_20\ntarget_delta_v4_60\ntarget_echo_v4_20\ntarget_echo_v4_60\ntarget_jeremy_v4_20\ntarget_jeremy_v4_60\nera\ndata_type\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn003bba8a98662e4\n2\n0.25\n0.25\n0.00\n0.50\n0.25\n0.25\n0.00\n0.25\n0.25\n...\n0.50\n0.25\n0.25\n0.00\n0.25\n0.00\n0.25\n0.25\n0001\ntrain\n\n\nn003bee128c2fcfc\n2\n0.75\n0.75\n0.75\n0.75\n0.75\n0.75\n0.75\n0.75\n0.75\n...\n0.75\n0.75\n0.75\n0.75\n0.75\n0.75\n0.75\n1.00\n0001\ntrain\n\n\n\n\n2 rows × 52 columns\n\n\n\n\n\n1.0.3. TargetSelectionPreProcessor\nTargetSelectionPreProcessor will keep all targets that you pass + all other columns that are not targets.\nNot relevant for an inference pipeline, but especially convenient for Numerai Classic training if you train on a subset of the available targets. Can also be applied to Signals if you are using engineered targets in your pipeline.\n\nsource\n\n\nTargetSelectionPreProcessor\n\n TargetSelectionPreProcessor (target_cols:Union[str,list])\n\nKeep only features given + all target, predictions and aux columns.\n\ndataset = create_numerframe(\n    \"test_assets/train_int8_5_eras.parquet\"\n)\ntarget_cols = [\"target\", \"target_janet_v4_60\", \"target_william_v4_20\"]\nselected_dataset = TargetSelectionPreProcessor(target_cols=target_cols).transform(\n    dataset\n)\nassert selected_dataset.get_target_data.shape[1] == len(target_cols)\nselected_dataset.head(2)\n\n✅ Finished step TargetSelectionPreProcessor. Output shape=(11471, 2137). Time taken for step: 0:00:00.076958. ✅\n\n\n\n\n\n\n\n\n\n\ntarget\ntarget_janet_v4_60\ntarget_william_v4_20\nfeature_honoured_observational_balaamite\nfeature_polaroid_vadose_quinze\nfeature_untidy_withdrawn_bargeman\nfeature_genuine_kyphotic_trehala\nfeature_unenthralled_sportful_schoolhouse\nfeature_divulsive_explanatory_ideologue\nfeature_ichthyotic_roofed_yeshiva\n...\nfeature_shimmering_coverable_congolese\nfeature_biserial_fulfilled_harpoon\nfeature_pitiable_authoritative_clangor\nfeature_abdominal_subtriplicate_fin\nfeature_centenarian_ileac_caschrom\nfeature_expected_beatified_coparcenary\nfeature_unread_isopodan_ethic\nfeature_china_fistular_phenylketonuria\nera\ndata_type\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn003bba8a98662e4\n0.25\n0.5\n0.333333\n4\n2\n4\n4\n0\n0\n4\n...\n2\n2\n2\n2\n2\n2\n2\n2\n0001\ntrain\n\n\nn003bee128c2fcfc\n0.75\n0.5\n0.666667\n2\n4\n1\n3\n0\n3\n2\n...\n2\n2\n2\n2\n2\n2\n2\n2\n0001\ntrain\n\n\n\n\n2 rows × 2137 columns\n\n\n\n\n\n1.0.4. ReduceMemoryProcessor\nNumerai datasets can take up a lot of RAM and may put a strain on your compute environment.\nFor Numerai Classic, many of the feature and target columns can be downscaled to float16. int8 if you are using the Numerai int8 datasets. For Signals it depends on the features you are generating.\nReduceMemoryProcessor downscales the type of your numeric columns to reduce the memory footprint as much as possible.\n\nsource\n\n\nReduceMemoryProcessor\n\n ReduceMemoryProcessor (deep_mem_inspect=False)\n\nReduce memory usage as much as possible.\nCredits to kainsama and others for writing about memory usage reduction for Numerai data: https://forum.numer.ai/t/reducing-memory/313\n:param deep_mem_inspect: Introspect the data deeply by interrogating object dtypes. Yields a more accurate representation of memory usage if you have complex object columns.\n\ndataf = create_numerframe(\"test_assets/train_int8_5_eras.parquet\")\nrmp = ReduceMemoryProcessor()\ndataf = rmp.transform(dataf)\n\nMemory usage of DataFrame is 25.73 MB\n\n\n\nMemory usage after optimization is: 47.98 MB\n\n\n\n Usage decreased by -86.48%\n\n\n\n✅ Finished step ReduceMemoryProcessor. Output shape=(11471, 2183). Time taken for step: 0:00:05.473843. ✅"
  },
  {
    "objectID": "preprocessing.html#numerai-classic",
    "href": "preprocessing.html#numerai-classic",
    "title": "Preprocessing",
    "section": "1.1. Numerai Classic",
    "text": "1.1. Numerai Classic\nThe Numerai Classic dataset has a certain structure that you may not encounter in the Numerai Signals tournament. Therefore, this section has all preprocessors that can only be applied to Numerai Classic.\n\n1.1.0 Numerai Classic: Version agnostic\nPreprocessors that work for all Numerai Classic versions.\n\n1.1.0.1. BayesianGMMTargetProcessor\n\nsource\n\n\n\nBayesianGMMTargetProcessor\n\n BayesianGMMTargetProcessor (target_col:str='target',\n                             feature_names:list=None, n_components:int=6)\n\nGenerate synthetic (fake) target using a Bayesian Gaussian Mixture model.\nBased on Michael Oliver’s GitHub Gist implementation:\nhttps://gist.github.com/the-moliver/dcdd2862dc2c78dda600f1b449071c93\n:param target_col: Column from which to create fake target.\n:param feature_names: Selection of features used for Bayesian GMM. All features by default. :param n_components: Number of components for fitting Bayesian Gaussian Mixture Model.\n\n\n1.1.1. Numerai Classic: v4.2 (Rain) specific\nPreprocessors that only work for the v4.2 (Rain) dataset.\n\n1.1.1.1. GroupStatsProcessor\nv4.2. reintroduces the concept of feature groups. This allows us to create new features out of aggregate statistics. GroupStatsProcessor will calculate the mean, median, standard deviation and skewness for each feature group in a row.\nFirst we define a mapping for each feature group to its associated features.\n\nsource\n\n\n\nGroupStatsPreProcessor\n\n GroupStatsPreProcessor (groups:list=None)\n\nWARNING: Only supported for v4.2 (Rain) data. The Rain dataset (re)introduced feature groups.\nCalculates group statistics for all data groups.\n:param groups: Groups to create features for. All groups by default.\n\nv4_2_data = create_numerframe(\"test_assets/train_int8_5_eras.parquet\")\n\n# Groups can be created for a subset of groups. Processes all groups by default.\ngsp = GroupStatsPreProcessor(groups=['rain', 'sunshine'])\nv4_2_group_stats_df = gsp.transform(v4_2_data)\n\n✅ Finished step GroupStatsPreProcessor. Output shape=(11471, 2189). Time taken for step: 0:00:00.470712. ✅\n\n\n\n\nnew_cols = [col for col in v4_2_group_stats_df.columns if any(k in col for k in [\"rain_mean\", \"rain_std\", \"rain_skew\",\n                                                                                 \"sunshine_mean\", \"sunshine_std\", \"sunshine_skew\"])]\nv4_2_group_stats_df[new_cols].head(2)\n\n\n\n\n\n\n\n\nfeature_rain_mean\nfeature_rain_std\nfeature_rain_skew\nfeature_sunshine_mean\nfeature_sunshine_std\nfeature_sunshine_skew\n\n\nid\n\n\n\n\n\n\n\n\n\n\nn003bba8a98662e4\n2.0\n0.0\n0.0\n2.153846\n0.533761\n3.190169\n\n\nn003bee128c2fcfc\n2.0\n0.0\n0.0\n1.898462\n0.456940\n-3.517183"
  },
  {
    "objectID": "preprocessing.html#numerai-signals",
    "href": "preprocessing.html#numerai-signals",
    "title": "Preprocessing",
    "section": "1.2. Numerai Signals",
    "text": "1.2. Numerai Signals\nPreprocessors that are specific to Numerai Signals.\n\n1.2.1. KatsuFeatureGenerator\nKatsu1110 provides an excellent and fast feature engineering scheme in his Kaggle notebook on starting with Numerai Signals. It is surprisingly effective, fast and works well for modeling. This preprocessor is based on his feature engineering setup in that notebook.\nFeatures generated: 1. MACD and MACD signal 2. RSI 3. Percentage rate of return 4. Volatility 5. MA (moving average) gap\n\nsource\n\n\nKatsuFeatureGenerator\n\n KatsuFeatureGenerator (windows:list, ticker_col:str='ticker',\n                        close_col:str='close', num_cores:int=None)\n\nEffective feature engineering setup based on Katsu’s starter notebook. Based on source by Katsu1110: https://www.kaggle.com/code1110/numeraisignals-starter-for-beginners\n:param windows: Time interval to apply for window features:\n\nPercentage Rate of change\nVolatility\nMoving Average gap\n\n:param ticker_col: Columns with tickers to iterate over.\n:param close_col: Column name where you have closing price stored.\nLet’s create a simple synthetic dataset to test preprocessors on. Many preprocessor require at least ticker, date and close columns. More advanced feature engineering preprocessors should also have open, high, low and volume columns.\n\ninstances = []\ntickers = [\"ABC.US\", \"DEF.US\", \"GHI.US\"]\nfor ticker in tickers:\n    price = np.random.randint(10, 100)\n    for i in range(100):\n        price += np.random.uniform(-1, 1)\n        instances.append(\n            {\n                \"ticker\": ticker,\n                \"date\": pd.Timestamp(\"2020-01-01\") + pd.Timedelta(days=i),\n                \"open\": price - 0.05,\n                \"high\": price + 0.02,\n                \"low\": price - 0.01,\n                \"close\": price,\n                \"volume\": np.random.randint(1000, 10000),\n            }\n        )\ndummy_df = NumerFrame(instances)\n\n\ndummy_df.head(2)\n\n\n\n\n\n\n\n\nticker\ndate\nopen\nhigh\nlow\nclose\nvolume\n\n\n\n\n0\nABC.US\n2020-01-01\n59.472206\n59.542206\n59.512206\n59.522206\n5683\n\n\n1\nABC.US\n2020-01-02\n58.490325\n58.560325\n58.530325\n58.540325\n8416\n\n\n\n\n\n\n\n\ndataf = NumerFrame(dummy_df)\ndataf.loc[:, \"friday_date\"] = dataf[\"date\"]\n\n\nkfpp = KatsuFeatureGenerator(windows=[20, 40, 60], num_cores=8)\nnew_dataf = kfpp.transform(dataf)\n\nFeature engineering for 3 tickers using 8 CPU cores.\n\n\n\n\n\n\n\n\n\n✅ Finished step KatsuFeatureGenerator. Output shape=(300, 20). Time taken for step: 0:00:00.153968. ✅\n\n\n\n12 features are generated in this test (3*3 window features + 3 non window features).\n\nnew_dataf.sort_values([\"ticker\", \"date\"]).get_feature_data.tail(2)\n\n\n\n\n\n\n\n\nfeature_close_ROCP_20\nfeature_close_VOL_20\nfeature_close_MA_gap_20\nfeature_close_ROCP_40\nfeature_close_VOL_40\nfeature_close_MA_gap_40\nfeature_close_ROCP_60\nfeature_close_VOL_60\nfeature_close_MA_gap_60\nfeature_RSI\nfeature_MACD\nfeature_MACD_signal\n\n\n\n\n298\n-0.003848\n0.001384\n1.013424\n-0.003641\n0.001535\n1.007679\n0.024575\n0.001445\n1.007059\n57.889148\n-0.124340\n-0.414811\n\n\n299\n-0.001148\n0.001413\n1.020715\n0.014429\n0.001505\n1.014503\n0.036681\n0.001449\n1.013638\n61.647071\n0.045781\n-0.322692\n\n\n\n\n\n\n\n\n\n1.2.2. EraQuantileProcessor\nNumerai Signals’ objective is predicting a ranking of equities. Therefore, we can benefit from creating rankings out of the features. Doing this reduces noise and works as a normalization mechanism for your features. EraQuantileProcessor bins features in a given number of quantiles for each era in the dataset.\n\nsource\n\n\nEraQuantileProcessor\n\n EraQuantileProcessor (num_quantiles:int=50, era_col:str='friday_date',\n                       features:list=None, num_cores:int=None,\n                       random_state:int=0, batch_size:int=1)\n\nTransform features into quantiles on a per-era basis\n:param num_quantiles: Number of buckets to split data into.\n:param era_col: Era column name in the dataframe to perform each transformation.\n:param features: All features that you want quantized. All feature cols by default.\n:param num_cores: CPU cores to allocate for quantile transforming. All available cores by default.\n:param random_state: Seed for QuantileTransformer.\n:param batch_size: How many feature to process at the same time. For Numerai Signals scale data it is advisable to process features one by one. This is the default setting.\n\nera_quantiler = EraQuantileProcessor(num_quantiles=50)\nera_dataf = era_quantiler.transform(new_dataf)\n\nQuantiling for 12 features using 32 CPU cores.\n\n\n\n\n\n\n✅ Finished step EraQuantileProcessor. Output shape=(300, 21). Time taken for step: 0:00:00.457428. ✅\n\n\n\n\nera_dataf.get_feature_data.tail(2)\n\n\n\n\n\n\n\n\nfeature_close_ROCP_20\nfeature_close_VOL_20\nfeature_close_MA_gap_20\nfeature_close_ROCP_40\nfeature_close_VOL_40\nfeature_close_MA_gap_40\nfeature_close_ROCP_60\nfeature_close_VOL_60\nfeature_close_MA_gap_60\nfeature_RSI\nfeature_MACD\nfeature_MACD_signal\nfeature_close_ROCP_20_quantile50\n\n\n\n\n298\n-0.003848\n0.001384\n1.013424\n-0.003641\n0.001535\n1.007679\n0.024575\n0.001445\n1.007059\n57.889148\n-0.124340\n-0.414811\n0.5\n\n\n299\n-0.001148\n0.001413\n1.020715\n0.014429\n0.001505\n1.014503\n0.036681\n0.001449\n1.013638\n61.647071\n0.045781\n-0.322692\n0.5\n\n\n\n\n\n\n\n\n\n1.2.3. TickerMapper\nNumerai Signals data APIs may work with different ticker formats. Our goal with TickerMapper is to map ticker_col to target_ticker_format.\n\nsource\n\n\nTickerMapper\n\n TickerMapper (ticker_col:str='ticker',\n               target_ticker_format:str='bloomberg_ticker',\n               mapper_path:str='https://numerai-signals-public-data.s3-us-\n               west-2.amazonaws.com/signals_ticker_map_w_bbg.csv')\n\nMap ticker from one format to another.\n:param ticker_col: Column used for mapping. Must already be present in the input data.\n:param target_ticker_format: Format to map tickers to. Must be present in the ticker map.\nFor default mapper supported ticker formats are: [‘ticker’, ‘bloomberg_ticker’, ‘yahoo’]\n:param mapper_path: Path to CSV file containing at least ticker_col and target_ticker_format columns.\nCan be either a web link of local path. Numerai Signals mapping by default.\nUse default signals mapping to convert between Numerai ticker, Bloomberg ticker and Yahoo ticker formats.\n\ntest_dataf = pd.DataFrame([\"AAPL\", \"MSFT\"], columns=[\"ticker\"])\nmapper = TickerMapper()\nmapper.transform(test_dataf)\n\n✅ Finished step TickerMapper. Output shape=(2, 2). Time taken for step: 0:00:00.001820. ✅\n\n\n\n\n\n\n\n\n\n\nticker\nbloomberg_ticker\n\n\n\n\n0\nAAPL\nAAPL US\n\n\n1\nMSFT\nMSFT US\n\n\n\n\n\n\n\nYou can also use a CSV file for mapping. For example, the mapping Numerai user degerhan provides in dsignals for EOD data.\n\ntest_dataf = pd.DataFrame([\"LLB SW\", \"DRAK NA\", \"SWB MK\", \"ELEKTRA* MF\", \"NOT_A_TICKER\"], columns=[\"bloomberg_ticker\"])\nmapper = TickerMapper(ticker_col=\"bloomberg_ticker\", target_ticker_format=\"signals_ticker\",\n                      mapper_path=\"test_assets/eodhd-map.csv\")\nmapper.transform(test_dataf)\n\n✅ Finished step TickerMapper. Output shape=(5, 2). Time taken for step: 0:00:00.002913. ✅\n\n\n\n\n\n\n\n\n\n\nbloomberg_ticker\nsignals_ticker\n\n\n\n\n0\nLLB SW\nLLB.SW\n\n\n1\nDRAK NA\nDRAK.AS\n\n\n2\nSWB MK\n5211.KLSE\n\n\n3\nELEKTRA* MF\nELEKTRA.MX\n\n\n4\nNOT_A_TICKER\nNaN\n\n\n\n\n\n\n\n\n\n1.2.4. SignalsTargetProcessor\nNumerai provides targets for 5000 stocks that are neutralized against all sorts of factors. However, it can be helpful to experiment with creating your own targets. You might want to explore different windows, different target binning and/or neutralization. SignalsTargetProcessor engineers 3 different targets for every given windows: - _raw: Raw return based on price movements. - _rank: Ranks of raw return. - _group: Binned returns based on rank.\nNote that Numerai provides targets based on 4-day returns and 20-day returns. While you can explore any window you like, it makes sense to start with windows close to these timeframes.\nFor the bins argument there are also many options possible. The followed are commonly used binning: - Nomi bins: [0, 0.05, 0.25, 0.75, 0.95, 1] - Uniform bins: [0, 0.20, 0.40, 0.60, 0.80, 1]\n\nsource\n\n\nSignalsTargetProcessor\n\n SignalsTargetProcessor (price_col:str='close', windows:list=None,\n                         bins:list=None, labels:list=None)\n\nEngineer targets for Numerai Signals.\nMore information on implements Numerai Signals targets:\nhttps://forum.numer.ai/t/decoding-the-signals-target/2501\n:param price_col: Column from which target will be derived.\n:param windows: Timeframes to use for engineering targets. 10 and 20-day by default.\n:param bins: Binning used to create group targets. Nomi binning by default.\n:param labels: Scaling for binned target. Must be same length as resulting bins (bins-1). Numerai labels by default.\n\nstp = SignalsTargetProcessor()\nera_dataf.meta.era_col = \"date\"\nnew_target_dataf = stp.transform(era_dataf)\nnew_target_dataf.get_target_data.head(2)\n\n\n\n\n✅ Finished step SignalsTargetProcessor. Output shape=(300, 27). Time taken for step: 0:00:00.270008. ✅\n\n\n\n\n\n\n\n\n\n\ntarget_10d_raw\ntarget_10d_rank\ntarget_10d_group\ntarget_20d_raw\ntarget_20d_rank\ntarget_20d_group\n\n\n\n\n0\n-0.027611\n0.666667\n0.5\n-0.02454\n0.333333\n0.5\n\n\n1\n-0.004478\n0.666667\n0.5\n-0.00889\n0.666667\n0.5\n\n\n\n\n\n\n\n\n\n1.2.5. LagPreProcessor\nMany models like Gradient Boosting Machines (GBMs) don’t learn any time-series patterns by itself. However, if we create lags of our features the models will pick up on time dependencies between features. LagPreProcessor create lag features for given features and windows.\n\nsource\n\n\nLagPreProcessor\n\n LagPreProcessor (windows:list=None, ticker_col:str='bloomberg_ticker',\n                  feature_names:list=None)\n\nAdd lag features based on given windows.\n:param windows: All lag windows to process for all features.\n[5, 10, 15, 20] by default (4 weeks lookback)\n:param ticker_col: Column name for grouping by tickers.\n:param feature_names: All features for which you want to create lags. All features by default.\n\nlpp = LagPreProcessor(ticker_col=\"ticker\", feature_names=[\"close\", \"volume\"])\ndataf = lpp(dataf)\n\n\n\n\n✅ Finished step LagPreProcessor. Output shape=(300, 16). Time taken for step: 0:00:00.026052. ✅\n\n\n\nAll lag features will contain lag in the column name.\n\ndataf.get_pattern_data(\"lag\").tail(2)\n\n\n\n\n\n\n\n\nclose_lag5\nclose_lag10\nclose_lag15\nclose_lag20\nvolume_lag5\nvolume_lag10\nvolume_lag15\nvolume_lag20\n\n\n\n\n298\n85.300109\n84.936054\n86.874928\n87.625224\n3420.0\n8816.0\n1034.0\n7048.0\n\n\n299\n85.761071\n83.983167\n87.079665\n88.011967\n2626.0\n2588.0\n8234.0\n9973.0\n\n\n\n\n\n\n\n\n\n1.2.6. DifferencePreProcessor\nAfter creating lags with the LagPreProcessor, it may be useful to create new features that calculate the difference between those lags. Through this process in DifferencePreProcessor, we can provide models with more time-series related patterns.\n\nsource\n\n\nDifferencePreProcessor\n\n DifferencePreProcessor (windows:list=None, feature_names:list=None,\n                         pct_diff:bool=False, abs_diff:bool=False)\n\nAdd difference features based on given windows. Run LagPreProcessor first.\n:param windows: All lag windows to process for all features.\n:param feature_names: All features for which you want to create differences. All features that also have lags by default.\n:param pct_change: Method to calculate differences. If True, will calculate differences with a percentage change. Otherwise calculates a simple difference. Defaults to False\n:param abs_diff: Whether to also calculate the absolute value of all differences. Defaults to True\n\ndpp = DifferencePreProcessor(\n    feature_names=[\"close\", \"volume\"], windows=[5, 10, 15, 20], pct_diff=True\n)\ndataf = dpp.transform(dataf)\n\n\n\n\n✅ Finished step DifferencePreProcessor. Output shape=(300, 24). Time taken for step: 0:00:00.026679. ✅\n\n\n\nAll difference features will contain diff in the column name.\n\ndataf.get_pattern_data(\"diff\").tail(2)\n\n\n\n\n\n\n\n\nclose_diff5\nclose_diff10\nclose_diff15\nclose_diff20\nvolume_diff5\nvolume_diff10\nvolume_diff15\nvolume_diff20\n\n\n\n\n298\n0.023306\n0.027692\n0.004756\n-0.003848\n0.409064\n-0.45338\n3.660542\n-0.316260\n\n\n299\n0.025068\n0.046768\n0.009546\n-0.001148\n0.691927\n0.71677\n-0.460408\n-0.554497\n\n\n\n\n\n\n\n\n\n1.2.7. PandasTaFeatureGenerator\nThis generator takes in a pandas-ta strategy and processing them on multiple cores. There is a simple default strategy available with RSI features for 14 and 60 rows.\nTo learn more about defining pandas-ta strategies. Check this section of the pandas-ta README.\n\nsource\n\n\nPandasTaFeatureGenerator\n\n PandasTaFeatureGenerator (strategy:pandas_ta.core.Strategy=None,\n                           ticker_col:str='ticker', num_cores:int=None)\n\nGenerate features with pandas-ta. https://github.com/twopirllc/pandas-ta\n:param strategy: Valid Pandas Ta strategy.\nFor more information on creating a strategy, see:\nhttps://github.com/twopirllc/pandas-ta#pandas-ta-strategy\nBy default, a strategy with RSI(14) and RSI(60) is used.\n:param ticker_col: Column name for grouping by tickers.\n:param num_cores: Number of cores to use for multiprocessing.\nBy default, all available cores are used.\n\npta = PandasTaFeatureGenerator()\nnew_pta_df = pta.transform(dummy_df)\nnew_pta_df.tail(2)\n\n\n\n\n\n\n\n✅ Finished step PandasTaFeatureGenerator. Output shape=(300, 10). Time taken for step: 0:00:00.422446. ✅\n\n\n\n\n\n\n\n\n\n\nticker\ndate\nopen\nhigh\nlow\nclose\nvolume\nfriday_date\nfeature_RSI_14\nfeature_RSI_60\n\n\n\n\n298\nGHI.US\n2020-04-08\n87.238077\n87.308077\n87.278077\n87.288077\n4819\n2020-04-08\n57.889148\n53.066919\n\n\n299\nGHI.US\n2020-04-09\n87.860901\n87.930901\n87.900901\n87.910901\n4443\n2020-04-09\n61.647071\n54.309328\n\n\n\n\n\n\n\nThe feature data can be selected directly through a NumerFrame convenience method called .get_feature_data.\n\nnew_pta_df.get_feature_data.tail(2)\n\n\n\n\n\n\n\n\nfeature_RSI_14\nfeature_RSI_60\n\n\n\n\n298\n57.889148\n53.066919\n\n\n299\n61.647071\n54.309328\n\n\n\n\n\n\n\nA custom pandas-ta strategy can be defined as follows. Check the pandas-ta docs for more information on available indicators and arguments.\nta takes in a list of dictionaries defining indicators and optional additional arguments. We use col_names for convenience so features are prefixed by feature_ and can be easily retrieved within a NumerFrame.\n\nstrategy = ta.Strategy(name=\"mystrategy\",\n                       ta=[{\"kind\": \"cmo\", \"col_names\": (\"feature_CMO\")}, # Chande Momentum Oscillator\n                           {\"kind\": \"rsi\", \"length\": 60, \"col_names\": (\"feature_RSI_60\")} # Relative Strength Index\n                           ])\n\n\npta = PandasTaFeatureGenerator(strategy=strategy)\nnew_pta_df = pta.transform(dummy_df)\nnew_pta_df.get_feature_data.tail(5)\n\n\n\n\n\n\n\n✅ Finished step PandasTaFeatureGenerator. Output shape=(300, 10). Time taken for step: 0:00:00.375046. ✅\n\n\n\n\n\n\n\n\n\n\nfeature_CMO\nfeature_RSI_60\n\n\n\n\n295\n-2.293119\n50.268290\n\n\n296\n6.295807\n51.480114\n\n\n297\n19.281522\n53.528935\n\n\n298\n15.778296\n53.066919\n\n\n299\n23.294142\n54.309328"
  },
  {
    "objectID": "preprocessing.html#custom-preprocessors",
    "href": "preprocessing.html#custom-preprocessors",
    "title": "Preprocessing",
    "section": "2. Custom preprocessors",
    "text": "2. Custom preprocessors\nThere are an almost unlimited number of ways to preprocess (selection, engineering and manipulation). We have only scratched the surface with the preprocessors currently implemented. We invite the Numerai community to develop Numerai Classic and Numerai Signals preprocessors.\nA new Preprocessor should inherit from BaseProcessor and implement a transform method. For efficient implementation, we recommend you use NumerFrame functionality for preprocessing. You can also support Pandas DataFrame input as long as the transform method returns a NumerFrame. This ensures that the Preprocessor still works within a full numerai-blocks pipeline. A template for new preprocessors is given below.\nTo enable fancy logging output. Add the @display_processor_info decorator to the transform method.\n\nsource\n\nAwesomePreProcessor\n\n AwesomePreProcessor ()\n\nTEMPLATE - Do some awesome preprocessing."
  },
  {
    "objectID": "numerframe.html",
    "href": "numerframe.html",
    "title": "NumerFrame",
    "section": "",
    "text": "NumerFrame is a data structure that extends pd.DataFrame with functionality convenient for Numerai users. The main benefits include: 1. Automatically track features, targets, prediction and other columns + easily retrieve these data slices. 2. Other library functionality automatically recognizes era column (era, friday_date or date). 3. Integrations with other library components (i.e. preprocessing, model, modelpipeline, postprocessing, evaluation and submission) to create more solid inference pipelines and increase reliability.\nBesides, all functionality of Pandas DataFrames is still available in the NumerFrame. You therefore don’t have to create new pipelines to process your data when using NumerFrame.\nWe adopt the convention: 1. All feature column names should start with 'feature'. 2. All target column names should start with 'target'. 3. All prediction column names should start with 'prediction'. 4. Data should contain an 'era', 'friday_date' or 'date' column, as is almost always the case with Numerai datasets.\nEvery column for which these conditions do not hold will be classified as an 'aux' column.\n\nsource\n\n\n\n NumerFrame (*args, **kwargs)\n\nData structure which extends Pandas DataFrames and allows for additional Numerai specific functionality.\ncreate_numerframe automatically recognizes your data file format, loads it into a NumerFrame and allows for column selection before loading.\nSupport file formats are .csv, .parquet, .pkl, .pickle, .xsl, .xslx, .xlsm, .xlsb, .odf, .ods and .odt. If the file format for your use case is missing, feel free to create a Github issue or submit a pull request. See README.md for more information on contributing.\n\nsource\n\n\n\n\n create_numerframe (file_path:str, columns:list=None, *args, **kwargs)\n\nConvenient function to initialize NumerFrame. Support most used file formats for Pandas DataFrames\n(.csv, .parquet, .xls, .pkl, etc.). For more details check https://pandas.pydata.org/docs/reference/io.html\n:param file_path: Relative or absolute path to data file.\n:param columns: Which columns to read (All by default).\n*args, **kwargs will be passed to Pandas loading function."
  },
  {
    "objectID": "numerframe.html#overview-the-numerframe",
    "href": "numerframe.html#overview-the-numerframe",
    "title": "NumerFrame",
    "section": "",
    "text": "NumerFrame is a data structure that extends pd.DataFrame with functionality convenient for Numerai users. The main benefits include: 1. Automatically track features, targets, prediction and other columns + easily retrieve these data slices. 2. Other library functionality automatically recognizes era column (era, friday_date or date). 3. Integrations with other library components (i.e. preprocessing, model, modelpipeline, postprocessing, evaluation and submission) to create more solid inference pipelines and increase reliability.\nBesides, all functionality of Pandas DataFrames is still available in the NumerFrame. You therefore don’t have to create new pipelines to process your data when using NumerFrame.\nWe adopt the convention: 1. All feature column names should start with 'feature'. 2. All target column names should start with 'target'. 3. All prediction column names should start with 'prediction'. 4. Data should contain an 'era', 'friday_date' or 'date' column, as is almost always the case with Numerai datasets.\nEvery column for which these conditions do not hold will be classified as an 'aux' column.\n\nsource\n\n\n\n NumerFrame (*args, **kwargs)\n\nData structure which extends Pandas DataFrames and allows for additional Numerai specific functionality.\ncreate_numerframe automatically recognizes your data file format, loads it into a NumerFrame and allows for column selection before loading.\nSupport file formats are .csv, .parquet, .pkl, .pickle, .xsl, .xslx, .xlsm, .xlsb, .odf, .ods and .odt. If the file format for your use case is missing, feel free to create a Github issue or submit a pull request. See README.md for more information on contributing.\n\nsource\n\n\n\n\n create_numerframe (file_path:str, columns:list=None, *args, **kwargs)\n\nConvenient function to initialize NumerFrame. Support most used file formats for Pandas DataFrames\n(.csv, .parquet, .xls, .pkl, etc.). For more details check https://pandas.pydata.org/docs/reference/io.html\n:param file_path: Relative or absolute path to data file.\n:param columns: Which columns to read (All by default).\n*args, **kwargs will be passed to Pandas loading function."
  },
  {
    "objectID": "numerframe.html#numerframe-usage",
    "href": "numerframe.html#numerframe-usage",
    "title": "NumerFrame",
    "section": "NumerFrame Usage",
    "text": "NumerFrame Usage\nA NumerFrame object can be initialized from memory just like you would with a Pandas DataFrame.\n\n1. Initialize from memory\n\ntest_features = [f\"feature_{l}\" for l in \"ABCDEFGHIK\"]\nid_col = [uuid.uuid4().hex for _ in range(100)]\n\n# Random DataFrame\ndataf = pd.DataFrame(np.random.uniform(size=(100, 10)), columns=test_features)\ndataf[\"id\"] = id_col\ndataf[[\"target\", \"target_1\", \"target_2\"]] = np.random.normal(size=(100, 3))\ndataf[\"date\"] = range(100)\n\n\nmemory_dataf = NumerFrame(dataf)\nassert memory_dataf.meta.era_col == \"date\"\n\n\nmemory_dataf.head(2)\n\n\n\n\n\n\n\n\nfeature_A\nfeature_B\nfeature_C\nfeature_D\nfeature_E\nfeature_F\nfeature_G\nfeature_H\nfeature_I\nfeature_K\nid\ntarget\ntarget_1\ntarget_2\ndate\n\n\n\n\n0\n0.939419\n0.248241\n0.131653\n0.078532\n0.203165\n0.704196\n0.152887\n0.053677\n0.890165\n0.629224\n35b61083cea044b280f3d33ebd55b420\n1.630649\n-1.932858\n0.542668\n0\n\n\n1\n0.422693\n0.537752\n0.137142\n0.686566\n0.582026\n0.103345\n0.748019\n0.186389\n0.500554\n0.943307\nce5d2abc49fd40849981a3694b96334f\n0.811435\n-0.841065\n1.128461\n1\n\n\n\n\n\n\n\nThe meta attribute will store which era column is used. This is used in NumerBlox processors to group computations by era where needed.\n\nmemory_dataf.meta\n\n{'era_col': 'date'}\n\n\n\n\n2. Initialize from file path\nYou can also use the convenience function create_numerframe so NumerFrame can be easily initialized. Think of it as a dynamic pd.read_csv, pd.read_parquet, etc.\n\nnum_dataf = create_numerframe(\"test_assets/mini_numerai_version_2_data.parquet\",\n                          )\nassert num_dataf.meta.era_col == \"era\"\n\n\n\n3. Example functionality\n.get_feature_data will retrieve all columns where the column name starts with feature.\n\nnum_dataf.get_feature_data.head(2)\n\n\n\n\n\n\n\n\nfeature_dichasial_hammier_spawner\nfeature_rheumy_epistemic_prancer\nfeature_pert_performative_hormuz\nfeature_hillier_unpitied_theobromine\nfeature_perigean_bewitching_thruster\nfeature_renegade_undomestic_milord\nfeature_koranic_rude_corf\nfeature_demisable_expiring_millepede\nfeature_unscheduled_malignant_shingling\nfeature_clawed_unwept_adaptability\n...\nfeature_unpruned_pedagoguish_inkblot\nfeature_forworn_hask_haet\nfeature_drawable_exhortative_dispersant\nfeature_metabolic_minded_armorist\nfeature_investigatory_inerasable_circumvallation\nfeature_centroclinal_incentive_lancelet\nfeature_unemotional_quietistic_chirper\nfeature_behaviorist_microbiological_farina\nfeature_lofty_acceptable_challenge\nfeature_coactive_prefatorial_lucy\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn559bd06a8861222\n0.25\n0.75\n0.25\n0.75\n0.25\n0.50\n1.0\n0.25\n0.25\n0.75\n...\n0.75\n0.0\n1.00\n0.0\n0.0\n0.25\n0.00\n0.0\n1.00\n0.25\n\n\nn9d39dea58c9e3cf\n0.75\n0.50\n0.75\n1.00\n0.50\n0.25\n0.5\n0.00\n1.00\n0.25\n...\n1.00\n1.0\n0.25\n0.5\n0.0\n0.25\n0.75\n1.0\n0.75\n1.00\n\n\n\n\n2 rows × 1050 columns\n\n\n\n.get_target_data retrieves all columns if the column name starts with \"target\".\n\nnum_dataf.get_target_data.head(2)\n\n\n\n\n\n\n\n\ntarget\ntarget_nomi_20\ntarget_nomi_60\ntarget_jerome_20\ntarget_jerome_60\ntarget_janet_20\ntarget_janet_60\ntarget_ben_20\ntarget_ben_60\ntarget_alan_20\n...\ntarget_paul_20\ntarget_paul_60\ntarget_george_20\ntarget_george_60\ntarget_william_20\ntarget_william_60\ntarget_arthur_20\ntarget_arthur_60\ntarget_thomas_20\ntarget_thomas_60\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn559bd06a8861222\n0.25\n0.25\n0.50\n0.0\n0.50\n0.5\n0.5\n0.25\n0.5\n0.5\n...\n0.0\n0.50\n0.25\n0.5\n0.000000\n0.500000\n0.166667\n0.500000\n0.333333\n0.500000\n\n\nn9d39dea58c9e3cf\n0.50\n0.50\n0.75\n0.5\n0.75\n0.5\n0.5\n0.50\n0.5\n0.5\n...\n0.5\n0.75\n0.50\n0.5\n0.666667\n0.666667\n0.500000\n0.666667\n0.500000\n0.666667\n\n\n\n\n2 rows × 21 columns\n\n\n\n.get_single_target_data only retrieves the column \"target\".\n\nnum_dataf.get_single_target_data.head(2)\n\n\n\n\n\n\n\n\ntarget\n\n\nid\n\n\n\n\n\nn559bd06a8861222\n0.25\n\n\nn9d39dea58c9e3cf\n0.50\n\n\n\n\n\n\n\n.get_pattern_data allows you to get columns based on a certain pattern. In this example we retrieve all 20-day targets.\n\nnum_dataf.get_pattern_data(\"_20\").head(2)\n\n\n\n\n\n\n\n\ntarget_nomi_20\ntarget_jerome_20\ntarget_janet_20\ntarget_ben_20\ntarget_alan_20\ntarget_paul_20\ntarget_george_20\ntarget_william_20\ntarget_arthur_20\ntarget_thomas_20\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn559bd06a8861222\n0.25\n0.0\n0.5\n0.25\n0.5\n0.0\n0.25\n0.000000\n0.166667\n0.333333\n\n\nn9d39dea58c9e3cf\n0.50\n0.5\n0.5\n0.50\n0.5\n0.5\n0.50\n0.666667\n0.500000\n0.500000\n\n\n\n\n\n\n\n\nnum_dataf.head()\n\n\n\n\n\n\n\n\nera\ndata_type\nfeature_dichasial_hammier_spawner\nfeature_rheumy_epistemic_prancer\nfeature_pert_performative_hormuz\nfeature_hillier_unpitied_theobromine\nfeature_perigean_bewitching_thruster\nfeature_renegade_undomestic_milord\nfeature_koranic_rude_corf\nfeature_demisable_expiring_millepede\n...\ntarget_paul_20\ntarget_paul_60\ntarget_george_20\ntarget_george_60\ntarget_william_20\ntarget_william_60\ntarget_arthur_20\ntarget_arthur_60\ntarget_thomas_20\ntarget_thomas_60\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn559bd06a8861222\n0297\ntrain\n0.25\n0.75\n0.25\n0.75\n0.25\n0.50\n1.00\n0.25\n...\n0.00\n0.50\n0.25\n0.50\n0.000000\n0.500000\n0.166667\n0.500000\n0.333333\n0.500000\n\n\nn9d39dea58c9e3cf\n0003\ntrain\n0.75\n0.50\n0.75\n1.00\n0.50\n0.25\n0.50\n0.00\n...\n0.50\n0.75\n0.50\n0.50\n0.666667\n0.666667\n0.500000\n0.666667\n0.500000\n0.666667\n\n\nnb64f06d3a9fc9f1\n0472\ntrain\n1.00\n1.00\n1.00\n0.50\n0.00\n1.00\n0.25\n0.50\n...\n0.00\n0.25\n0.50\n0.50\n0.333333\n0.333333\n0.333333\n0.333333\n0.333333\n0.333333\n\n\nn1927b4862500882\n0265\ntrain\n0.00\n0.00\n0.25\n0.00\n1.00\n0.00\n0.00\n0.00\n...\n0.75\n0.75\n0.50\n0.75\n0.833333\n0.833333\n0.666667\n0.833333\n0.666667\n0.666667\n\n\nnc3234b6eeacd6b7\n0299\ntrain\n0.75\n0.25\n0.00\n0.75\n1.00\n0.25\n0.00\n0.00\n...\n0.25\n0.50\n0.50\n0.50\n0.166667\n0.666667\n0.333333\n0.500000\n0.500000\n0.666667\n\n\n\n\n5 rows × 1073 columns\n\n\n\n.get_era_batch will return a tf.Tensor or np.array with feature data and target data for one or more eras. Convenient for creating neural network DataGenerators. Define convert_to_tf to return a tf.Tensor object. Else a np.array will be returned.\n\nX_era, y_era = num_dataf.get_era_batch(['0297'], convert_to_tf=False)\nX_era\n\n2023-03-23 18:06:22.672389: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-03-23 18:06:22.918932: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-03-23 18:06:24.031738: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/miniconda3/envs/classic_prod/lib/\n2023-03-23 18:06:24.031878: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/miniconda3/envs/classic_prod/lib/\n2023-03-23 18:06:24.031889: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n2023-03-23 18:06:25.689024: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n2023-03-23 18:06:25.689181: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (numerai-training): /proc/driver/nvidia/version does not exist\n2023-03-23 18:06:25.697668: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n&lt;tf.Tensor: shape=(1, 1050), dtype=float32, numpy=array([[0.25, 0.75, 0.25, ..., 0.  , 1.  , 0.25]], dtype=float32)&gt;\n\n\nFor people training autoencoders + MLP you can get a target that contains 3 elements: features, targets and targets. Just define aemlp_batch=True. More info on this setup: AutoEncoder and multitask MLP on new dataset forum post.\n\n# _, y_era_aemlp = num_dataf.get_era_batch(['0297'], convert_to_tf=True, aemlp_batch=True)\n# y_era_aemlp\n\n[&lt;tf.Tensor: shape=(1, 1050), dtype=float32, numpy=array([[0.25, 0.75, 0.25, ..., 0.  , 1.  , 0.25]], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(1, 21), dtype=float32, numpy=\n array([[0.25      , 0.25      , 0.5       , 0.        , 0.5       ,\n         0.5       , 0.5       , 0.25      , 0.5       , 0.5       ,\n         0.5       , 0.        , 0.5       , 0.25      , 0.5       ,\n         0.        , 0.5       , 0.16666667, 0.5       , 0.33333334,\n         0.5       ]], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(1, 21), dtype=float32, numpy=\n array([[0.25      , 0.25      , 0.5       , 0.        , 0.5       ,\n         0.5       , 0.5       , 0.25      , 0.5       , 0.5       ,\n         0.5       , 0.        , 0.5       , 0.25      , 0.5       ,\n         0.        , 0.5       , 0.16666667, 0.5       , 0.33333334,\n         0.5       ]], dtype=float32)&gt;]\n\n\n.aux_cols denotes all columns that are not features, targets or prediction columns.\n\nnum_dataf.aux_cols\n\n['era', 'data_type']\n\n\n\nnum_dataf.get_aux_data.head(2)\n\n\n\n\n\n\n\n\nera\ndata_type\n\n\nid\n\n\n\n\n\n\nn559bd06a8861222\n0297\ntrain\n\n\nn9d39dea58c9e3cf\n0003\ntrain\n\n\n\n\n\n\n\n\nnum_dataf['prediction_1'] = np.random.uniform(size=len(num_dataf))\nnum_dataf['prediction_2'] = np.random.uniform(size=len(num_dataf))\n\nTo track new columns like prediction columns, make sure to initialize a new NumerFrame. Prediction columns can easily be retrieved with .get_prediction_data and get_prediction_aux_data if you want to also get columns like era and data_type. This can be handy for ensembling and submission use cases.\n\nnum_dataf = NumerFrame(num_dataf)\n\n\nnum_dataf.get_prediction_data.head(2)\n\n\n\n\n\n\n\n\nprediction_1\nprediction_2\n\n\nid\n\n\n\n\n\n\nn559bd06a8861222\n0.087993\n0.729183\n\n\nn9d39dea58c9e3cf\n0.238604\n0.384513\n\n\n\n\n\n\n\n\nnum_dataf.get_prediction_aux_data.head(2)\n\n\n\n\n\n\n\n\nprediction_1\nprediction_2\nera\ndata_type\n\n\nid\n\n\n\n\n\n\n\n\nn559bd06a8861222\n0.087993\n0.729183\n0297\ntrain\n\n\nn9d39dea58c9e3cf\n0.238604\n0.384513\n0003\ntrain\n\n\n\n\n\n\n\n\nnum_dataf.meta\n\n{'era_col': 'era'}\n\n\nBecause NumerFrame inherits from pd.DataFrame you still have all functionality of a normal DataFrame at your disposal, like copying.\n\ndataf2 = num_dataf.copy()\nassert dataf2.equals(num_dataf)\n\nNumerFrame dynamically tracks which feature, target, aux and prediction columns there are when initialized. For example, here we add a new prediction column. Upon initialization the column will be contained in prediction_cols. Prediction columns are all column names that start with prediction.\n\nnum_dataf.loc[:, \"prediction_test_1\"] = np.random.uniform(size=len(num_dataf))\nnew_dataset = NumerFrame(num_dataf)\nassert \"prediction_test_1\" in new_dataset.prediction_cols\n\nArbitrary columns van be retrieved with .get_column_selection. The input argument can be either a string or a list with column names.\n\nselection1 = num_dataf.get_column_selection(\"era\")\nselection1.head(2)\n\n\n\n\n\n\n\n\nera\n\n\nid\n\n\n\n\n\nn559bd06a8861222\n0297\n\n\nn9d39dea58c9e3cf\n0003\n\n\n\n\n\n\n\n\nselection2 = num_dataf.get_column_selection([\"era\", \"prediction_test_1\"])\nselection2.head(2)\n\n\n\n\n\n\n\n\nera\nprediction_test_1\n\n\nid\n\n\n\n\n\n\nn559bd06a8861222\n0297\n0.952676\n\n\nn9d39dea58c9e3cf\n0003\n0.616081\n\n\n\n\n\n\n\nFor convenience we can get a feature, target pair with one method. If multi_target=True all columns where the column name starts with target will be retrieved.\n\nfeatures, single_target = num_dataf.get_feature_target_pair(multi_target=False)\nfeatures.head(2)\n\n\n\n\n\n\n\n\nfeature_dichasial_hammier_spawner\nfeature_rheumy_epistemic_prancer\nfeature_pert_performative_hormuz\nfeature_hillier_unpitied_theobromine\nfeature_perigean_bewitching_thruster\nfeature_renegade_undomestic_milord\nfeature_koranic_rude_corf\nfeature_demisable_expiring_millepede\nfeature_unscheduled_malignant_shingling\nfeature_clawed_unwept_adaptability\n...\nfeature_unpruned_pedagoguish_inkblot\nfeature_forworn_hask_haet\nfeature_drawable_exhortative_dispersant\nfeature_metabolic_minded_armorist\nfeature_investigatory_inerasable_circumvallation\nfeature_centroclinal_incentive_lancelet\nfeature_unemotional_quietistic_chirper\nfeature_behaviorist_microbiological_farina\nfeature_lofty_acceptable_challenge\nfeature_coactive_prefatorial_lucy\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn559bd06a8861222\n0.25\n0.75\n0.25\n0.75\n0.25\n0.50\n1.0\n0.25\n0.25\n0.75\n...\n0.75\n0.0\n1.00\n0.0\n0.0\n0.25\n0.00\n0.0\n1.00\n0.25\n\n\nn9d39dea58c9e3cf\n0.75\n0.50\n0.75\n1.00\n0.50\n0.25\n0.5\n0.00\n1.00\n0.25\n...\n1.00\n1.0\n0.25\n0.5\n0.0\n0.25\n0.75\n1.0\n0.75\n1.00\n\n\n\n\n2 rows × 1050 columns\n\n\n\n\nsingle_target.head(2)\n\n\n\n\n\n\n\n\ntarget\n\n\nid\n\n\n\n\n\nn559bd06a8861222\n0.25\n\n\nn9d39dea58c9e3cf\n0.50"
  },
  {
    "objectID": "modelpipeline.html",
    "href": "modelpipeline.html",
    "title": "ModelPipeline",
    "section": "",
    "text": "The functionality below uses the NumerFrame, PreProcessor, Model and PostProcessor objects to easily propagate data, generate predictions and postprocess them in one go.\nSpecifically, this section introduces two objects: 1. ModelPipeline: Run all preprocessing, models and postprocessing that you define and return a NumerFrame. 2. ModelPipelineCollection: Manage and run multiple ModelPipeline objects."
  },
  {
    "objectID": "modelpipeline.html#overview",
    "href": "modelpipeline.html#overview",
    "title": "ModelPipeline",
    "section": "",
    "text": "The functionality below uses the NumerFrame, PreProcessor, Model and PostProcessor objects to easily propagate data, generate predictions and postprocess them in one go.\nSpecifically, this section introduces two objects: 1. ModelPipeline: Run all preprocessing, models and postprocessing that you define and return a NumerFrame. 2. ModelPipelineCollection: Manage and run multiple ModelPipeline objects."
  },
  {
    "objectID": "modelpipeline.html#modelpipeline",
    "href": "modelpipeline.html#modelpipeline",
    "title": "ModelPipeline",
    "section": "1. ModelPipeline",
    "text": "1. ModelPipeline\nModelPipeline handles all preprocessing, model prediction and postprocessing. It returns a NumerFrame with the preprocessed data, metadata and postprocessed prediction columns.\n\nsource\n\nModelPipeline\n\n ModelPipeline (models:List[numerblox.model.BaseModel],\n                preprocessors:List[numerblox.preprocessing.BaseProcessor]=\n                [], postprocessors:List[numerblox.preprocessing.BaseProces\n                sor]=[], copy_first=True, standardize=True,\n                pipeline_name:str=None)\n\nExecute all preprocessing, prediction and postprocessing for a given setup.\n:param models: Initiliazed numerai-blocks Models (Objects inheriting from BaseModel)\n:param preprocessors: List of initialized Preprocessors.\n:param postprocessors: List of initialized Postprocessors.\n:param copy_first: Whether to copy the NumerFrame as a first preprocessing step.\nHighly recommended in order to avoid surprise behaviour by manipulating the original dataset.\n:param pipeline_name: Unique name for pipeline. Only used for display purposes.\nExample using several preprocessor, dummy models and postprocessors\n\nmodel_names = [\"test_0.5\", \"test_0.8\"]\n\ndataf = create_numerframe(\"test_assets/mini_numerai_version_2_data.parquet\")\npreprocessors = [FeatureSelectionPreProcessor(feature_cols=['feature_rheumy_epistemic_prancer'])]\nmodels = [ConstantModel(constant=0.5, model_name=model_names[0]), ConstantModel(constant=0.8, model_name=model_names[1])]\npostprocessors = [MeanEnsembler(cols=[f\"prediction_{name}\" for name in model_names], final_col_name='prediction_ensembled'),\n                  FeatureNeutralizer(feature_names=['feature_rheumy_epistemic_prancer'],\n                                     pred_name='prediction_ensembled', proportion=0.8)]\n\n\ntest_pipeline = ModelPipeline(preprocessors=preprocessors, models=models,\n                              postprocessors=postprocessors, pipeline_name=\"test_pipeline\",\n                              standardize=False)\nprocessed_dataf = test_pipeline(dataf)\n\n✅ Finished step CopyPreProcessor. Output shape=(10, 314). Time taken for step: 0:00:00.001927. ✅\n\n\n\n\n\n\n🚧 Applying preprocessing: 'GroupStatsPreProcessor' 🚧\n\n\n\n✅ Finished step GroupStatsPreProcessor. Output shape=(10, 332). Time taken for step: 0:00:00.045558. ✅\n\n\n\n🚧 Applying preprocessing: 'FeatureSelectionPreProcessor' 🚧\n\n\n\n✅ Finished step FeatureSelectionPreProcessor. Output shape=(10, 6). Time taken for step: 0:00:00.000785. ✅\n\n\n\n\n\n\n🤖 Generating model predictions with 'ConstantModel'. 🤖\n\n\n\n🤖 Generating model predictions with 'ConstantModel'. 🤖\n\n\n\n\n\n\n🚧 Applying postprocessing: 'MeanEnsembler' 🚧\n\n\n\n🍲 Ensembled '['prediction_test_0.5', 'prediction_test_0.8']' with simple mean and saved in 'prediction_ensembled' \n🍲\n\n\n\n✅ Finished step MeanEnsembler. Output shape=(10, 9). Time taken for step: 0:00:00.003600. ✅\n\n\n\n🚧 Applying postprocessing: 'FeatureNeutralizer' 🚧\n\n\n\n🤖 Neutralized 'prediction_ensembled' with proportion '0.8' 🤖\n\n\n\nNew neutralized column = 'prediction_ensembled_neutralized_0.8'.\n\n\n\n✅ Finished step FeatureNeutralizer. Output shape=(10, 10). Time taken for step: 0:00:00.015465. ✅\n\n\n\n🏁 Finished pipeline: 'test_pipeline'! 🏁\n\n\n\n\nassert processed_dataf.meta == dataf.meta\nassert isinstance(processed_dataf, NumerFrame)\nprocessed_dataf.head(2)\n\n\n\n\n\n\n\n\nfeature_intelligence_mean\nfeature_intelligence_std\ntarget\nid\nera\ndata_type\nprediction_test_0.5\nprediction_test_0.8\nprediction_ensembled\nprediction_ensembled_neutralized_0.8\n\n\n\n\n0\n0.333333\n0.246183\n0.50\nn000315175b67977\nera1\ntrain\n0.5\n0.8\n0.65\n0.00000\n\n\n1\n0.208333\n0.234359\n0.25\nn0014af834a96cdd\nera1\ntrain\n0.5\n0.8\n0.65\n0.36088"
  },
  {
    "objectID": "modelpipeline.html#modelpipelinecollection",
    "href": "modelpipeline.html#modelpipelinecollection",
    "title": "ModelPipeline",
    "section": "2. ModelPipelineCollection",
    "text": "2. ModelPipelineCollection\nModelPipelineCollection can be used to manage and run multiple ModelPipeline objects.\nModelPipelineCollection simply takes a list of ModelPipeline objects as input.\n\nsource\n\nModelPipelineCollection\n\n ModelPipelineCollection (pipelines:List[__main__.ModelPipeline])\n\nExecute multiple initialized ModelPipelines in a sequence.\n:param pipelines: List of initialized ModelPipelines.\nWe introduce a different pipeline with no preprocessing or postprocessing. Only a RandomModel.\n\ntest_pipeline2 = ModelPipeline(models=[RandomModel()], pipeline_name=\"test_pipeline2\")\n\nWe process two ModelPipelines with different characteristics on the same data.\n\ncollection = ModelPipelineCollection([test_pipeline, test_pipeline2])\nassert collection.get_pipeline(\"test_pipeline2\").pipeline_name == 'test_pipeline2'\n\n\nresult_datasets = collection(dataf=dataf)\n\n\n\n\n👷 Processing model pipeline: 'test_pipeline' 👷\n\n\n\n✅ Finished step CopyPreProcessor. Output shape=(10, 314). Time taken for step: 0:00:00.002404. ✅\n\n\n\n\n\n\n🚧 Applying preprocessing: 'GroupStatsPreProcessor' 🚧\n\n\n\n✅ Finished step GroupStatsPreProcessor. Output shape=(10, 332). Time taken for step: 0:00:00.026447. ✅\n\n\n\n🚧 Applying preprocessing: 'FeatureSelectionPreProcessor' 🚧\n\n\n\n✅ Finished step FeatureSelectionPreProcessor. Output shape=(10, 6). Time taken for step: 0:00:00.000721. ✅\n\n\n\n\n\n\n🤖 Generating model predictions with 'ConstantModel'. 🤖\n\n\n\n🤖 Generating model predictions with 'ConstantModel'. 🤖\n\n\n\n\n\n\n🚧 Applying postprocessing: 'MeanEnsembler' 🚧\n\n\n\n🍲 Ensembled '['prediction_test_0.5', 'prediction_test_0.8']' with simple mean and saved in 'prediction_ensembled' \n🍲\n\n\n\n✅ Finished step MeanEnsembler. Output shape=(10, 9). Time taken for step: 0:00:00.003288. ✅\n\n\n\n🚧 Applying postprocessing: 'FeatureNeutralizer' 🚧\n\n\n\n🤖 Neutralized 'prediction_ensembled' with proportion '0.8' 🤖\n\n\n\nNew neutralized column = 'prediction_ensembled_neutralized_0.8'.\n\n\n\n✅ Finished step FeatureNeutralizer. Output shape=(10, 10). Time taken for step: 0:00:00.012334. ✅\n\n\n\n🏁 Finished pipeline: 'test_pipeline'! 🏁\n\n\n\n👷 Processing model pipeline: 'test_pipeline2' 👷\n\n\n\n✅ Finished step CopyPreProcessor. Output shape=(10, 314). Time taken for step: 0:00:00.002063. ✅\n\n\n\n\n\n\n\n\n\n🤖 Generating model predictions with 'RandomModel'. 🤖\n\n\n\n✅ Finished step Standardizer. Output shape=(10, 315). Time taken for step: 0:00:00.004046. ✅\n\n\n\n\n\n\n🏁 Finished pipeline: 'test_pipeline2'! 🏁\n\n\n\nThe ModelPipelineCollection returns a dictionary mapping pipeline names to NumerFrame objects, retaining all metadata and added prediction columns for each. Note that in this example, the 1st NumerFrame had a feature selection step, so it did not retain all columns. However, the second dataset retained all feature columns, because no preprocessing was done.\n\nresult_datasets.keys()\n\ndict_keys(['test_pipeline', 'test_pipeline2'])\n\n\n\nresult_datasets['test_pipeline'].head(2)\n\n\n\n\n\n\n\n\nfeature_intelligence_mean\nfeature_intelligence_std\ntarget\nid\nera\ndata_type\nprediction_test_0.5\nprediction_test_0.8\nprediction_ensembled\nprediction_ensembled_neutralized_0.8\n\n\n\n\n0\n0.333333\n0.246183\n0.50\nn000315175b67977\nera1\ntrain\n0.5\n0.8\n0.65\n0.00000\n\n\n1\n0.208333\n0.234359\n0.25\nn0014af834a96cdd\nera1\ntrain\n0.5\n0.8\n0.65\n0.36088\n\n\n\n\n\n\n\n\nresult_datasets['test_pipeline2'].head(2)\n\n\n\n\n\n\n\n\nid\nera\ndata_type\nfeature_intelligence1\nfeature_intelligence2\nfeature_intelligence3\nfeature_intelligence4\nfeature_intelligence5\nfeature_intelligence6\nfeature_intelligence7\n...\nfeature_wisdom39\nfeature_wisdom40\nfeature_wisdom41\nfeature_wisdom42\nfeature_wisdom43\nfeature_wisdom44\nfeature_wisdom45\nfeature_wisdom46\ntarget\nprediction_random\n\n\n\n\n0\nn000315175b67977\nera1\ntrain\n0.0\n0.5\n0.25\n0.00\n0.5\n0.25\n0.25\n...\n1.0\n0.75\n0.5\n0.75\n0.50\n1.0\n0.50\n0.75\n0.50\n0.5\n\n\n1\nn0014af834a96cdd\nera1\ntrain\n0.0\n0.0\n0.00\n0.25\n0.5\n0.00\n0.00\n...\n1.0\n0.00\n0.0\n0.75\n0.25\n0.0\n0.25\n1.00\n0.25\n0.1\n\n\n\n\n2 rows × 315 columns"
  },
  {
    "objectID": "evaluation.html",
    "href": "evaluation.html",
    "title": "Evaluation",
    "section": "",
    "text": "This section provides evaluation schemes for both Numerai Classic and Signals. The Evaluator takes a NumerFrame as input and returns a Pandas DataFrame containing metrics for each given prediction column."
  },
  {
    "objectID": "evaluation.html#overview",
    "href": "evaluation.html#overview",
    "title": "Evaluation",
    "section": "",
    "text": "This section provides evaluation schemes for both Numerai Classic and Signals. The Evaluator takes a NumerFrame as input and returns a Pandas DataFrame containing metrics for each given prediction column."
  },
  {
    "objectID": "evaluation.html#base",
    "href": "evaluation.html#base",
    "title": "Evaluation",
    "section": "0. Base",
    "text": "0. Base\nBaseEvaluator implements all the evaluation logic that is common for Numerai Classic and Signals. This includes: - Mean, Standard Deviation and Sharpe for era returns. - Max drawdown - Annual Percentage Yield (APY) - Correlation with example predictions - Max feature exposure - Feature Neutral Mean (FNC), Standard deviation and Sharpe - Exposure Dissimilarity - Calmar Ratio - Mean, Standard Deviation and Sharpe for TB200 (Buy top 200 stocks and sell bottom 200 stocks). - Mean, Standard Deviation and Sharpe for TB500 (Buy top 500 stocks and sell bottom 500 stocks).\n\nsource\n\nBaseEvaluator\n\n BaseEvaluator (era_col:str='era', fast_mode=False)\n\nEvaluation functionality that is relevant for both Numerai Classic and Numerai Signals.\n:param era_col: Column name pointing to eras.\nMost commonly “era” for Numerai Classic and “friday_date” for Numerai Signals.\n:param fast_mode: Will skip compute intensive metrics if set to True, namely max_exposure, feature neutral mean, TB200 and TB500.\nNote that we calculate the sample standard deviation with ddof=0. It may differ slightly from the standard Pandas calculation, but is consistent with how NumPy computes standard deviation. More info: https://stackoverflow.com/questions/24984178/different-std-in-pandas-vs-numpy"
  },
  {
    "objectID": "evaluation.html#numerai-classic",
    "href": "evaluation.html#numerai-classic",
    "title": "Evaluation",
    "section": "1. Numerai Classic",
    "text": "1. Numerai Classic\nNumeraiClassicEvaluator extends the base evaluation scheme with metrics specific to Numerai Classic.\n\nsource\n\nNumeraiClassicEvaluator\n\n NumeraiClassicEvaluator (era_col:str='era', fast_mode=False)\n\nEvaluator for all metrics that are relevant in Numerai Classic."
  },
  {
    "objectID": "evaluation.html#numerai-signals",
    "href": "evaluation.html#numerai-signals",
    "title": "Evaluation",
    "section": "2. Numerai Signals",
    "text": "2. Numerai Signals\nNumeraiSignalsEvaluator extends the base evaluation scheme with metrics specific to Numerai Signals.\n\nsource\n\nNumeraiSignalsEvaluator\n\n NumeraiSignalsEvaluator (era_col:str='friday_date', fast_mode=False)\n\nEvaluator for all metrics that are relevant in Numerai Signals.\n\n\nExample usage\n\nNumeraiClassicEvaluator\nWe will test NumeraiClassicEvaluator on version v4.2 evaluation data with example predictions. The baseline reference (example_col) will be random predictions.\n\nfrom numerblox.download import NumeraiClassicDownloader\n\ndirectory = \"eval_test_1234321/\"\ndownloader = NumeraiClassicDownloader(directory_path=directory)\n\nNo existing directory found at 'eval_test_1234321'. Creating directory...\n\n\n\ndownloader.download_single_dataset(filename=\"v4.2/validation_int8.parquet\",\n                                   dest_path=directory + \"validation.parquet\")\ndownloader.download_single_dataset(filename=\"v4.2/validation_example_preds.parquet\",\n                                   dest_path=directory + \"validation_example_preds.parquet\")\n\n📁 Downloading 'v4.2/validation_int8.parquet' 📁\n\n\n\n2023-09-11 12:56:23,926 INFO numerapi.utils: starting download\neval_test_1234321/validation.parquet: 2.16GB [00:52, 41.1MB/s]                            \n2023-09-11 12:57:17,092 INFO numerapi.utils: starting download\neval_test_1234321/validation_example_preds.parquet: 60.7MB [00:04, 14.0MB/s]                            \n\n\n📁 Downloading 'v4.2/validation_example_preds.parquet' 📁\n\n\n\n\nnp.random.seed(1234)\ntest_dataf = create_numerframe(directory + \"validation.parquet\",\n                               columns=['era', 'data_type', 'feature_honoured_observational_balaamite',\n                                        'feature_polaroid_vadose_quinze', 'target', 'target_nomi_v4_20', 'target_nomi_v4_60', 'id'])\nexample_preds = pd.read_parquet(directory + \"validation_example_preds.parquet\")\n\n\ntest_dataf = test_dataf.merge(example_preds, on=\"id\", how=\"left\").reset_index()\ntest_dataf = test_dataf.sample(10_000, random_state=1234)\n\ntest_dataf.loc[:, \"prediction_random\"] = np.random.uniform(size=len(test_dataf))\ntest_dataf.head(2)\n\n\n\n\n\n\n\n\nid\nera\ndata_type\nfeature_honoured_observational_balaamite\nfeature_polaroid_vadose_quinze\ntarget\ntarget_nomi_v4_20\ntarget_nomi_v4_60\nprediction\nprediction_random\n\n\n\n\n2199551\nn3b8237d69852ab6\n1006\nvalidation\n1\n1\n0.75\n0.5\n0.5\n0.638587\n0.191519\n\n\n2116441\nnabfc4390f355fa4\n0990\nvalidation\n3\n1\n0.75\n0.5\n0.5\n0.088223\n0.622109\n\n\n\n\n\n\n\nThe Evaluator returns a Pandas DataFrame containing metrics for each prediction column defined. Note that any column can be used as example prediction. For practical use cases we recommend using proper example predictions (provided by Numerai) instead of random predictions.\n\n\nFast evaluation\nfast_mode skips max. feature exposure, feature neutral mean, FNCv3, Exposure Dissimilarity, TB200 and TB500 calculations, which can take a while to compute on full Numerai datasets.\n\nevaluator = NumeraiClassicEvaluator(fast_mode=True)\nval_stats_fast = evaluator.full_evaluation(\n    dataf=test_dataf,\n    target_col=\"target\",\n    pred_cols=[\"prediction\", \"prediction_random\"],\n    example_col=\"prediction_random\",\n)\nval_stats_fast\n\nWARNING: No suitable feature set defined for FNC. Skipping calculation of FNC.\n\n\n\n\n\n\n\n\n\n\n\n\ntarget\nmean\nstd\nsharpe\nmax_drawdown\napy\ncalmar_ratio\ncorr_with_example_preds\nlegacy_mean\nlegacy_std\nlegacy_sharpe\n\n\n\n\nprediction\ntarget\n0.016806\n0.245725\n0.068394\n-0.999953\n-6.785841\n-6.786159\n0.011225\n0.010627\n0.243476\n0.043646\n\n\nprediction_random\ntarget\n-0.003442\n0.236131\n-0.014578\n-1.000000\n-60.911901\n-60.911904\n0.981155\n-0.001949\n0.237189\n-0.008219\n\n\n\n\n\n\n\n\n\nFull evaluation\nThe full evaluation also computes the metrics from fast mode. Additionally, it computes max. feature exposure, feature neutral mean, FNCv3, Exposure Dissimilarity, TB200 and TB500 calculations. Note that this can take a long time when computing on the full dataset and using all features.\n\nevaluator = NumeraiClassicEvaluator(fast_mode=False)\nval_stats_full = evaluator.full_evaluation(\n    dataf=test_dataf,\n    target_col=\"target\",\n    pred_cols=[\"prediction\", \"prediction_random\"],\n    example_col=\"prediction_random\",\n)\nval_stats_full\n\nWARNING: No suitable feature set defined for FNC. Skipping calculation of FNC.\n\n\n\n\n\n🤖 Neutralized 'prediction' with proportion '1.0' 🤖\n\n\n\nNew neutralized column = 'prediction_neutralized_1.0'.\n\n\n\n✅ Finished step FeatureNeutralizer. Output shape=(10000, 11). Time taken for step: 0:00:02.280491. ✅\n\n\n\n🤖 Neutralized 'prediction_random' with proportion '1.0' 🤖\n\n\n\nNew neutralized column = 'prediction_random_neutralized_1.0'.\n\n\n\n✅ Finished step FeatureNeutralizer. Output shape=(10000, 12). Time taken for step: 0:00:02.307913. ✅\n\n\n\n\n\n\n\n\n\n\ntarget\nmean\nstd\nsharpe\nmax_drawdown\napy\ncalmar_ratio\ncorr_with_example_preds\nlegacy_mean\nlegacy_std\n...\nfeature_neutral_mean\nfeature_neutral_std\nfeature_neutral_sharpe\ntb200_mean\ntb200_std\ntb200_sharpe\ntb500_mean\ntb500_std\ntb500_sharpe\nexposure_dissimilarity\n\n\n\n\nprediction\ntarget\n0.016806\n0.245725\n0.068394\n-0.999953\n-6.785841\n-6.786159\n0.011225\n0.010627\n0.243476\n...\n0.015166\n0.246742\n0.061465\n0.009263\n0.241614\n0.038338\n0.009263\n0.241614\n0.038338\n-1.37308\n\n\nprediction_random\ntarget\n-0.003442\n0.236131\n-0.014578\n-1.000000\n-60.911901\n-60.911904\n0.981155\n-0.001949\n0.237189\n...\n-0.005719\n0.242857\n-0.023548\n0.002266\n0.236810\n0.009570\n0.002266\n0.236810\n0.009570\n0.00000\n\n\n\n\n2 rows × 22 columns\n\n\n\n\n\nPlot correlations\nThe plot_correlations method will use matplotlib to plot per era correlation scores over time. The plots default to a rolling window of 20 eras in order to best align with repuation scores as measured on the Numerai leaderboards.\n\nevaluator.plot_correlations(\n    test_dataf.fillna(0.5), pred_cols=[\"prediction\", \"prediction_random\"], roll_mean=20\n)\n\n\n\n\n\n\n\n\n# Clean up environment\ndownloader.remove_base_directory()\n\n⚠ Deleting directory for 'NumeraiClassicDownloader' ⚠\nPath: '/home/clepelaars/numerblox/nbs/eval_test_1234321'"
  },
  {
    "objectID": "misc.html",
    "href": "misc.html",
    "title": "Misc",
    "section": "",
    "text": "This class wraps dict functionality and extends it so all values can also be retrieved as attributes.\n\nsource\n\n\n\n AttrDict (*args, **kwargs)\n\nAccess dictionary elements as attributes.\n\ntest_dict = AttrDict({\"test1\": \"hello\", \"test2\": \"world\"})\ntest_dict.test1, test_dict['test2']"
  },
  {
    "objectID": "misc.html#attrdict",
    "href": "misc.html#attrdict",
    "title": "Misc",
    "section": "",
    "text": "This class wraps dict functionality and extends it so all values can also be retrieved as attributes.\n\nsource\n\n\n\n AttrDict (*args, **kwargs)\n\nAccess dictionary elements as attributes.\n\ntest_dict = AttrDict({\"test1\": \"hello\", \"test2\": \"world\"})\ntest_dict.test1, test_dict['test2']"
  }
]