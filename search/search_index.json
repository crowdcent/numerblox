{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#numerblox","title":"NumerBlox","text":"<p>NumerBlox offers components that help with developing strong Numerai models and inference pipelines. From downloading data to submitting predictions, NumerBlox has you covered.</p> <p>All components can be used standalone and all processors are fully compatible to use within scikit-learn pipelines.  </p>"},{"location":"#1-installation","title":"1. Installation","text":"<p>Install numerblox from PyPi by running:</p> <p><code>pip install numerblox</code></p> <p>Alternatively you can clone this repository and install it in development mode by installing using <code>poetry</code>:</p> <pre><code>git clone https://github.com/crowdcent/numerblox.git\npip install poetry\ncd numerblox\npoetry install\n</code></pre> <p>Installation without dev dependencies can be done by adding <code>--only main</code> to the <code>poetry install</code> line.</p> <p>Test your installation using one of the education notebooks in examples. Good places to start are quickstart.ipynb and numerframe_tutorial.ipynb. Run it in your Notebook environment to quickly test if your installation has succeeded. The documentation contains examples and explanations for each component of NumerBlox.</p>"},{"location":"#2-core-functionality","title":"2. Core functionality","text":"<p>NumerBlox has the following features for both Numerai Classic and Signals:</p> <p>Data Download: Automated retrieval of Numerai datasets.</p> <p>NumerFrame: A custom Pandas DataFrame for easier Numerai data manipulation.</p> <p>Preprocessors: Customizable techniques for data preprocessing.</p> <p>Target Engineering: Tools for creating new target variables.</p> <p>Postprocessors: Ensembling, neutralization, and penalization.</p> <p>MetaPipeline: An era-aware pipeline extension of scikit-learn's Pipeline. Specifically designed to integrate with era-specific Postprocessors such as neutralization and ensembling. Can be optionally bypassed for custom implementations.</p> <p>MetaEstimators: Era-aware estimators that extend scikit-learn's functionality. Includes features like CrossValEstimator which allow for era-specific, multiple-folds fitting seamlessly integrated into the pipeline.</p> <p>Evaluation: Comprehensive metrics aligned with Numerai's evaluation criteria.</p> <p>Submitters: Facilitates secure and easy submission of predictions.</p> <p>Example notebooks for each of these components can be found in the examples directory.</p>"},{"location":"#3-examples","title":"3. Examples","text":"<p>Below we will illustrate some common use cases in NumerBlox. To learn more in-depth about the features of this library, check out notebooks in examples.</p>"},{"location":"#31-downloading-numerai-classic-data","title":"3.1. Downloading Numerai Classic Data","text":"<p>NumeraiClassicDownloader allows you to download just the data you need with a few lines of code and handles the directory structure for you. All data from v4+ is supported. For Numerai Signals we provide downloaders from several sources for which you can find more information in the Downloaders section.</p> <pre><code>import pandas as pd\nfrom numerblox.download import NumeraiClassicDownloader\n\ndownloader = NumeraiClassicDownloader(\"data\")\n# Training and validation data\ndownloader.download_training_data(\"train_val\", version=\"4.2\", int8=True)\n# Live data\ndownloader.download_inference_data(\"current_round\", version=\"4.2\", int8=True)\ndf = pd.read_parquet(file_path=\"data/current_round/live.parquet\")\n</code></pre>"},{"location":"#32-core-numerframe-features","title":"3.2. Core NumerFrame features","text":"<p>NumerFrame is powerful data structure which simplifies working with Numerai data. Below are a few examples of how you can leverage NumerFrame for your Numerai workflow. Under the hood NumerFrame is a Pandas DataFrame so you still have access to all Pandas functionality when using NumerFrame.</p> <p>NumerFrame usage is completely optional. Other NumerBlox components do not depend on it, though they are compatible with it.</p> <pre><code>from numerblox.numerframe import create_numerframe\n\ndf = create_numerframe(file_path=\"data/current_round/live.parquet\")\n# Get data for features, targets and predictions\nfeatures = df.get_feature_data\ntargets = df.get_target_data\npredictions = df.get_prediction_data\n\n# Get specific data groups\nfncv3_features = df.get_fncv3_features\ngroup_features = df.get_group_features(group='rain')\n\n# Fetch columns by pattern. For example all 20 day targets.\npattern_data = df.get_pattern_data(pattern='_20')\n# Or for example Jerome targets.\njerome_targets = df.get_pattern_data(pattern='_jerome_')\n\n# Split into feature and target pairs. Will get single target by default.\nX, y = df.get_feature_target_pair()\n# Optionally get all targets\nX, y = df.get_feature_target_pair(multi_target=True)\n\n# Fetch data for specified eras\nX, y = df.get_era_batch(eras=['0001', '0002'])\n\n# Since every operation returns a NumerFrame they can be chained.\n# An example chained operation is getting features and targets for the last 2 eras.\nX, y = df.get_last_eras(2).get_feature_target_pair()\n</code></pre>"},{"location":"#33-advanced-numerai-models","title":"3.3. Advanced Numerai models","text":"<p>All core processors in <code>numerblox</code> are compatible with <code>scikit-learn</code> and therefore also <code>scikit-learn</code> extension libraries like scikit-lego, umap and scikit-llm. </p> <p>The example below illustrates its seamless integration with <code>scikit-learn</code>. Aside from core <code>scikit-learn</code> processors we use <code>ColumnSelector</code> from the scikit-lego extension library.</p> <p>For more examples check out the notebooks in the <code>examples</code> directory and the End-To-End Examples section.</p> <pre><code>import pandas as pd\nfrom xgboost import XGBRegressor\nfrom sklearn.pipeline import make_union\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklego.preprocessing import ColumnSelector\n\nfrom numerblox.meta import CrossValEstimator\nfrom numerblox.preprocessing import GroupStatsPreProcessor\nfrom numerblox.numerframe import create_numerframe\n\n# Easy data parsing with NumerFrame\ndf = create_numerframe(file_path=\"data/train_val/train_int8.parquet\")\nval_df = create_numerframe(file_path=\"data/train_val/validation_int8.parquet\")\n\nX, y = df.get_feature_target_pair()\neras = df.get_era_data()\n\nval_X, val_y = val_df.get_feature_target_pair()\nval_eras = val_df.get_era_data()\n\nfncv3_cols = nf.get_fncv3_features.columns.tolist()\n\n# Sunshine/Rain group statistics and FNCv3 features as model input\ngpp = GroupStatsPreProcessor(groups=['sunshine', 'rain'])\nfncv3_selector = ColumnSelector(fncv3_cols)\npreproc_pipe = make_union(gpp, fncv3_selector)\n\n# 5 fold cross validation with XGBoost as model\nmodel = CrossValEstimator(XGBRegressor(), cv=TimeSeriesSplit(n_splits=5))\n# Ensemble 5 folds with weighted average\nensembler = NumeraiEnsemble(donate_weighted=True)\n\nfull_pipe = make_pipeline(preproc_pipe, model, ensembler)\n\nfull_pipe.fit(X, y, numeraiensemble__eras=eras)\n\nval_preds = full_pipe.predict(val_X, eras=val_eras)\n</code></pre>"},{"location":"#34-evaluation","title":"3.4. Evaluation","text":"<p><code>NumeraiClassicEvaluator</code> and <code>NumeraiSignalsEvaluator</code> take care of computing all evaluation metrics for you. Below is a quick example of using it for Numerai Classic.  For more information on advanced usage and which metrics are computed check the Evaluators section.</p> <pre><code>from numerblox.evaluation import NumeraiClassicEvaluator\n\n# Validation DataFrame to compute metrics on\nval_df = ...\n\nevaluator = NumeraiClassicEvaluator()\nmetrics = evaluator.full_evaluation(val_df, \n                                    example_col=\"example_preds\", \n                                    pred_cols=[\"prediction\"], \n                                    target_col=\"target\")\n</code></pre>"},{"location":"#35-submission","title":"3.5. Submission","text":"<p>Submission for both Numerai Class and Signals can be done with a few lines of code. Here we illustrate an example for Numerai Classic. Check out the Submitters section in the documentation for more information.</p> <pre><code>from numerblox.misc import Key\nfrom numerblox.submission import NumeraiClassicSubmitter\n\nNUMERAI_PUBLIC_ID = \"YOUR_PUBLIC_ID\"\nNUMERAI_SECRET_KEY = \"YOUR_SECRET_KEY\"\n\n# Your predictions on the live data\npredictions = ...\n\n# Fill in you public and secret key for Numerai\nkey = Key(pub_id=NUMERAI_PUBLIC_ID, secret_key=NUMERAI_SECRET_KEY)\nsubmitter = NumeraiClassicSubmitter(directory_path=\"sub_current_round\", key=key)\n# full_submission checks contents, saves as csv and submits.\nsubmitter.full_submission(dataf=predictions,\n                          cols=\"prediction\",\n                          model_name=\"YOUR_MODEL_NAME\")\n# (optional) Clean up directory after submission\nsubmitter.remove_base_directory()\n</code></pre>"},{"location":"#4-contributing","title":"4. Contributing","text":"<p>Be sure to read the How To Contribute section for detailed instructions on contributing.</p> <p>If you have questions or want to discuss new ideas for NumerBlox, please create a Github issue first.</p>"},{"location":"#5-crediting-sources","title":"5. Crediting sources","text":"<p>Some of the components in this library may be based on forum posts, notebooks or ideas made public by the Numerai community. We have done our best to ask all parties who posted a specific piece of code for their permission and credit their work in the documentation. If your code is used in this library without credits, please let us know, so we can add a link to your article/code.</p> <p>If you are contributing to NumerBlox and are using ideas posted earlier by someone else, make sure to credit them by posting a link to their article/code in documentation.</p>"},{"location":"api/","title":"Full API Reference","text":"<p>This section provides a detailed reference to all objects defined in NumerBlox.</p>"},{"location":"api/#numerblox.download.BaseDownloader","title":"<code>BaseDownloader</code>","text":"<p>             Bases: <code>BaseIO</code></p> <p>Abstract base class for downloaders.</p> <p>:param directory_path: Base folder to download files to.</p> Source code in <code>numerblox/download.py</code> <pre><code>class BaseDownloader(BaseIO):\n    \"\"\"\n    Abstract base class for downloaders.\n\n    :param directory_path: Base folder to download files to.\n    \"\"\"\n    def __init__(self, directory_path: str):\n        super().__init__(directory_path=directory_path)\n\n    @abstractmethod\n    def download_training_data(self, *args, **kwargs):\n        \"\"\" Download all necessary files needed for training. \"\"\"\n        ...\n\n    @abstractmethod\n    def download_inference_data(self, *args, **kwargs):\n        \"\"\" Download minimal amount of files needed for weekly inference. \"\"\"\n        ...\n\n    @staticmethod\n    def _load_json(file_path: str, verbose=False, *args, **kwargs) -&gt; dict:\n        \"\"\" Load JSON from file and return as dictionary. \"\"\"\n        with open(Path(file_path)) as json_file:\n            json_data = json.load(json_file, *args, **kwargs)\n        if verbose:\n            print(json_data)\n        return json_data\n\n    def _default_save_path(self, start: dt, end: dt, backend: str):\n        \"\"\" Save to downloader directory indicating backend, start date and end date as parquet file. \"\"\"\n        return f\"{self.dir}/{backend}_{start.strftime('%Y%m%d')}_{end.strftime('%Y%m%d')}.parquet\"\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"\n        The most common use case will be to get weekly inference data. So calling the class itself returns inference data.\n        \"\"\"\n        self.download_inference_data(*args, **kwargs)\n</code></pre>"},{"location":"api/#numerblox.download.BaseDownloader.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>The most common use case will be to get weekly inference data. So calling the class itself returns inference data.</p> Source code in <code>numerblox/download.py</code> <pre><code>def __call__(self, *args, **kwargs):\n    \"\"\"\n    The most common use case will be to get weekly inference data. So calling the class itself returns inference data.\n    \"\"\"\n    self.download_inference_data(*args, **kwargs)\n</code></pre>"},{"location":"api/#numerblox.download.BaseDownloader.download_inference_data","title":"<code>download_inference_data(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Download minimal amount of files needed for weekly inference.</p> Source code in <code>numerblox/download.py</code> <pre><code>@abstractmethod\ndef download_inference_data(self, *args, **kwargs):\n    \"\"\" Download minimal amount of files needed for weekly inference. \"\"\"\n    ...\n</code></pre>"},{"location":"api/#numerblox.download.BaseDownloader.download_training_data","title":"<code>download_training_data(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Download all necessary files needed for training.</p> Source code in <code>numerblox/download.py</code> <pre><code>@abstractmethod\ndef download_training_data(self, *args, **kwargs):\n    \"\"\" Download all necessary files needed for training. \"\"\"\n    ...\n</code></pre>"},{"location":"api/#numerblox.download.BaseIO","title":"<code>BaseIO</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Basic functionality for IO (downloading and uploading).</p> <p>:param directory_path: Base folder for IO. Will be created if it does not exist.</p> Source code in <code>numerblox/download.py</code> <pre><code>class BaseIO(ABC):\n    \"\"\"\n    Basic functionality for IO (downloading and uploading).\n\n    :param directory_path: Base folder for IO. Will be created if it does not exist.\n    \"\"\"\n    def __init__(self, directory_path: str):\n        self.dir = Path(directory_path)\n        self._create_directory()\n\n    def remove_base_directory(self):\n        \"\"\"Remove directory with all contents.\"\"\"\n        abs_path = self.dir.resolve()\n        print(\n            f\"WARNING: Deleting directory for '{self.__class__.__name__}'\\nPath: '{abs_path}'\"\n        )\n        shutil.rmtree(abs_path)\n\n    def download_file_from_gcs(self, bucket_name: str, gcs_path: str):\n        \"\"\"\n        Get file from GCS bucket and download to local directory.\n        :param gcs_path: Path to file on GCS bucket.\n        \"\"\"\n        blob_path = str(self.dir.resolve())\n        blob = self._get_gcs_blob(bucket_name=bucket_name, blob_path=blob_path)\n        blob.download_to_filename(gcs_path)\n        print(\n            f\"Downloaded GCS object '{gcs_path}' from bucket '{blob.bucket.id}' to local directory '{blob_path}'.\"\n        )\n\n    def upload_file_to_gcs(self, bucket_name: str, gcs_path: str, local_path: str):\n        \"\"\"\n        Upload file to some GCS bucket.\n        :param gcs_path: Path to file on GCS bucket.\n        \"\"\"\n        blob = self._get_gcs_blob(bucket_name=bucket_name, blob_path=gcs_path)\n        blob.upload_from_filename(local_path)\n        print(\n            f\"Local file '{local_path}' uploaded to '{gcs_path}' in bucket {blob.bucket.id}\"\n        )\n\n    def download_directory_from_gcs(self, bucket_name: str, gcs_path: str):\n        \"\"\"\n        Copy full directory from GCS bucket to local environment.\n        :param gcs_path: Name of directory on GCS bucket.\n        \"\"\"\n        blob_path = str(self.dir.resolve())\n        blob = self._get_gcs_blob(bucket_name=bucket_name, blob_path=blob_path)\n        for gcs_file in glob.glob(gcs_path + \"/**\", recursive=True):\n            if os.path.isfile(gcs_file):\n                blob.download_to_filename(blob_path)\n        print(\n            f\"Directory '{gcs_path}' from bucket '{blob.bucket.id}' downloaded to '{blob_path}'\"\n        )\n\n    def upload_directory_to_gcs(self, bucket_name: str, gcs_path: str):\n        \"\"\"\n        Upload full base directory to GCS bucket.\n        :param gcs_path: Name of directory on GCS bucket.\n        \"\"\"\n        blob = self._get_gcs_blob(bucket_name=bucket_name, blob_path=gcs_path)\n        for local_path in glob.glob(str(self.dir) + \"/**\", recursive=True):\n            if os.path.isfile(local_path):\n                blob.upload_from_filename(local_path)\n        print(\n            f\"Directory '{self.dir}' uploaded to '{gcs_path}' in bucket {blob.bucket.id}\"\n        )\n\n    def _get_gcs_blob(self, bucket_name: str, blob_path: str) -&gt; storage.Blob:\n        \"\"\" Create blob that interacts with Google Cloud Storage (GCS). \"\"\"\n        client = storage.Client()\n        # https://console.cloud.google.com/storage/browser/[bucket_name]\n        bucket = client.get_bucket(bucket_name)\n        blob = bucket.blob(blob_path)\n        return blob\n\n    def _append_folder(self, folder: str) -&gt; Path:\n        \"\"\"\n        Return base directory Path object appended with 'folder'.\n        Create directory if it does not exist.\n        \"\"\"\n        dir = Path(self.dir / folder)\n        dir.mkdir(parents=True, exist_ok=True)\n        return dir\n\n    def _create_directory(self):\n        \"\"\" Create base directory if it does not exist. \"\"\"\n        if not self.dir.is_dir():\n            print(\n                f\"No existing directory found at '{self.dir}'. Creating directory...\"\n            )\n            self.dir.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def get_all_files(self) -&gt; list:\n        \"\"\" Return all paths of contents in directory. \"\"\"\n        return list(self.dir.iterdir())\n\n    @property\n    def is_empty(self) -&gt; bool:\n        \"\"\" Check if directory is empty. \"\"\"\n        return not bool(self.get_all_files)\n</code></pre>"},{"location":"api/#numerblox.download.BaseIO.get_all_files","title":"<code>get_all_files: list</code>  <code>property</code>","text":"<p>Return all paths of contents in directory.</p>"},{"location":"api/#numerblox.download.BaseIO.is_empty","title":"<code>is_empty: bool</code>  <code>property</code>","text":"<p>Check if directory is empty.</p>"},{"location":"api/#numerblox.download.BaseIO.download_directory_from_gcs","title":"<code>download_directory_from_gcs(bucket_name, gcs_path)</code>","text":"<p>Copy full directory from GCS bucket to local environment. :param gcs_path: Name of directory on GCS bucket.</p> Source code in <code>numerblox/download.py</code> <pre><code>def download_directory_from_gcs(self, bucket_name: str, gcs_path: str):\n    \"\"\"\n    Copy full directory from GCS bucket to local environment.\n    :param gcs_path: Name of directory on GCS bucket.\n    \"\"\"\n    blob_path = str(self.dir.resolve())\n    blob = self._get_gcs_blob(bucket_name=bucket_name, blob_path=blob_path)\n    for gcs_file in glob.glob(gcs_path + \"/**\", recursive=True):\n        if os.path.isfile(gcs_file):\n            blob.download_to_filename(blob_path)\n    print(\n        f\"Directory '{gcs_path}' from bucket '{blob.bucket.id}' downloaded to '{blob_path}'\"\n    )\n</code></pre>"},{"location":"api/#numerblox.download.BaseIO.download_file_from_gcs","title":"<code>download_file_from_gcs(bucket_name, gcs_path)</code>","text":"<p>Get file from GCS bucket and download to local directory. :param gcs_path: Path to file on GCS bucket.</p> Source code in <code>numerblox/download.py</code> <pre><code>def download_file_from_gcs(self, bucket_name: str, gcs_path: str):\n    \"\"\"\n    Get file from GCS bucket and download to local directory.\n    :param gcs_path: Path to file on GCS bucket.\n    \"\"\"\n    blob_path = str(self.dir.resolve())\n    blob = self._get_gcs_blob(bucket_name=bucket_name, blob_path=blob_path)\n    blob.download_to_filename(gcs_path)\n    print(\n        f\"Downloaded GCS object '{gcs_path}' from bucket '{blob.bucket.id}' to local directory '{blob_path}'.\"\n    )\n</code></pre>"},{"location":"api/#numerblox.download.BaseIO.remove_base_directory","title":"<code>remove_base_directory()</code>","text":"<p>Remove directory with all contents.</p> Source code in <code>numerblox/download.py</code> <pre><code>def remove_base_directory(self):\n    \"\"\"Remove directory with all contents.\"\"\"\n    abs_path = self.dir.resolve()\n    print(\n        f\"WARNING: Deleting directory for '{self.__class__.__name__}'\\nPath: '{abs_path}'\"\n    )\n    shutil.rmtree(abs_path)\n</code></pre>"},{"location":"api/#numerblox.download.BaseIO.upload_directory_to_gcs","title":"<code>upload_directory_to_gcs(bucket_name, gcs_path)</code>","text":"<p>Upload full base directory to GCS bucket. :param gcs_path: Name of directory on GCS bucket.</p> Source code in <code>numerblox/download.py</code> <pre><code>def upload_directory_to_gcs(self, bucket_name: str, gcs_path: str):\n    \"\"\"\n    Upload full base directory to GCS bucket.\n    :param gcs_path: Name of directory on GCS bucket.\n    \"\"\"\n    blob = self._get_gcs_blob(bucket_name=bucket_name, blob_path=gcs_path)\n    for local_path in glob.glob(str(self.dir) + \"/**\", recursive=True):\n        if os.path.isfile(local_path):\n            blob.upload_from_filename(local_path)\n    print(\n        f\"Directory '{self.dir}' uploaded to '{gcs_path}' in bucket {blob.bucket.id}\"\n    )\n</code></pre>"},{"location":"api/#numerblox.download.BaseIO.upload_file_to_gcs","title":"<code>upload_file_to_gcs(bucket_name, gcs_path, local_path)</code>","text":"<p>Upload file to some GCS bucket. :param gcs_path: Path to file on GCS bucket.</p> Source code in <code>numerblox/download.py</code> <pre><code>def upload_file_to_gcs(self, bucket_name: str, gcs_path: str, local_path: str):\n    \"\"\"\n    Upload file to some GCS bucket.\n    :param gcs_path: Path to file on GCS bucket.\n    \"\"\"\n    blob = self._get_gcs_blob(bucket_name=bucket_name, blob_path=gcs_path)\n    blob.upload_from_filename(local_path)\n    print(\n        f\"Local file '{local_path}' uploaded to '{gcs_path}' in bucket {blob.bucket.id}\"\n    )\n</code></pre>"},{"location":"api/#numerblox.download.EODDownloader","title":"<code>EODDownloader</code>","text":"<p>             Bases: <code>BaseDownloader</code></p> <p>Download data from EOD historical data. </p> <p>More info: https://eodhistoricaldata.com/</p> <p>Make sure you have the underlying Python package installed. <code>pip install eod</code>.</p> <p>:param directory_path: Base folder to download files to. </p> <p>:param key: Valid EOD client key. </p> <p>:param tickers: List of valid EOD tickers (Bloomberg ticker format). </p> <p>:param frequency: Choose from [d, w, m]. </p> <p>Daily data by default.</p> Source code in <code>numerblox/download.py</code> <pre><code>class EODDownloader(BaseDownloader):\n    \"\"\"\n    Download data from EOD historical data. \\n\n    More info: https://eodhistoricaldata.com/\n\n    Make sure you have the underlying Python package installed.\n    `pip install eod`.\n\n    :param directory_path: Base folder to download files to. \\n\n    :param key: Valid EOD client key. \\n\n    :param tickers: List of valid EOD tickers (Bloomberg ticker format). \\n\n    :param frequency: Choose from [d, w, m]. \\n\n    Daily data by default.\n    \"\"\"\n    def __init__(self,\n                 directory_path: str,\n                 key: str,\n                 tickers: list,\n                 frequency: str = \"d\"):\n        super().__init__(directory_path=directory_path)\n        self.key = key\n        self.tickers = tickers\n        try: \n            from eod import EodHistoricalData\n        except ImportError:\n            raise ImportError(\"Could not import eod package. Please install eod package with 'pip install eod'\")\n        self.client = EodHistoricalData(self.key)\n        self.frequency = frequency\n        self.current_time = dt.now()\n        self.end_date = self.current_time.strftime(\"%Y-%m-%d\")\n        self.cpu_count = os.cpu_count()\n        # Time to sleep in between API calls to avoid hitting EOD rate limits.\n        # EOD rate limit is set at 1000 calls per minute.\n        self.sleep_time = self.cpu_count / 32\n\n    def download_inference_data(self):\n        \"\"\" Download one year of data for defined tickers. \"\"\"\n        start = (pd.Timestamp(self.current_time) - relativedelta(years=1)).strftime(\"%Y-%m-%d\")\n        dataf = self.get_live_data(start=start)\n        dataf.to_parquet(self._default_save_path(start=pd.Timestamp(start),\n                                                 end=pd.Timestamp(self.end_date),\n                                                 backend=\"eod\"))\n\n    def download_training_data(self, start: str = None):\n        \"\"\"\n        Download full date length available.\n        start: Starting data in %Y-%m-%d format.\n        \"\"\"\n        start = start if start else \"1970-01-01\"\n        dataf = self.generate_full_dataf(start=start)\n        dataf.to_parquet(self._default_save_path(start=pd.Timestamp(start),\n                                                 end=pd.Timestamp(self.end_date),\n                                                 backend=\"eod\"))\n\n    def get_live_data(self, start: str) -&gt; NumerFrame:\n        \"\"\"\n        Get NumerFrame data from some starting date.\n        start: Starting data in %Y-%m-%d format.\n        \"\"\"\n        dataf = self.generate_full_dataf(start=start)\n        return NumerFrame(dataf)\n\n    def generate_full_dataf(self, start: str) -&gt; pd.DataFrame:\n        \"\"\"\n        Collect all price data for list of EOD ticker symbols (Bloomberg tickers).\n        start: Starting data in %Y-%m-%d format.\n        \"\"\"\n        price_datafs = []\n        with ThreadPoolExecutor(max_workers=self.cpu_count) as executor:\n            tasks = [executor.submit(self.generate_stock_dataf, ticker, start) for ticker in self.tickers]\n            for task in tqdm(concurrent.futures.as_completed(tasks),\n                             total=len(self.tickers),\n                             desc=\"EOD price data extraction\"):\n                price_datafs.append(task.result())\n        return pd.concat(price_datafs)\n\n    def generate_stock_dataf(self, ticker: str, start: str) -&gt; pd.DataFrame:\n        \"\"\"\n        Generate Price DataFrame for a single ticker.\n        ticker: EOD ticker symbol (Bloomberg tickers).\n        For example, Apple stock = AAPL.US.\n        start: Starting data in %Y-%m-%d format.\n        \"\"\"\n        time.sleep(self.sleep_time)\n        try:\n            resp = self.client.get_prices_eod(ticker, period=self.frequency,\n                                              from_=start, to=self.end_date)\n            stock_df = pd.DataFrame(resp).set_index('date')\n            stock_df['ticker'] = ticker\n        except Exception as e:\n            print(f\"WARNING: Date pull failed on ticker: '{ticker}'. Exception: {e}\")\n            stock_df = pd.DataFrame()\n        return stock_df\n</code></pre>"},{"location":"api/#numerblox.download.EODDownloader.download_inference_data","title":"<code>download_inference_data()</code>","text":"<p>Download one year of data for defined tickers.</p> Source code in <code>numerblox/download.py</code> <pre><code>def download_inference_data(self):\n    \"\"\" Download one year of data for defined tickers. \"\"\"\n    start = (pd.Timestamp(self.current_time) - relativedelta(years=1)).strftime(\"%Y-%m-%d\")\n    dataf = self.get_live_data(start=start)\n    dataf.to_parquet(self._default_save_path(start=pd.Timestamp(start),\n                                             end=pd.Timestamp(self.end_date),\n                                             backend=\"eod\"))\n</code></pre>"},{"location":"api/#numerblox.download.EODDownloader.download_training_data","title":"<code>download_training_data(start=None)</code>","text":"<p>Download full date length available. start: Starting data in %Y-%m-%d format.</p> Source code in <code>numerblox/download.py</code> <pre><code>def download_training_data(self, start: str = None):\n    \"\"\"\n    Download full date length available.\n    start: Starting data in %Y-%m-%d format.\n    \"\"\"\n    start = start if start else \"1970-01-01\"\n    dataf = self.generate_full_dataf(start=start)\n    dataf.to_parquet(self._default_save_path(start=pd.Timestamp(start),\n                                             end=pd.Timestamp(self.end_date),\n                                             backend=\"eod\"))\n</code></pre>"},{"location":"api/#numerblox.download.EODDownloader.generate_full_dataf","title":"<code>generate_full_dataf(start)</code>","text":"<p>Collect all price data for list of EOD ticker symbols (Bloomberg tickers). start: Starting data in %Y-%m-%d format.</p> Source code in <code>numerblox/download.py</code> <pre><code>def generate_full_dataf(self, start: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Collect all price data for list of EOD ticker symbols (Bloomberg tickers).\n    start: Starting data in %Y-%m-%d format.\n    \"\"\"\n    price_datafs = []\n    with ThreadPoolExecutor(max_workers=self.cpu_count) as executor:\n        tasks = [executor.submit(self.generate_stock_dataf, ticker, start) for ticker in self.tickers]\n        for task in tqdm(concurrent.futures.as_completed(tasks),\n                         total=len(self.tickers),\n                         desc=\"EOD price data extraction\"):\n            price_datafs.append(task.result())\n    return pd.concat(price_datafs)\n</code></pre>"},{"location":"api/#numerblox.download.EODDownloader.generate_stock_dataf","title":"<code>generate_stock_dataf(ticker, start)</code>","text":"<p>Generate Price DataFrame for a single ticker. ticker: EOD ticker symbol (Bloomberg tickers). For example, Apple stock = AAPL.US. start: Starting data in %Y-%m-%d format.</p> Source code in <code>numerblox/download.py</code> <pre><code>def generate_stock_dataf(self, ticker: str, start: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate Price DataFrame for a single ticker.\n    ticker: EOD ticker symbol (Bloomberg tickers).\n    For example, Apple stock = AAPL.US.\n    start: Starting data in %Y-%m-%d format.\n    \"\"\"\n    time.sleep(self.sleep_time)\n    try:\n        resp = self.client.get_prices_eod(ticker, period=self.frequency,\n                                          from_=start, to=self.end_date)\n        stock_df = pd.DataFrame(resp).set_index('date')\n        stock_df['ticker'] = ticker\n    except Exception as e:\n        print(f\"WARNING: Date pull failed on ticker: '{ticker}'. Exception: {e}\")\n        stock_df = pd.DataFrame()\n    return stock_df\n</code></pre>"},{"location":"api/#numerblox.download.EODDownloader.get_live_data","title":"<code>get_live_data(start)</code>","text":"<p>Get NumerFrame data from some starting date. start: Starting data in %Y-%m-%d format.</p> Source code in <code>numerblox/download.py</code> <pre><code>def get_live_data(self, start: str) -&gt; NumerFrame:\n    \"\"\"\n    Get NumerFrame data from some starting date.\n    start: Starting data in %Y-%m-%d format.\n    \"\"\"\n    dataf = self.generate_full_dataf(start=start)\n    return NumerFrame(dataf)\n</code></pre>"},{"location":"api/#numerblox.download.KaggleDownloader","title":"<code>KaggleDownloader</code>","text":"<p>             Bases: <code>BaseDownloader</code></p> <p>Download financial data from Kaggle.</p> <p>For authentication, make sure you have a directory called .kaggle in your home directory with therein a kaggle.json file. kaggle.json should have the following structure: </p> <p><code>{\"username\": USERNAME, \"key\": KAGGLE_API_KEY}</code> </p> <p>More info on authentication: github.com/Kaggle/kaggle-api#api-credentials </p> <p>More info on the Kaggle Python API: kaggle.com/donkeys/kaggle-python-api </p> <p>:param directory_path: Base folder to download files to.</p> Source code in <code>numerblox/download.py</code> <pre><code>class KaggleDownloader(BaseDownloader):\n    \"\"\"\n    Download financial data from Kaggle.\n\n    For authentication, make sure you have a directory called .kaggle in your home directory\n    with therein a kaggle.json file. kaggle.json should have the following structure: \\n\n    `{\"username\": USERNAME, \"key\": KAGGLE_API_KEY}` \\n\n    More info on authentication: github.com/Kaggle/kaggle-api#api-credentials \\n\n\n    More info on the Kaggle Python API: kaggle.com/donkeys/kaggle-python-api \\n\n\n    :param directory_path: Base folder to download files to.\n    \"\"\"\n    def __init__(self, directory_path: str):\n        self.__check_kaggle_import()\n        super().__init__(directory_path=directory_path)\n\n    def download_inference_data(self, kaggle_dataset_path: str):\n        \"\"\"\n        Download arbitrary Kaggle dataset.\n        :param kaggle_dataset_path: Path on Kaggle (URL slug on kaggle.com/)\n        \"\"\"\n        self.download_training_data(kaggle_dataset_path)\n\n    def download_training_data(self, kaggle_dataset_path: str):\n        \"\"\"\n        Download arbitrary Kaggle dataset.\n        :param kaggle_dataset_path: Path on Kaggle (URL slug on kaggle.com/)\n        \"\"\"\n        import kaggle\n        kaggle.api.dataset_download_files(kaggle_dataset_path,\n                                          path=self.dir, unzip=True)\n\n    @staticmethod\n    def __check_kaggle_import():\n        try:\n            import kaggle\n        except OSError:\n            raise OSError(\"Could not find kaggle.json credentials. Make sure it's located in /home/runner/.kaggle. Or use the environment method. Check github.com/Kaggle/kaggle-api#api-credentials for more information on authentication.\")\n</code></pre>"},{"location":"api/#numerblox.download.KaggleDownloader.download_inference_data","title":"<code>download_inference_data(kaggle_dataset_path)</code>","text":"<p>Download arbitrary Kaggle dataset. :param kaggle_dataset_path: Path on Kaggle (URL slug on kaggle.com/)</p> Source code in <code>numerblox/download.py</code> <pre><code>def download_inference_data(self, kaggle_dataset_path: str):\n    \"\"\"\n    Download arbitrary Kaggle dataset.\n    :param kaggle_dataset_path: Path on Kaggle (URL slug on kaggle.com/)\n    \"\"\"\n    self.download_training_data(kaggle_dataset_path)\n</code></pre>"},{"location":"api/#numerblox.download.KaggleDownloader.download_training_data","title":"<code>download_training_data(kaggle_dataset_path)</code>","text":"<p>Download arbitrary Kaggle dataset. :param kaggle_dataset_path: Path on Kaggle (URL slug on kaggle.com/)</p> Source code in <code>numerblox/download.py</code> <pre><code>def download_training_data(self, kaggle_dataset_path: str):\n    \"\"\"\n    Download arbitrary Kaggle dataset.\n    :param kaggle_dataset_path: Path on Kaggle (URL slug on kaggle.com/)\n    \"\"\"\n    import kaggle\n    kaggle.api.dataset_download_files(kaggle_dataset_path,\n                                      path=self.dir, unzip=True)\n</code></pre>"},{"location":"api/#numerblox.download.NumeraiClassicDownloader","title":"<code>NumeraiClassicDownloader</code>","text":"<p>             Bases: <code>BaseDownloader</code></p> <p>WARNING: Versions 1-3 (legacy data) are deprecated. Only supporting version 4+.</p> <p>Downloading from NumerAPI for Numerai Classic data. </p> <p>:param directory_path: Base folder to download files to. </p> <p>All args, *kwargs will be passed to NumerAPI initialization.</p> Source code in <code>numerblox/download.py</code> <pre><code>class NumeraiClassicDownloader(BaseDownloader):\n    \"\"\"\n    WARNING: Versions 1-3 (legacy data) are deprecated. Only supporting version 4+.\n\n    Downloading from NumerAPI for Numerai Classic data. \\n\n    :param directory_path: Base folder to download files to. \\n\n    All *args, **kwargs will be passed to NumerAPI initialization.\n    \"\"\"\n    TRAIN_DATASET_NAME = \"train_int8.parquet\"\n    VALIDATION_DATASET_NAME = \"validation_int8.parquet\"\n    LIVE_DATASET_NAME = \"live_int8.parquet\"\n    LIVE_EXAMPLE_PREDS_NAME = \"live_example_preds.parquet\"\n    VALIDATION_EXAMPLE_PREDS_NAME = \"validation_example_preds.parquet\"\n\n    def __init__(self, directory_path: str, *args, **kwargs):\n        super().__init__(directory_path=directory_path)\n        self.napi = NumerAPI(*args, **kwargs)\n        self.current_round = self.napi.get_current_round()\n        # Get all available versions available for Numerai.\n        self.dataset_versions = set(s.split(\"/\")[0] for s in NumerAPI().list_datasets())\n        self.dataset_versions.discard(\"signals\")\n\n    def download_training_data(\n        self, subfolder: str = \"\", version: str = \"4.3\"\n    ):\n        \"\"\"\n        Get Numerai classic training and validation data.\n        :param subfolder: Specify folder to create folder within base directory root.\n        Saves in base directory root by default.\n        :param version: Numerai dataset version.\n        4 = April 2022 dataset\n        4.1 = Sunshine dataset\n        4.2 (default) = Rain Dataset\n        4.3 = Midnight dataset\n        \"\"\"\n        self._check_dataset_version(version)\n        train_val_files = [f\"v{version}/{self.TRAIN_DATASET_NAME}\",\n                           f\"v{version}/{self.VALIDATION_DATASET_NAME}\"]\n        for file in train_val_files:\n            dest_path = self.__get_dest_path(subfolder, file)\n            self.download_single_dataset(\n                filename=file,\n                dest_path=dest_path\n            )\n\n    def download_inference_data(\n        self,\n        subfolder: str = \"\",\n        version: str = \"4.3\",\n        round_num: int = None,\n    ):\n        \"\"\"\n        Get Numerai classic inference (tournament) data.\n        :param subfolder: Specify folder to create folder within base directory root.\n        Saves in base directory root by default.\n        :param version: Numerai dataset version.\n        4 = April 2022 dataset\n        4.1 = Sunshine dataset\n        4.2 (default) = Rain Dataset\n        4.3 = Midnight dataset\n        :param round_num: Numerai tournament round number. Downloads latest round by default.\n        \"\"\"\n        self.download_live_data(subfolder=subfolder, version=version, round_num=round_num)\n\n    def download_single_dataset(\n        self, filename: str, dest_path: str, round_num: int = None\n    ):\n        \"\"\"\n        Download one of the available datasets through NumerAPI.\n\n        :param filename: Name as listed in NumerAPI (Check NumerAPI().list_datasets() for full overview)\n        :param dest_path: Full path where file will be saved.\n        :param round_num: Numerai tournament round number. Downloads latest round by default.\n        \"\"\"\n        print(\n            f\"Downloading '{filename}'.\"\n        )\n        self.napi.download_dataset(\n            filename=filename,\n            dest_path=dest_path,\n            round_num=round_num\n        )\n\n    def download_live_data(\n            self,\n            subfolder: str = \"\",\n            version: str = \"4.3\",\n            round_num: int = None\n    ):\n        \"\"\"\n        Download all live data in specified folder for given version (i.e. minimal data needed for inference).\n\n        :param subfolder: Specify folder to create folder within directory root.\n        Saves in directory root by default.\n        :param version: Numerai dataset version. \n        4 = April 2022 dataset\n        4.1 = Sunshine dataset\n        4.2 (default) = Rain Dataset\n        4.3 = Midnight dataset\n        :param round_num: Numerai tournament round number. Downloads latest round by default.\n        \"\"\"\n        self._check_dataset_version(version)\n        live_files = [f\"v{version}/{self.LIVE_DATASET_NAME}\"]\n        for file in live_files:\n            dest_path = self.__get_dest_path(subfolder, file)\n            self.download_single_dataset(\n                filename=file,\n                dest_path=dest_path,\n                round_num=round_num\n            )\n\n    def download_example_data(\n        self, subfolder: str = \"\", version: str = \"4.3\", round_num: int = None\n    ):\n        \"\"\"\n        Download all example prediction data in specified folder for given version.\n\n        :param subfolder: Specify folder to create folder within base directory root.\n        Saves in base directory root by default.\n        :param version: Numerai dataset version.\n        4 = April 2022 dataset\n        4.1 = Sunshine dataset\n        4.2 (default) = Rain Dataset\n        4.3 = Midnight dataset\n        :param round_num: Numerai tournament round number. Downloads latest round by default.\n        \"\"\"\n        self._check_dataset_version(version)\n        example_files = [f\"v{version}/{self.LIVE_EXAMPLE_PREDS_NAME}\", \n                         f\"v{version}/{self.VALIDATION_EXAMPLE_PREDS_NAME}\"]\n        for file in example_files:\n            dest_path = self.__get_dest_path(subfolder, file)\n            self.download_single_dataset(\n                filename=file,\n                dest_path=dest_path,\n                round_num=round_num\n            )\n\n    def get_classic_features(self, subfolder: str = \"\", filename=\"v4.3/features.json\", *args, **kwargs) -&gt; dict:\n        \"\"\"\n        Download feature overview (stats and feature sets) through NumerAPI and load as dict.\n        :param subfolder: Specify folder to create folder within base directory root.\n        Saves in base directory root by default.\n        :param filename: name for feature overview.\n        *args, **kwargs will be passed to the JSON loader.\n        :return: Feature overview dict\n        \"\"\"\n        version = filename.split(\"/\")[0].replace(\"v\", \"\")\n        self._check_dataset_version(version)\n        dest_path = self.__get_dest_path(subfolder, filename)\n        self.download_single_dataset(filename=filename,\n                                     dest_path=dest_path)\n        json_data = self._load_json(dest_path, *args, **kwargs)\n        return json_data\n\n    def download_meta_model_preds(self, subfolder: str = \"\", filename=\"v4.3/meta_model.parquet\") -&gt; pd.DataFrame:\n        \"\"\"\n        Download Meta model predictions through NumerAPI.\n        :param subfolder: Specify folder to create folder within base directory root.\n        Saves in base directory root by default.\n        :param filename: name for meta model predictions file.\n        :return: Meta model predictions as DataFrame.\n        \"\"\"\n        version = filename.split(\"/\")[0].replace(\"v\", \"\")\n        self._check_dataset_version(version)\n        dest_path = self.__get_dest_path(subfolder, filename)\n        self.download_single_dataset(\n            filename=filename,\n            dest_path=dest_path,\n            )\n        return pd.read_parquet(dest_path)\n\n    def __get_dest_path(self, subfolder: str, filename: str) -&gt; str:\n        \"\"\" Prepare destination path for downloading. \"\"\"\n        dir = self._append_folder(subfolder)\n        dest_path = str(dir.joinpath(filename.split(\"/\")[-1]))\n        return dest_path\n\n    def _check_dataset_version(self, version: str):\n        assert f\"v{version}\" in self.dataset_versions, f\"Version '{version}' is not available in NumerAPI.\"\n</code></pre>"},{"location":"api/#numerblox.download.NumeraiClassicDownloader.__get_dest_path","title":"<code>__get_dest_path(subfolder, filename)</code>","text":"<p>Prepare destination path for downloading.</p> Source code in <code>numerblox/download.py</code> <pre><code>def __get_dest_path(self, subfolder: str, filename: str) -&gt; str:\n    \"\"\" Prepare destination path for downloading. \"\"\"\n    dir = self._append_folder(subfolder)\n    dest_path = str(dir.joinpath(filename.split(\"/\")[-1]))\n    return dest_path\n</code></pre>"},{"location":"api/#numerblox.download.NumeraiClassicDownloader.download_example_data","title":"<code>download_example_data(subfolder='', version='4.3', round_num=None)</code>","text":"<p>Download all example prediction data in specified folder for given version.</p> <p>:param subfolder: Specify folder to create folder within base directory root. Saves in base directory root by default. :param version: Numerai dataset version. 4 = April 2022 dataset 4.1 = Sunshine dataset 4.2 (default) = Rain Dataset 4.3 = Midnight dataset :param round_num: Numerai tournament round number. Downloads latest round by default.</p> Source code in <code>numerblox/download.py</code> <pre><code>def download_example_data(\n    self, subfolder: str = \"\", version: str = \"4.3\", round_num: int = None\n):\n    \"\"\"\n    Download all example prediction data in specified folder for given version.\n\n    :param subfolder: Specify folder to create folder within base directory root.\n    Saves in base directory root by default.\n    :param version: Numerai dataset version.\n    4 = April 2022 dataset\n    4.1 = Sunshine dataset\n    4.2 (default) = Rain Dataset\n    4.3 = Midnight dataset\n    :param round_num: Numerai tournament round number. Downloads latest round by default.\n    \"\"\"\n    self._check_dataset_version(version)\n    example_files = [f\"v{version}/{self.LIVE_EXAMPLE_PREDS_NAME}\", \n                     f\"v{version}/{self.VALIDATION_EXAMPLE_PREDS_NAME}\"]\n    for file in example_files:\n        dest_path = self.__get_dest_path(subfolder, file)\n        self.download_single_dataset(\n            filename=file,\n            dest_path=dest_path,\n            round_num=round_num\n        )\n</code></pre>"},{"location":"api/#numerblox.download.NumeraiClassicDownloader.download_inference_data","title":"<code>download_inference_data(subfolder='', version='4.3', round_num=None)</code>","text":"<p>Get Numerai classic inference (tournament) data. :param subfolder: Specify folder to create folder within base directory root. Saves in base directory root by default. :param version: Numerai dataset version. 4 = April 2022 dataset 4.1 = Sunshine dataset 4.2 (default) = Rain Dataset 4.3 = Midnight dataset :param round_num: Numerai tournament round number. Downloads latest round by default.</p> Source code in <code>numerblox/download.py</code> <pre><code>def download_inference_data(\n    self,\n    subfolder: str = \"\",\n    version: str = \"4.3\",\n    round_num: int = None,\n):\n    \"\"\"\n    Get Numerai classic inference (tournament) data.\n    :param subfolder: Specify folder to create folder within base directory root.\n    Saves in base directory root by default.\n    :param version: Numerai dataset version.\n    4 = April 2022 dataset\n    4.1 = Sunshine dataset\n    4.2 (default) = Rain Dataset\n    4.3 = Midnight dataset\n    :param round_num: Numerai tournament round number. Downloads latest round by default.\n    \"\"\"\n    self.download_live_data(subfolder=subfolder, version=version, round_num=round_num)\n</code></pre>"},{"location":"api/#numerblox.download.NumeraiClassicDownloader.download_live_data","title":"<code>download_live_data(subfolder='', version='4.3', round_num=None)</code>","text":"<p>Download all live data in specified folder for given version (i.e. minimal data needed for inference).</p> <p>:param subfolder: Specify folder to create folder within directory root. Saves in directory root by default. :param version: Numerai dataset version.  4 = April 2022 dataset 4.1 = Sunshine dataset 4.2 (default) = Rain Dataset 4.3 = Midnight dataset :param round_num: Numerai tournament round number. Downloads latest round by default.</p> Source code in <code>numerblox/download.py</code> <pre><code>def download_live_data(\n        self,\n        subfolder: str = \"\",\n        version: str = \"4.3\",\n        round_num: int = None\n):\n    \"\"\"\n    Download all live data in specified folder for given version (i.e. minimal data needed for inference).\n\n    :param subfolder: Specify folder to create folder within directory root.\n    Saves in directory root by default.\n    :param version: Numerai dataset version. \n    4 = April 2022 dataset\n    4.1 = Sunshine dataset\n    4.2 (default) = Rain Dataset\n    4.3 = Midnight dataset\n    :param round_num: Numerai tournament round number. Downloads latest round by default.\n    \"\"\"\n    self._check_dataset_version(version)\n    live_files = [f\"v{version}/{self.LIVE_DATASET_NAME}\"]\n    for file in live_files:\n        dest_path = self.__get_dest_path(subfolder, file)\n        self.download_single_dataset(\n            filename=file,\n            dest_path=dest_path,\n            round_num=round_num\n        )\n</code></pre>"},{"location":"api/#numerblox.download.NumeraiClassicDownloader.download_meta_model_preds","title":"<code>download_meta_model_preds(subfolder='', filename='v4.3/meta_model.parquet')</code>","text":"<p>Download Meta model predictions through NumerAPI. :param subfolder: Specify folder to create folder within base directory root. Saves in base directory root by default. :param filename: name for meta model predictions file. :return: Meta model predictions as DataFrame.</p> Source code in <code>numerblox/download.py</code> <pre><code>def download_meta_model_preds(self, subfolder: str = \"\", filename=\"v4.3/meta_model.parquet\") -&gt; pd.DataFrame:\n    \"\"\"\n    Download Meta model predictions through NumerAPI.\n    :param subfolder: Specify folder to create folder within base directory root.\n    Saves in base directory root by default.\n    :param filename: name for meta model predictions file.\n    :return: Meta model predictions as DataFrame.\n    \"\"\"\n    version = filename.split(\"/\")[0].replace(\"v\", \"\")\n    self._check_dataset_version(version)\n    dest_path = self.__get_dest_path(subfolder, filename)\n    self.download_single_dataset(\n        filename=filename,\n        dest_path=dest_path,\n        )\n    return pd.read_parquet(dest_path)\n</code></pre>"},{"location":"api/#numerblox.download.NumeraiClassicDownloader.download_single_dataset","title":"<code>download_single_dataset(filename, dest_path, round_num=None)</code>","text":"<p>Download one of the available datasets through NumerAPI.</p> <p>:param filename: Name as listed in NumerAPI (Check NumerAPI().list_datasets() for full overview) :param dest_path: Full path where file will be saved. :param round_num: Numerai tournament round number. Downloads latest round by default.</p> Source code in <code>numerblox/download.py</code> <pre><code>def download_single_dataset(\n    self, filename: str, dest_path: str, round_num: int = None\n):\n    \"\"\"\n    Download one of the available datasets through NumerAPI.\n\n    :param filename: Name as listed in NumerAPI (Check NumerAPI().list_datasets() for full overview)\n    :param dest_path: Full path where file will be saved.\n    :param round_num: Numerai tournament round number. Downloads latest round by default.\n    \"\"\"\n    print(\n        f\"Downloading '{filename}'.\"\n    )\n    self.napi.download_dataset(\n        filename=filename,\n        dest_path=dest_path,\n        round_num=round_num\n    )\n</code></pre>"},{"location":"api/#numerblox.download.NumeraiClassicDownloader.download_training_data","title":"<code>download_training_data(subfolder='', version='4.3')</code>","text":"<p>Get Numerai classic training and validation data. :param subfolder: Specify folder to create folder within base directory root. Saves in base directory root by default. :param version: Numerai dataset version. 4 = April 2022 dataset 4.1 = Sunshine dataset 4.2 (default) = Rain Dataset 4.3 = Midnight dataset</p> Source code in <code>numerblox/download.py</code> <pre><code>def download_training_data(\n    self, subfolder: str = \"\", version: str = \"4.3\"\n):\n    \"\"\"\n    Get Numerai classic training and validation data.\n    :param subfolder: Specify folder to create folder within base directory root.\n    Saves in base directory root by default.\n    :param version: Numerai dataset version.\n    4 = April 2022 dataset\n    4.1 = Sunshine dataset\n    4.2 (default) = Rain Dataset\n    4.3 = Midnight dataset\n    \"\"\"\n    self._check_dataset_version(version)\n    train_val_files = [f\"v{version}/{self.TRAIN_DATASET_NAME}\",\n                       f\"v{version}/{self.VALIDATION_DATASET_NAME}\"]\n    for file in train_val_files:\n        dest_path = self.__get_dest_path(subfolder, file)\n        self.download_single_dataset(\n            filename=file,\n            dest_path=dest_path\n        )\n</code></pre>"},{"location":"api/#numerblox.download.NumeraiClassicDownloader.get_classic_features","title":"<code>get_classic_features(subfolder='', filename='v4.3/features.json', *args, **kwargs)</code>","text":"<p>Download feature overview (stats and feature sets) through NumerAPI and load as dict. :param subfolder: Specify folder to create folder within base directory root. Saves in base directory root by default. :param filename: name for feature overview. args, *kwargs will be passed to the JSON loader. :return: Feature overview dict</p> Source code in <code>numerblox/download.py</code> <pre><code>def get_classic_features(self, subfolder: str = \"\", filename=\"v4.3/features.json\", *args, **kwargs) -&gt; dict:\n    \"\"\"\n    Download feature overview (stats and feature sets) through NumerAPI and load as dict.\n    :param subfolder: Specify folder to create folder within base directory root.\n    Saves in base directory root by default.\n    :param filename: name for feature overview.\n    *args, **kwargs will be passed to the JSON loader.\n    :return: Feature overview dict\n    \"\"\"\n    version = filename.split(\"/\")[0].replace(\"v\", \"\")\n    self._check_dataset_version(version)\n    dest_path = self.__get_dest_path(subfolder, filename)\n    self.download_single_dataset(filename=filename,\n                                 dest_path=dest_path)\n    json_data = self._load_json(dest_path, *args, **kwargs)\n    return json_data\n</code></pre>"},{"location":"api/#numerblox.numerframe.NumerFrame","title":"<code>NumerFrame</code>","text":"<p>             Bases: <code>DataFrame</code></p> <p>Data structure which extends Pandas DataFrames and allows for additional Numerai specific functionality.</p> Source code in <code>numerblox/numerframe.py</code> <pre><code>class NumerFrame(pd.DataFrame):\n    \"\"\"\n    Data structure which extends Pandas DataFrames and\n    allows for additional Numerai specific functionality.\n    \"\"\"\n    _metadata = [\"meta\", \"feature_cols\", \"target_cols\",\n                 \"prediction_cols\", \"not_aux_cols\", \"aux_cols\"]\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.meta = AttrDict()\n        self.__set_era_col()\n        self.__init_meta_attrs()\n\n    @property\n    def _constructor(self):\n        return NumerFrame\n\n    def __init_meta_attrs(self):\n        \"\"\" Dynamically track column groups. \"\"\"\n        self.feature_cols = [col for col in self.columns if str(col).startswith(\"feature\")]\n        self.target_cols = [col for col in self.columns if str(col).startswith(\"target\")]\n        self.prediction_cols = [\n            col for col in self.columns if str(col).startswith(\"prediction\")\n        ]\n        self.not_aux_cols = self.feature_cols + self.target_cols + self.prediction_cols\n        self.aux_cols = [\n            col for col in self.columns if col not in self.not_aux_cols\n        ]\n\n    def __set_era_col(self):\n        \"\"\" Each NumerFrame should have an era column to benefit from all functionality. \"\"\"\n        if \"era\" in self.columns:\n            self.meta.era_col = \"era\"\n        elif \"friday_date\" in self.columns:\n            self.meta.era_col = \"friday_date\"\n        elif \"date\" in self.columns:\n            self.meta.era_col = \"date\"\n        else:\n            self.meta.era_col = None\n\n    def get_column_selection(self, cols: Union[str, list]) -&gt; \"NumerFrame\":\n        \"\"\" Return NumerFrame from selection of columns. \"\"\"\n        return self.loc[:, cols if isinstance(cols, list) else [cols]]\n\n    @property\n    def get_feature_data(self) -&gt; \"NumerFrame\":\n        \"\"\" All columns for which name starts with 'target'.\"\"\"\n        return self.get_column_selection(cols=self.feature_cols)\n\n    @property\n    def get_target_data(self) -&gt; \"NumerFrame\":\n        \"\"\" All columns for which name starts with 'target'.\"\"\"\n        return self.get_column_selection(cols=self.target_cols)\n\n    @property\n    def get_single_target_data(self) -&gt; \"NumerFrame\":\n        \"\"\" Column with name 'target' (Main Numerai target column). \"\"\"\n        return self.get_column_selection(cols=['target'])\n\n    @property\n    def get_prediction_data(self) -&gt; \"NumerFrame\":\n        \"\"\" All columns for which name starts with 'prediction'.\"\"\"\n        return self.get_column_selection(cols=self.prediction_cols)\n\n    @property\n    def get_aux_data(self) -&gt; \"NumerFrame\":\n        \"\"\" All columns that are not features, targets or predictions. \"\"\"\n        return self.get_column_selection(cols=self.aux_cols)\n\n    @property\n    def get_era_data(self) -&gt; \"NumerFrame\":\n        \"\"\" Column of all eras. \"\"\"\n        return self.get_column_selection(cols=self.meta.era_col)\n\n    @property\n    def get_prediction_aux_data(self) -&gt; \"NumerFrame\":\n        \"\"\" All predictions columns and aux columns (for ensembling, etc.). \"\"\"\n        return self.get_column_selection(cols=self.prediction_cols+self.aux_cols)\n\n    @property\n    def get_fncv3_feature_data(self) -&gt; \"NumerFrame\":\n        \"\"\" Get FNCv3 features. \"\"\"\n        return self.get_column_selection(cols=FNCV3_FEATURES)\n\n    @property\n    def get_small_feature_data(self) -&gt; \"NumerFrame\":\n        \"\"\" Small subset of the Numerai dataset for v4.2 data. \"\"\"\n        return self.get_column_selection(cols=SMALL_FEATURES)\n\n    @property\n    def get_medium_feature_data(self) -&gt; \"NumerFrame\":\n        \"\"\" Medium subset of the Numerai dataset for v4.2 data. \"\"\"\n        return self.get_column_selection(cols=MEDIUM_FEATURES)\n\n    @property\n    def get_v2_equivalent_feature_data(self) -&gt; \"NumerFrame\":\n        \"\"\" Features equivalent to the deprecated v2 Numerai data. For v4.2 data. \"\"\"\n        return self.get_column_selection(cols=V2_EQUIVALENT_FEATURES)\n\n    @property\n    def get_v3_equivalent_feature_data(self) -&gt; \"NumerFrame\":\n        \"\"\" Features equivalent to the deprecated v3 Numerai data. For v4.2 data. \"\"\"\n        return self.get_column_selection(cols=V3_EQUIVALENT_FEATURES)\n\n    @property\n    def get_unique_eras(self) -&gt; List[str]:\n        \"\"\" Get all unique eras in the data. \"\"\"\n        return self[self.meta.era_col].unique().tolist()\n\n    def get_last_n_eras(self, n: int) -&gt; \"NumerFrame\":\n        \"\"\" \n        Get data for the last n eras. \n        Make sure eras are sorted in the way you prefer.\n        :param n: Number of eras to select.\n        :return: NumerFrame with last n eras.\n        \"\"\"\n        eras = self[self.meta.era_col].unique()[-n:]\n        return self.loc[self[self.meta.era_col].isin(eras)]\n\n    def get_feature_group(self, group: str) -&gt; \"NumerFrame\":\n        \"\"\" Get feature group based on name or list of names. \"\"\"\n        assert group in V4_2_FEATURE_GROUP_MAPPING.keys(), \\\n            f\"Group '{group}' not found in {V4_2_FEATURE_GROUP_MAPPING.keys()}\"\n        return self.get_column_selection(cols=V4_2_FEATURE_GROUP_MAPPING[group])\n\n    def get_pattern_data(self, pattern: str) -&gt; \"NumerFrame\":\n        \"\"\"\n        Get columns based on pattern (for example '_20' to get all 20-day Numerai targets).\n        :param pattern: A 'like' pattern (pattern in column_name == True)\n        \"\"\"\n        return self.filter(like=pattern)\n\n    def get_feature_target_pair(self, multi_target=False) -&gt; Tuple[\"NumerFrame\", \"NumerFrame\"]:\n        \"\"\"\n        Get split of feature and target columns.\n        :param multi_target: Returns only 'target' column by default.\n        Returns all target columns when set to True.\n        \"\"\"\n        X = self.get_feature_data\n        y = self.get_target_data if multi_target else self.get_single_target_data\n        return X, y\n\n    def get_era_batch(self, eras: List[Any],\n                      convert_to_tf = False,\n                      aemlp_batch = False,\n                      features: list = None,\n                      targets: list = None,\n                      *args, **kwargs) -&gt; Tuple[\"NumerFrame\", \"NumerFrame\"]:\n        \"\"\"\n        Get feature target pair batch of 1 or multiple eras. \\n\n        :param eras: Selection of era names that should be present in era_col. \\n\n        :param convert_to_tf: Convert to tf.Tensor. \\n\n        :param aemlp_batch: Specific target batch for autoencoder training. \\n\n        `y` output will contain three components: features, targets and targets. \\n\n        :param features: List of features to select. All by default \\n\n        :param targets: List of targets to select. All by default. \\n\n        *args, **kwargs are passed to initialization of Tensor.\n        \"\"\"\n        valid_eras = []\n        for era in eras:\n            assert era in self[self.meta.era_col].unique(), f\"Era '{era}' not found in era column ({self.meta.era_col})\"\n            valid_eras.append(era)\n        features = features if features else self.feature_cols\n        targets = targets if targets else self.target_cols\n        X = self.loc[self[self.meta.era_col].isin(valid_eras)][features].values\n        y = self.loc[self[self.meta.era_col].isin(valid_eras)][targets].values\n        if aemlp_batch:\n            y = [X.copy(), y.copy(), y.copy()]\n\n        if convert_to_tf:\n            try:\n                import tensorflow as tf\n            except ImportError:\n                raise ImportError(\"TensorFlow is not installed. Please make sure to have Tensorflow installed when setting `convert_to_tf=True`.\")\n            X = tf.convert_to_tensor(X, *args, **kwargs)\n            if aemlp_batch:\n                y = [tf.convert_to_tensor(i, *args, **kwargs) for i in y]\n            else:\n                y = tf.convert_to_tensor(y, *args, **kwargs)\n        return X, y\n\n    @property\n    def get_dates_from_era_col(self) -&gt; pd.Series:\n        \"\"\" Column of all dates from era column. \"\"\"\n        assert self.meta.era_col == \"era\", \\\n            \"Era col is not 'era'. Please make sure to have a valid 'era' column to use for converting to dates.\"\n        return self[self.meta.era_col].astype(int).apply(self.get_date_from_era)\n\n    @property\n    def get_eras_from_date_col(self) -&gt; pd.Series:\n        \"\"\" Column of all eras from date column. \"\"\"\n        assert self.meta.era_col == \"date\" or self.meta.era_col == \"friday_date\", \\\n            \"Era col is not 'date' or 'friday_date'. Please make sure to have a valid 'date' or 'friday_date column to use for converting to eras.\"\n        return self[self.meta.era_col].apply(self.get_era_from_date)\n\n    def get_era_range(self, start_era: int, end_era: int) -&gt; \"NumerFrame\":\n        \"\"\" \n        Get all eras between two era numbers. \n        :param start_era: Era number to start from (inclusive).\n        :param end_era: Era number to end with (inclusive).\n        :return: NumerFrame with all eras between start_era and end_era.\n        \"\"\"\n        assert \"era\" in self.columns, \"Era column not found. Please make sure to have an 'era' column in your data.\"\n        assert isinstance(start_era, int), f\"start_era should be of type 'int' but is '{type(start_era)}'\"\n        assert isinstance(end_era, int), f\"end_era should be of type 'int' but is '{type(end_era)}'\"\n        assert 1 &lt;= start_era &lt;= end_era &lt;= get_current_era(), \\\n            f\"start_era should be between 1 and {get_current_era()}. Got '{start_era}'.\"\n        assert 1 &lt;= start_era &lt;= end_era &lt;= get_current_era(), \\\n            f\"end_era should be between 1 and {get_current_era()}. Got '{end_era}'.\"\n        assert start_era &lt;= end_era, f\"start_era should be before end_era. Got '{start_era}' and '{end_era}'\"\n\n        temp_df = self.copy()\n        temp_df['era_int'] = temp_df['era'].astype(int)\n        result_df = temp_df[(temp_df['era_int'] &gt;= start_era) &amp; (temp_df['era_int'] &lt;= end_era)]\n        return result_df.drop(columns=['era_int'])\n\n    def get_date_range(self, start_date: pd.Timestamp, end_date: pd.Timestamp) -&gt; \"NumerFrame\":\n        \"\"\"\n        Get all eras between two dates.\n        :param start_date: Starting date (inclusive).\n        :param end_date: Ending date (inclusive).\n        :return: NumerFrame with all eras between start_date and end_date.\n        \"\"\"\n        assert self.meta.era_col == \"date\" or self.meta.era_col == \"friday_date\", \\\n            \"Era col is not 'date' or 'friday_date'. Please make sure to have a valid 'era' column.\"\n        assert isinstance(start_date, pd.Timestamp), f\"start_date should be of type 'pd.Timestamp' but is '{type(start_date)}'\"\n        assert isinstance(end_date, pd.Timestamp), f\"end_date should be of type 'pd.Timestamp' but is '{type(end_date)}'\"\n        assert ERA1_TIMESTAMP &lt;= start_date &lt;= pd.Timestamp(get_current_date()), \\\n            f\"start_date should be between {ERA_ONE_START} and {pd.Timestamp(get_current_date())}\"\n        assert ERA1_TIMESTAMP &lt;= end_date &lt;= pd.Timestamp(get_current_date()), \\\n            f\"end_date should be between {ERA_ONE_START} and {pd.Timestamp(get_current_date())}\"\n        assert start_date &lt;= end_date, f\"start_date should be before end_date. Got '{start_date}' and '{end_date}'\"\n\n        temp_df = self.copy()\n        result_df = temp_df[(temp_df[self.meta.era_col] &gt;= start_date) &amp; (temp_df[self.meta.era_col] &lt;= end_date)]\n        return result_df\n\n    @staticmethod\n    def get_era_from_date(date_object: pd.Timestamp) -&gt; int:\n        \"\"\" \n        Get the era number from a specific date. \n        :param date_object: Pandas Timestamp object for which to get era.\n        :return: Era number.\n        \"\"\"\n        assert isinstance(date_object, pd.Timestamp), f\"date_object should be of type 'date' but is '{type(date_object)}'\"\n        current_date = pd.Timestamp(get_current_date())\n        assert ERA1_TIMESTAMP &lt;= date_object &lt;= current_date, \\\n            f\"date_object should be between {ERA_ONE_START} and {current_date}\"\n        return get_era_for_date(date_object.date())\n\n    @staticmethod\n    def get_date_from_era(era: int) -&gt; pd.Timestamp:\n        \"\"\" \n        Get the date from a specific era. \n        :param era: Era number for which to get date.\n        Should be an integer which is at least 1.\n        :return: Datetime object representing the date of the given era.\n        \"\"\"\n        assert isinstance(era, int), f\"era should be of type 'int' but is '{type(era)}'\"\n        assert 1 &lt;= era &lt;= get_current_era(), \\\n            f\"era should be between 1 and {get_current_era()}. Got '{era}'.\"\n        return pd.Timestamp(get_date_for_era(era))\n</code></pre>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_aux_data","title":"<code>get_aux_data: NumerFrame</code>  <code>property</code>","text":"<p>All columns that are not features, targets or predictions.</p>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_dates_from_era_col","title":"<code>get_dates_from_era_col: pd.Series</code>  <code>property</code>","text":"<p>Column of all dates from era column.</p>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_era_data","title":"<code>get_era_data: NumerFrame</code>  <code>property</code>","text":"<p>Column of all eras.</p>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_eras_from_date_col","title":"<code>get_eras_from_date_col: pd.Series</code>  <code>property</code>","text":"<p>Column of all eras from date column.</p>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_feature_data","title":"<code>get_feature_data: NumerFrame</code>  <code>property</code>","text":"<p>All columns for which name starts with 'target'.</p>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_fncv3_feature_data","title":"<code>get_fncv3_feature_data: NumerFrame</code>  <code>property</code>","text":"<p>Get FNCv3 features.</p>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_medium_feature_data","title":"<code>get_medium_feature_data: NumerFrame</code>  <code>property</code>","text":"<p>Medium subset of the Numerai dataset for v4.2 data.</p>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_prediction_aux_data","title":"<code>get_prediction_aux_data: NumerFrame</code>  <code>property</code>","text":"<p>All predictions columns and aux columns (for ensembling, etc.).</p>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_prediction_data","title":"<code>get_prediction_data: NumerFrame</code>  <code>property</code>","text":"<p>All columns for which name starts with 'prediction'.</p>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_single_target_data","title":"<code>get_single_target_data: NumerFrame</code>  <code>property</code>","text":"<p>Column with name 'target' (Main Numerai target column).</p>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_small_feature_data","title":"<code>get_small_feature_data: NumerFrame</code>  <code>property</code>","text":"<p>Small subset of the Numerai dataset for v4.2 data.</p>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_target_data","title":"<code>get_target_data: NumerFrame</code>  <code>property</code>","text":"<p>All columns for which name starts with 'target'.</p>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_unique_eras","title":"<code>get_unique_eras: List[str]</code>  <code>property</code>","text":"<p>Get all unique eras in the data.</p>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_v2_equivalent_feature_data","title":"<code>get_v2_equivalent_feature_data: NumerFrame</code>  <code>property</code>","text":"<p>Features equivalent to the deprecated v2 Numerai data. For v4.2 data.</p>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_v3_equivalent_feature_data","title":"<code>get_v3_equivalent_feature_data: NumerFrame</code>  <code>property</code>","text":"<p>Features equivalent to the deprecated v3 Numerai data. For v4.2 data.</p>"},{"location":"api/#numerblox.numerframe.NumerFrame.__init_meta_attrs","title":"<code>__init_meta_attrs()</code>","text":"<p>Dynamically track column groups.</p> Source code in <code>numerblox/numerframe.py</code> <pre><code>def __init_meta_attrs(self):\n    \"\"\" Dynamically track column groups. \"\"\"\n    self.feature_cols = [col for col in self.columns if str(col).startswith(\"feature\")]\n    self.target_cols = [col for col in self.columns if str(col).startswith(\"target\")]\n    self.prediction_cols = [\n        col for col in self.columns if str(col).startswith(\"prediction\")\n    ]\n    self.not_aux_cols = self.feature_cols + self.target_cols + self.prediction_cols\n    self.aux_cols = [\n        col for col in self.columns if col not in self.not_aux_cols\n    ]\n</code></pre>"},{"location":"api/#numerblox.numerframe.NumerFrame.__set_era_col","title":"<code>__set_era_col()</code>","text":"<p>Each NumerFrame should have an era column to benefit from all functionality.</p> Source code in <code>numerblox/numerframe.py</code> <pre><code>def __set_era_col(self):\n    \"\"\" Each NumerFrame should have an era column to benefit from all functionality. \"\"\"\n    if \"era\" in self.columns:\n        self.meta.era_col = \"era\"\n    elif \"friday_date\" in self.columns:\n        self.meta.era_col = \"friday_date\"\n    elif \"date\" in self.columns:\n        self.meta.era_col = \"date\"\n    else:\n        self.meta.era_col = None\n</code></pre>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_column_selection","title":"<code>get_column_selection(cols)</code>","text":"<p>Return NumerFrame from selection of columns.</p> Source code in <code>numerblox/numerframe.py</code> <pre><code>def get_column_selection(self, cols: Union[str, list]) -&gt; \"NumerFrame\":\n    \"\"\" Return NumerFrame from selection of columns. \"\"\"\n    return self.loc[:, cols if isinstance(cols, list) else [cols]]\n</code></pre>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_date_from_era","title":"<code>get_date_from_era(era)</code>  <code>staticmethod</code>","text":"<p>Get the date from a specific era.  :param era: Era number for which to get date. Should be an integer which is at least 1. :return: Datetime object representing the date of the given era.</p> Source code in <code>numerblox/numerframe.py</code> <pre><code>@staticmethod\ndef get_date_from_era(era: int) -&gt; pd.Timestamp:\n    \"\"\" \n    Get the date from a specific era. \n    :param era: Era number for which to get date.\n    Should be an integer which is at least 1.\n    :return: Datetime object representing the date of the given era.\n    \"\"\"\n    assert isinstance(era, int), f\"era should be of type 'int' but is '{type(era)}'\"\n    assert 1 &lt;= era &lt;= get_current_era(), \\\n        f\"era should be between 1 and {get_current_era()}. Got '{era}'.\"\n    return pd.Timestamp(get_date_for_era(era))\n</code></pre>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_date_range","title":"<code>get_date_range(start_date, end_date)</code>","text":"<p>Get all eras between two dates. :param start_date: Starting date (inclusive). :param end_date: Ending date (inclusive). :return: NumerFrame with all eras between start_date and end_date.</p> Source code in <code>numerblox/numerframe.py</code> <pre><code>def get_date_range(self, start_date: pd.Timestamp, end_date: pd.Timestamp) -&gt; \"NumerFrame\":\n    \"\"\"\n    Get all eras between two dates.\n    :param start_date: Starting date (inclusive).\n    :param end_date: Ending date (inclusive).\n    :return: NumerFrame with all eras between start_date and end_date.\n    \"\"\"\n    assert self.meta.era_col == \"date\" or self.meta.era_col == \"friday_date\", \\\n        \"Era col is not 'date' or 'friday_date'. Please make sure to have a valid 'era' column.\"\n    assert isinstance(start_date, pd.Timestamp), f\"start_date should be of type 'pd.Timestamp' but is '{type(start_date)}'\"\n    assert isinstance(end_date, pd.Timestamp), f\"end_date should be of type 'pd.Timestamp' but is '{type(end_date)}'\"\n    assert ERA1_TIMESTAMP &lt;= start_date &lt;= pd.Timestamp(get_current_date()), \\\n        f\"start_date should be between {ERA_ONE_START} and {pd.Timestamp(get_current_date())}\"\n    assert ERA1_TIMESTAMP &lt;= end_date &lt;= pd.Timestamp(get_current_date()), \\\n        f\"end_date should be between {ERA_ONE_START} and {pd.Timestamp(get_current_date())}\"\n    assert start_date &lt;= end_date, f\"start_date should be before end_date. Got '{start_date}' and '{end_date}'\"\n\n    temp_df = self.copy()\n    result_df = temp_df[(temp_df[self.meta.era_col] &gt;= start_date) &amp; (temp_df[self.meta.era_col] &lt;= end_date)]\n    return result_df\n</code></pre>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_era_batch","title":"<code>get_era_batch(eras, convert_to_tf=False, aemlp_batch=False, features=None, targets=None, *args, **kwargs)</code>","text":"<p>Get feature target pair batch of 1 or multiple eras. </p> <p>:param eras: Selection of era names that should be present in era_col. </p> <p>:param convert_to_tf: Convert to tf.Tensor. </p> <p>:param aemlp_batch: Specific target batch for autoencoder training. </p> <p><code>y</code> output will contain three components: features, targets and targets. </p> <p>:param features: List of features to select. All by default </p> <p>:param targets: List of targets to select. All by default. </p> <p>args, *kwargs are passed to initialization of Tensor.</p> Source code in <code>numerblox/numerframe.py</code> <pre><code>def get_era_batch(self, eras: List[Any],\n                  convert_to_tf = False,\n                  aemlp_batch = False,\n                  features: list = None,\n                  targets: list = None,\n                  *args, **kwargs) -&gt; Tuple[\"NumerFrame\", \"NumerFrame\"]:\n    \"\"\"\n    Get feature target pair batch of 1 or multiple eras. \\n\n    :param eras: Selection of era names that should be present in era_col. \\n\n    :param convert_to_tf: Convert to tf.Tensor. \\n\n    :param aemlp_batch: Specific target batch for autoencoder training. \\n\n    `y` output will contain three components: features, targets and targets. \\n\n    :param features: List of features to select. All by default \\n\n    :param targets: List of targets to select. All by default. \\n\n    *args, **kwargs are passed to initialization of Tensor.\n    \"\"\"\n    valid_eras = []\n    for era in eras:\n        assert era in self[self.meta.era_col].unique(), f\"Era '{era}' not found in era column ({self.meta.era_col})\"\n        valid_eras.append(era)\n    features = features if features else self.feature_cols\n    targets = targets if targets else self.target_cols\n    X = self.loc[self[self.meta.era_col].isin(valid_eras)][features].values\n    y = self.loc[self[self.meta.era_col].isin(valid_eras)][targets].values\n    if aemlp_batch:\n        y = [X.copy(), y.copy(), y.copy()]\n\n    if convert_to_tf:\n        try:\n            import tensorflow as tf\n        except ImportError:\n            raise ImportError(\"TensorFlow is not installed. Please make sure to have Tensorflow installed when setting `convert_to_tf=True`.\")\n        X = tf.convert_to_tensor(X, *args, **kwargs)\n        if aemlp_batch:\n            y = [tf.convert_to_tensor(i, *args, **kwargs) for i in y]\n        else:\n            y = tf.convert_to_tensor(y, *args, **kwargs)\n    return X, y\n</code></pre>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_era_from_date","title":"<code>get_era_from_date(date_object)</code>  <code>staticmethod</code>","text":"<p>Get the era number from a specific date.  :param date_object: Pandas Timestamp object for which to get era. :return: Era number.</p> Source code in <code>numerblox/numerframe.py</code> <pre><code>@staticmethod\ndef get_era_from_date(date_object: pd.Timestamp) -&gt; int:\n    \"\"\" \n    Get the era number from a specific date. \n    :param date_object: Pandas Timestamp object for which to get era.\n    :return: Era number.\n    \"\"\"\n    assert isinstance(date_object, pd.Timestamp), f\"date_object should be of type 'date' but is '{type(date_object)}'\"\n    current_date = pd.Timestamp(get_current_date())\n    assert ERA1_TIMESTAMP &lt;= date_object &lt;= current_date, \\\n        f\"date_object should be between {ERA_ONE_START} and {current_date}\"\n    return get_era_for_date(date_object.date())\n</code></pre>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_era_range","title":"<code>get_era_range(start_era, end_era)</code>","text":"<p>Get all eras between two era numbers.  :param start_era: Era number to start from (inclusive). :param end_era: Era number to end with (inclusive). :return: NumerFrame with all eras between start_era and end_era.</p> Source code in <code>numerblox/numerframe.py</code> <pre><code>def get_era_range(self, start_era: int, end_era: int) -&gt; \"NumerFrame\":\n    \"\"\" \n    Get all eras between two era numbers. \n    :param start_era: Era number to start from (inclusive).\n    :param end_era: Era number to end with (inclusive).\n    :return: NumerFrame with all eras between start_era and end_era.\n    \"\"\"\n    assert \"era\" in self.columns, \"Era column not found. Please make sure to have an 'era' column in your data.\"\n    assert isinstance(start_era, int), f\"start_era should be of type 'int' but is '{type(start_era)}'\"\n    assert isinstance(end_era, int), f\"end_era should be of type 'int' but is '{type(end_era)}'\"\n    assert 1 &lt;= start_era &lt;= end_era &lt;= get_current_era(), \\\n        f\"start_era should be between 1 and {get_current_era()}. Got '{start_era}'.\"\n    assert 1 &lt;= start_era &lt;= end_era &lt;= get_current_era(), \\\n        f\"end_era should be between 1 and {get_current_era()}. Got '{end_era}'.\"\n    assert start_era &lt;= end_era, f\"start_era should be before end_era. Got '{start_era}' and '{end_era}'\"\n\n    temp_df = self.copy()\n    temp_df['era_int'] = temp_df['era'].astype(int)\n    result_df = temp_df[(temp_df['era_int'] &gt;= start_era) &amp; (temp_df['era_int'] &lt;= end_era)]\n    return result_df.drop(columns=['era_int'])\n</code></pre>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_feature_group","title":"<code>get_feature_group(group)</code>","text":"<p>Get feature group based on name or list of names.</p> Source code in <code>numerblox/numerframe.py</code> <pre><code>def get_feature_group(self, group: str) -&gt; \"NumerFrame\":\n    \"\"\" Get feature group based on name or list of names. \"\"\"\n    assert group in V4_2_FEATURE_GROUP_MAPPING.keys(), \\\n        f\"Group '{group}' not found in {V4_2_FEATURE_GROUP_MAPPING.keys()}\"\n    return self.get_column_selection(cols=V4_2_FEATURE_GROUP_MAPPING[group])\n</code></pre>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_feature_target_pair","title":"<code>get_feature_target_pair(multi_target=False)</code>","text":"<p>Get split of feature and target columns. :param multi_target: Returns only 'target' column by default. Returns all target columns when set to True.</p> Source code in <code>numerblox/numerframe.py</code> <pre><code>def get_feature_target_pair(self, multi_target=False) -&gt; Tuple[\"NumerFrame\", \"NumerFrame\"]:\n    \"\"\"\n    Get split of feature and target columns.\n    :param multi_target: Returns only 'target' column by default.\n    Returns all target columns when set to True.\n    \"\"\"\n    X = self.get_feature_data\n    y = self.get_target_data if multi_target else self.get_single_target_data\n    return X, y\n</code></pre>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_last_n_eras","title":"<code>get_last_n_eras(n)</code>","text":"<p>Get data for the last n eras.  Make sure eras are sorted in the way you prefer. :param n: Number of eras to select. :return: NumerFrame with last n eras.</p> Source code in <code>numerblox/numerframe.py</code> <pre><code>def get_last_n_eras(self, n: int) -&gt; \"NumerFrame\":\n    \"\"\" \n    Get data for the last n eras. \n    Make sure eras are sorted in the way you prefer.\n    :param n: Number of eras to select.\n    :return: NumerFrame with last n eras.\n    \"\"\"\n    eras = self[self.meta.era_col].unique()[-n:]\n    return self.loc[self[self.meta.era_col].isin(eras)]\n</code></pre>"},{"location":"api/#numerblox.numerframe.NumerFrame.get_pattern_data","title":"<code>get_pattern_data(pattern)</code>","text":"<p>Get columns based on pattern (for example '_20' to get all 20-day Numerai targets). :param pattern: A 'like' pattern (pattern in column_name == True)</p> Source code in <code>numerblox/numerframe.py</code> <pre><code>def get_pattern_data(self, pattern: str) -&gt; \"NumerFrame\":\n    \"\"\"\n    Get columns based on pattern (for example '_20' to get all 20-day Numerai targets).\n    :param pattern: A 'like' pattern (pattern in column_name == True)\n    \"\"\"\n    return self.filter(like=pattern)\n</code></pre>"},{"location":"api/#numerblox.numerframe.create_numerframe","title":"<code>create_numerframe(file_path, columns=None, *args, **kwargs)</code>","text":"<p>Convenient function to initialize NumerFrame. Support most used file formats for Pandas DataFrames </p> <p>(.csv, .parquet, .xls, .pkl, etc.). For more details check https://pandas.pydata.org/docs/reference/io.html</p> <p>:param file_path: Relative or absolute path to data file. </p> <p>:param columns: Which columns to read (All by default). </p> <p>args, *kwargs will be passed to Pandas loading function.</p> Source code in <code>numerblox/numerframe.py</code> <pre><code>def create_numerframe(file_path: str, columns: list = None, *args, **kwargs) -&gt; NumerFrame:\n    \"\"\"\n    Convenient function to initialize NumerFrame.\n    Support most used file formats for Pandas DataFrames \\n\n    (.csv, .parquet, .xls, .pkl, etc.).\n    For more details check https://pandas.pydata.org/docs/reference/io.html\n\n    :param file_path: Relative or absolute path to data file. \\n\n    :param columns: Which columns to read (All by default). \\n\n    *args, **kwargs will be passed to Pandas loading function.\n    \"\"\"\n    assert Path(file_path).is_file(), f\"{file_path} does not point to file.\"\n    suffix = Path(file_path).suffix\n    if suffix in [\".csv\"]:\n        df = pd.read_csv(file_path, usecols=columns, *args, **kwargs)\n    elif suffix in [\".parquet\"]:\n        df = pd.read_parquet(file_path, columns=columns, *args, **kwargs)\n    elif suffix in [\".xls\", \".xlsx\", \".xlsm\", \"xlsb\", \".odf\", \".ods\", \".odt\"]:\n        df = pd.read_excel(file_path, usecols=columns, *args, **kwargs)\n    elif suffix in ['.pkl', '.pickle']:\n        df = pd.read_pickle(file_path, *args, **kwargs)\n        df = df.loc[:, columns] if columns else df\n    else:\n        raise NotImplementedError(f\"Suffix '{suffix}' is not supported.\")\n    num_frame = NumerFrame(df)\n    return num_frame\n</code></pre>"},{"location":"api/#numerblox.preprocessing.base.BasePreProcessor","title":"<code>BasePreProcessor</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Common functionality for preprocessors and postprocessors.</p> Source code in <code>numerblox/preprocessing/base.py</code> <pre><code>class BasePreProcessor(BaseEstimator, TransformerMixin):\n    \"\"\"Common functionality for preprocessors and postprocessors.\"\"\"\n\n    def __init__(self):\n        ...\n\n    def fit(self, X, y=None, **kwargs):\n        return self\n\n    @abstractmethod\n    def transform(\n        self, X: Union[np.array, pd.DataFrame], y=None, **kwargs\n    ) -&gt; pd.DataFrame:\n        ...\n\n    def __call__(\n        self, X: Union[np.array, pd.DataFrame], y=None, **kwargs\n    ) -&gt; pd.DataFrame:\n        return self.transform(X=X, y=y, **kwargs)\n\n    @abstractmethod\n    def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n        ...\n</code></pre>"},{"location":"api/#numerblox.preprocessing.classic.GroupStatsPreProcessor","title":"<code>GroupStatsPreProcessor</code>","text":"<p>             Bases: <code>BasePreProcessor</code></p> <p>WARNING: Only supported for v4.2 (Rain) data. The Rain dataset (re)introduced feature groups. </p> <p>Note that this class only works with <code>pd.DataFrame</code> input. We using in a Pipeline, make sure that the Pandas output API is set (<code>.set_output(transform=\"pandas\")</code>.</p> <p>Calculates group statistics for all data groups. </p> <p>:param groups: Groups to create features for. All groups by default.</p> Source code in <code>numerblox/preprocessing/classic.py</code> <pre><code>class GroupStatsPreProcessor(BasePreProcessor):\n    \"\"\"\n    WARNING: Only supported for v4.2 (Rain) data. The Rain dataset (re)introduced feature groups. \\n\n    Note that this class only works with `pd.DataFrame` input.\n    We using in a Pipeline, make sure that the Pandas output API is set (`.set_output(transform=\"pandas\")`.\n\n    Calculates group statistics for all data groups. \\n\n    :param groups: Groups to create features for. All groups by default. \\n\n    \"\"\"\n    def __init__(self, groups: list = None):\n        super().__init__()\n        self.all_groups = [\n            'intelligence', \n            'charisma', \n            'strength', \n            'dexterity', \n            'constitution', \n            'wisdom', \n            'agility', \n            'serenity', \n            'sunshine', \n            'rain'\n        ]\n        self.groups = groups \n        self.group_names = groups if self.groups else self.all_groups\n        self.feature_group_mapping = V4_2_FEATURE_GROUP_MAPPING\n\n    def transform(self, X: pd.DataFrame) -&gt; np.array:\n        \"\"\"Check validity and add group features.\"\"\"\n        dataf = self._add_group_features(X)\n        return dataf.to_numpy()\n\n    def _add_group_features(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Mean, standard deviation and skew for each group.\"\"\"\n        dataf = pd.DataFrame()\n        for group in self.group_names:\n            cols = self.feature_group_mapping[group]\n            valid_cols = [col for col in cols if col in X.columns]\n            if not valid_cols:\n                warnings.warn(f\"None of the columns of '{group}' are in the input data. Output will be nans for the group features.\")\n            elif len(cols) != len(valid_cols):\n                warnings.warn(f\"Not all columns of '{group}' are in the input data ({len(valid_cols)} &lt; {len(cols)}). Use remaining columns for stats features.\")\n            dataf.loc[:, f\"feature_{group}_mean\"] = X[valid_cols].mean(axis=1)\n            dataf.loc[:, f\"feature_{group}_std\"] = X[valid_cols].std(axis=1)\n            dataf.loc[:, f\"feature_{group}_skew\"] = X[valid_cols].skew(axis=1)\n        return dataf\n\n    def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n        \"\"\"Return feature names.\"\"\"\n        if not input_features:\n            feature_names = []\n            for group in self.group_names:\n                feature_names.append(f\"feature_{group}_mean\")\n                feature_names.append(f\"feature_{group}_std\")\n                feature_names.append(f\"feature_{group}_skew\")\n        else:\n            feature_names = input_features\n        return feature_names\n</code></pre>"},{"location":"api/#numerblox.preprocessing.classic.GroupStatsPreProcessor.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Return feature names.</p> Source code in <code>numerblox/preprocessing/classic.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n    \"\"\"Return feature names.\"\"\"\n    if not input_features:\n        feature_names = []\n        for group in self.group_names:\n            feature_names.append(f\"feature_{group}_mean\")\n            feature_names.append(f\"feature_{group}_std\")\n            feature_names.append(f\"feature_{group}_skew\")\n    else:\n        feature_names = input_features\n    return feature_names\n</code></pre>"},{"location":"api/#numerblox.preprocessing.classic.GroupStatsPreProcessor.transform","title":"<code>transform(X)</code>","text":"<p>Check validity and add group features.</p> Source code in <code>numerblox/preprocessing/classic.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; np.array:\n    \"\"\"Check validity and add group features.\"\"\"\n    dataf = self._add_group_features(X)\n    return dataf.to_numpy()\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.DifferencePreProcessor","title":"<code>DifferencePreProcessor</code>","text":"<p>             Bases: <code>BasePreProcessor</code></p> <p>Add difference features based on given windows. Run LagPreProcessor first. Usage in Pipeline works only with Pandas API.  Run <code>.set_output(\"pandas\")</code> on your pipeline first.</p> <p>:param windows: All lag windows to process for all features. </p> <p>:param feature_names: All features for which you want to create differences. All features that also have lags by default. </p> <p>:param pct_change: Method to calculate differences. If True, will calculate differences with a percentage change. Otherwise calculates a simple difference. Defaults to False </p> <p>:param abs_diff: Whether to also calculate the absolute value of all differences. Defaults to True</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>class DifferencePreProcessor(BasePreProcessor):\n    \"\"\"\n    Add difference features based on given windows. Run LagPreProcessor first.\n    Usage in Pipeline works only with Pandas API. \n    Run `.set_output(\"pandas\")` on your pipeline first.\n\n    :param windows: All lag windows to process for all features. \\n\n    :param feature_names: All features for which you want to create differences. All features that also have lags by default. \\n\n    :param pct_change: Method to calculate differences. If True, will calculate differences with a percentage change. Otherwise calculates a simple difference. Defaults to False \\n\n    :param abs_diff: Whether to also calculate the absolute value of all differences. Defaults to True \\n\n    \"\"\"\n\n    def __init__(\n        self,\n        windows: list = None,\n        pct_diff: bool = False,\n        abs_diff: bool = False,\n    ):\n        super().__init__()\n        self.windows = windows if windows else [5, 10, 15, 20]\n        self.pct_diff = pct_diff\n        self.abs_diff = abs_diff\n\n    def transform(self, X: pd.DataFrame) -&gt; np.array:\n        \"\"\"\n        Create difference feature from lag features.\n        :param X: DataFrame with lag features.\n        NOTE: Make sure only lag features are present in the DataFrame.\n        \"\"\"\n        feature_names = X.columns.tolist()\n        for col in feature_names:\n            assert \"_lag\" in col, \"DifferencePreProcessor expects only lag features. Got feature: '{col}'\"\n        output_features = []\n        for feature in tqdm(feature_names, desc=\"Difference feature generation\"):\n            for day in self.windows:\n                differenced_values = (\n                        (X[feature] / X[feature]) - 1\n                        if self.pct_diff\n                        else X[feature] - X[feature]\n                    )\n                X[f\"{feature}_diff{day}\"] = differenced_values\n                output_features.append(f\"{feature}_diff{day}\")\n                if self.abs_diff:\n                    X[f\"{feature}_absdiff{day}\"] = np.abs(\n                            X[f\"{feature}_diff{day}\"]\n                        )\n                    output_features.append(f\"{feature}_absdiff{day}\")\n        self.output_features = output_features\n        return X[self.output_features].to_numpy()\n\n    def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n        return self.output_features if not input_features else input_features\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.DifferencePreProcessor.transform","title":"<code>transform(X)</code>","text":"<p>Create difference feature from lag features. :param X: DataFrame with lag features. NOTE: Make sure only lag features are present in the DataFrame.</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; np.array:\n    \"\"\"\n    Create difference feature from lag features.\n    :param X: DataFrame with lag features.\n    NOTE: Make sure only lag features are present in the DataFrame.\n    \"\"\"\n    feature_names = X.columns.tolist()\n    for col in feature_names:\n        assert \"_lag\" in col, \"DifferencePreProcessor expects only lag features. Got feature: '{col}'\"\n    output_features = []\n    for feature in tqdm(feature_names, desc=\"Difference feature generation\"):\n        for day in self.windows:\n            differenced_values = (\n                    (X[feature] / X[feature]) - 1\n                    if self.pct_diff\n                    else X[feature] - X[feature]\n                )\n            X[f\"{feature}_diff{day}\"] = differenced_values\n            output_features.append(f\"{feature}_diff{day}\")\n            if self.abs_diff:\n                X[f\"{feature}_absdiff{day}\"] = np.abs(\n                        X[f\"{feature}_diff{day}\"]\n                    )\n                output_features.append(f\"{feature}_absdiff{day}\")\n    self.output_features = output_features\n    return X[self.output_features].to_numpy()\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.EraQuantileProcessor","title":"<code>EraQuantileProcessor</code>","text":"<p>             Bases: <code>BasePreProcessor</code></p> <p>Transform features into quantiles by era. :param num_quantiles: Number of quantiles to use for quantile transformation.  :param random_state: Random state for QuantileTransformer.</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>class EraQuantileProcessor(BasePreProcessor):\n    \"\"\"\n    Transform features into quantiles by era.\n    :param num_quantiles: Number of quantiles to use for quantile transformation. \n    :param random_state: Random state for QuantileTransformer.   \n    \"\"\"\n    def __init__(\n        self,\n        num_quantiles: int = 50,\n        random_state: int = 0\n    ):\n        super().__init__()\n        self.num_quantiles = num_quantiles\n        self.random_state = random_state\n\n    def _process_feature(self, group_data: pd.Series) -&gt; pd.Series:\n        quantizer = QuantileTransformer(\n            n_quantiles=self.num_quantiles, random_state=self.random_state\n        )\n        transformed_data = quantizer.fit_transform(group_data.to_frame()).ravel()\n        return pd.Series(transformed_data, index=group_data.index)\n\n    def fit(self, X: Union[np.array, pd.DataFrame], y=None, eras: pd.Series = None):\n        return self\n\n    def transform(\n        self, X: Union[np.array, pd.DataFrame],\n        eras: pd.Series,\n    ) -&gt; np.array:\n        X = pd.DataFrame(X)\n        self.features = [col for col in X.columns]\n        print(f\"Quantiling for {len(self.features)} features.\")\n        X.loc[:, \"era\"] = eras\n        date_groups = X.groupby('era', group_keys=False)\n        output_df = pd.DataFrame()\n        for feature in tqdm(self.features):\n            group_data = date_groups[feature].apply(lambda x: self._process_feature(x))\n            output_df[f\"{feature}_quantile{self.num_quantiles}\"] = group_data\n        return output_df.to_numpy()\n\n    def fit_transform(self, X: Union[np.array, pd.DataFrame], eras: pd.Series):\n        self.fit(X=X, eras=eras)\n        return self.transform(X=X, eras=eras)\n\n    def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n        \"\"\"Return feature names.\"\"\"\n        if not input_features:\n            feature_names = []\n            for feature in self.features:\n                feature_names.append(f\"{feature}_quantile{self.num_quantiles}\")\n        else:\n            feature_names = input_features\n        return feature_names\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.EraQuantileProcessor.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Return feature names.</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n    \"\"\"Return feature names.\"\"\"\n    if not input_features:\n        feature_names = []\n        for feature in self.features:\n            feature_names.append(f\"{feature}_quantile{self.num_quantiles}\")\n    else:\n        feature_names = input_features\n    return feature_names\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.HLOCVAdjuster","title":"<code>HLOCVAdjuster</code>","text":"<p>             Bases: <code>BasePreProcessor</code></p> <p>Adjust HLOCV data for splits and dividends based on ratio of unadjusted and adjusted close prices. NOTE: This step only works with DataFrame input.  Usage in intermediate steps of a scikit-learn Pipeline works with the Pandas set_output API. i.e. pipeline.set_output(transform=\"pandas\").</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>class HLOCVAdjuster(BasePreProcessor):\n    \"\"\" \n    Adjust HLOCV data for splits and dividends based on ratio of unadjusted and adjusted close prices.\n    NOTE: This step only works with DataFrame input. \n    Usage in intermediate steps of a scikit-learn Pipeline works with the Pandas set_output API.\n    i.e. pipeline.set_output(transform=\"pandas\").\n    \"\"\"\n    def __init__(self, open_col=\"open\", high_col=\"high\", low_col=\"low\", \n                 close_col=\"close\", volume_col=\"volume\", adj_close_col=\"adjusted_close\"):\n        super().__init__()\n        self.open_col = open_col\n        self.high_col = high_col\n        self.low_col = low_col\n        self.close_col = close_col\n        self.volume_col = volume_col\n        self.adj_close_col = adj_close_col\n        self.adjusted_col_names = [f\"adjusted_{self.high_col}\", f\"adjusted_{self.low_col}\",\n                                   f\"adjusted_{self.open_col}\", self.adj_close_col, \n                                   f\"adjusted_{self.volume_col}\"]\n\n    def fit(self, X: pd.DataFrame, y=None):\n        self.ratio_ = X[self.close_col] / X[self.adj_close_col]\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; np.array:\n        \"\"\"\n        Adjust open, high, low, close and volume for splits and dividends.\n        :param X: DataFrame with columns: [high, low, open, close, volume] (HLOCV)\n        :return: Array with adjusted HLOCV columns\n        \"\"\"\n        X_copy = X.copy()  \n        X_copy[f\"adjusted_{self.high_col}\"] = X[self.high_col] / self.ratio_\n        X_copy[f\"adjusted_{self.low_col}\"] = X[self.low_col] / self.ratio_\n        X_copy[f\"adjusted_{self.open_col}\"] = X[self.open_col] / self.ratio_\n        X_copy[f\"adjusted_{self.volume_col}\"] = X[self.volume_col] * self.ratio_\n        return X_copy[self.adjusted_col_names].to_numpy()\n\n    def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n        return self.adjusted_col_names\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.HLOCVAdjuster.transform","title":"<code>transform(X)</code>","text":"<p>Adjust open, high, low, close and volume for splits and dividends. :param X: DataFrame with columns: [high, low, open, close, volume] (HLOCV) :return: Array with adjusted HLOCV columns</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; np.array:\n    \"\"\"\n    Adjust open, high, low, close and volume for splits and dividends.\n    :param X: DataFrame with columns: [high, low, open, close, volume] (HLOCV)\n    :return: Array with adjusted HLOCV columns\n    \"\"\"\n    X_copy = X.copy()  \n    X_copy[f\"adjusted_{self.high_col}\"] = X[self.high_col] / self.ratio_\n    X_copy[f\"adjusted_{self.low_col}\"] = X[self.low_col] / self.ratio_\n    X_copy[f\"adjusted_{self.open_col}\"] = X[self.open_col] / self.ratio_\n    X_copy[f\"adjusted_{self.volume_col}\"] = X[self.volume_col] * self.ratio_\n    return X_copy[self.adjusted_col_names].to_numpy()\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.KatsuFeatureGenerator","title":"<code>KatsuFeatureGenerator</code>","text":"<p>             Bases: <code>BasePreProcessor</code></p> <p>Effective feature engineering setup based on Katsu's starter notebook. Based on source by Katsu1110: https://www.kaggle.com/code1110/numeraisignals-starter-for-beginners</p> <p>:param windows: Time interval to apply for window features: </p> <ol> <li> <p>Percentage Rate of change </p> </li> <li> <p>Volatility </p> </li> <li> <p>Moving Average gap </p> </li> </ol> <p>:param ticker_col: Columns with tickers to iterate over. </p> <p>:param close_col: Column name where you have closing price stored. :param num_cores: Number of cores to use for multiprocessing. </p> <p>:param verbose: Print additional information.</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>class KatsuFeatureGenerator(BasePreProcessor):\n    \"\"\"\n    Effective feature engineering setup based on Katsu's starter notebook.\n    Based on source by Katsu1110: https://www.kaggle.com/code1110/numeraisignals-starter-for-beginners\n\n    :param windows: Time interval to apply for window features: \\n\n    1. Percentage Rate of change \\n\n    2. Volatility \\n\n    3. Moving Average gap \\n\n    :param ticker_col: Columns with tickers to iterate over. \\n\n    :param close_col: Column name where you have closing price stored.\n    :param num_cores: Number of cores to use for multiprocessing. \\n\n    :param verbose: Print additional information.\n    \"\"\"\n\n    warnings.filterwarnings(\"ignore\")\n    def __init__(\n        self,\n        windows: list,\n        ticker_col: str = \"ticker\",\n        close_col: str = \"close\",\n        num_cores: int = None,\n        verbose=True\n    ):\n        super().__init__()\n        self.windows = windows\n        self.ticker_col = ticker_col\n        self.close_col = close_col\n        self.num_cores = num_cores if num_cores else os.cpu_count()\n        self.verbose = verbose\n\n    def transform(self, dataf: pd.DataFrame) -&gt; np.array:\n        \"\"\"\n        Multiprocessing feature engineering.\n\n        :param dataf: DataFrame with columns: [ticker, date, open, high, low, close, volume] \\n\n        \"\"\"\n        tickers = dataf.loc[:, self.ticker_col].unique().tolist()\n        if self.verbose:\n            print(\n                f\"Feature engineering for {len(tickers)} tickers using {self.num_cores} CPU cores.\"\n            )\n        dataf_list = [\n            x\n            for _, x in tqdm(\n                dataf.groupby(self.ticker_col), desc=\"Generating ticker DataFrames\"\n            )\n        ]\n        dataf = self._generate_features(dataf_list=dataf_list)\n        output_cols = self.get_feature_names_out()\n        return dataf[output_cols].to_numpy()\n\n    def feature_engineering(self, dataf: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Feature engineering for single ticker.\"\"\"\n        close_series = dataf.loc[:, self.close_col]\n        for x in self.windows:\n            dataf.loc[\n                :, f\"feature_{self.close_col}_ROCP_{x}\"\n            ] = close_series.pct_change(x)\n\n            dataf.loc[:, f\"feature_{self.close_col}_VOL_{x}\"] = (\n                np.log1p(close_series).pct_change().rolling(x).std()\n            )\n\n            dataf.loc[:, f\"feature_{self.close_col}_MA_gap_{x}\"] = (\n                close_series / close_series.rolling(x).mean()\n            )\n\n        dataf.loc[:, \"feature_RSI\"] = self._rsi(close_series)\n        macd, macd_signal = self._macd(close_series)\n        dataf.loc[:, \"feature_MACD\"] = macd\n        dataf.loc[:, \"feature_MACD_signal\"] = macd_signal\n        return dataf\n\n    def _generate_features(self, dataf_list: list) -&gt; pd.DataFrame:\n        \"\"\"Add features for list of ticker DataFrames and concatenate.\"\"\"\n        with Pool(self.num_cores) as p:\n            feature_datafs = list(\n                tqdm(\n                    p.imap(self.feature_engineering, dataf_list),\n                    desc=\"Generating features\",\n                    total=len(dataf_list),\n                )\n            )\n        return pd.concat(feature_datafs)\n\n    @staticmethod\n    def _rsi(close: pd.Series, period: int = 14) -&gt; pd.Series:\n        \"\"\"\n        See source https://github.com/peerchemist/finta\n        and fix https://www.tradingview.com/wiki/Talk:Relative_Strength_Index_(RSI)\n        \"\"\"\n        delta = close.diff()\n        up, down = delta.copy(), delta.copy()\n        up[up &lt; 0] = 0\n        down[down &gt; 0] = 0\n\n        gain = up.ewm(com=(period - 1), min_periods=period).mean()\n        loss = down.abs().ewm(com=(period - 1), min_periods=period).mean()\n\n        rs = gain / loss\n        return pd.Series(100 - (100 / (1 + rs)))\n\n    def _macd(\n        self, close: pd.Series, span1=12, span2=26, span3=9\n    ) -&gt; Tuple[pd.Series, pd.Series]:\n        \"\"\"Compute MACD and MACD signal.\"\"\"\n        exp1 = self.__ema1(close, span1)\n        exp2 = self.__ema1(close, span2)\n        macd = 100 * (exp1 - exp2) / exp2\n        signal = self.__ema1(macd, span3)\n        return macd, signal\n\n    @staticmethod\n    def __ema1(series: pd.Series, span: int) -&gt; pd.Series:\n        \"\"\"Exponential moving average\"\"\"\n        a = 2 / (span + 1)\n        return series.ewm(alpha=a).mean()\n\n    def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n        \"\"\"Return feature names.\"\"\"\n        if not input_features:\n            feature_names = []\n            for x in self.windows:\n                feature_names += [\n                    f\"feature_{self.close_col}_ROCP_{x}\",\n                    f\"feature_{self.close_col}_VOL_{x}\",\n                    f\"feature_{self.close_col}_MA_gap_{x}\",\n                ]\n            feature_names += [\n                \"feature_RSI\",\n                \"feature_MACD\",\n                \"feature_MACD_signal\",\n            ]\n        else:\n            feature_names = input_features\n        return feature_names\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.KatsuFeatureGenerator.__ema1","title":"<code>__ema1(series, span)</code>  <code>staticmethod</code>","text":"<p>Exponential moving average</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>@staticmethod\ndef __ema1(series: pd.Series, span: int) -&gt; pd.Series:\n    \"\"\"Exponential moving average\"\"\"\n    a = 2 / (span + 1)\n    return series.ewm(alpha=a).mean()\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.KatsuFeatureGenerator.feature_engineering","title":"<code>feature_engineering(dataf)</code>","text":"<p>Feature engineering for single ticker.</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>def feature_engineering(self, dataf: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Feature engineering for single ticker.\"\"\"\n    close_series = dataf.loc[:, self.close_col]\n    for x in self.windows:\n        dataf.loc[\n            :, f\"feature_{self.close_col}_ROCP_{x}\"\n        ] = close_series.pct_change(x)\n\n        dataf.loc[:, f\"feature_{self.close_col}_VOL_{x}\"] = (\n            np.log1p(close_series).pct_change().rolling(x).std()\n        )\n\n        dataf.loc[:, f\"feature_{self.close_col}_MA_gap_{x}\"] = (\n            close_series / close_series.rolling(x).mean()\n        )\n\n    dataf.loc[:, \"feature_RSI\"] = self._rsi(close_series)\n    macd, macd_signal = self._macd(close_series)\n    dataf.loc[:, \"feature_MACD\"] = macd\n    dataf.loc[:, \"feature_MACD_signal\"] = macd_signal\n    return dataf\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.KatsuFeatureGenerator.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Return feature names.</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n    \"\"\"Return feature names.\"\"\"\n    if not input_features:\n        feature_names = []\n        for x in self.windows:\n            feature_names += [\n                f\"feature_{self.close_col}_ROCP_{x}\",\n                f\"feature_{self.close_col}_VOL_{x}\",\n                f\"feature_{self.close_col}_MA_gap_{x}\",\n            ]\n        feature_names += [\n            \"feature_RSI\",\n            \"feature_MACD\",\n            \"feature_MACD_signal\",\n        ]\n    else:\n        feature_names = input_features\n    return feature_names\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.KatsuFeatureGenerator.transform","title":"<code>transform(dataf)</code>","text":"<p>Multiprocessing feature engineering.</p> <p>:param dataf: DataFrame with columns: [ticker, date, open, high, low, close, volume]</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>def transform(self, dataf: pd.DataFrame) -&gt; np.array:\n    \"\"\"\n    Multiprocessing feature engineering.\n\n    :param dataf: DataFrame with columns: [ticker, date, open, high, low, close, volume] \\n\n    \"\"\"\n    tickers = dataf.loc[:, self.ticker_col].unique().tolist()\n    if self.verbose:\n        print(\n            f\"Feature engineering for {len(tickers)} tickers using {self.num_cores} CPU cores.\"\n        )\n    dataf_list = [\n        x\n        for _, x in tqdm(\n            dataf.groupby(self.ticker_col), desc=\"Generating ticker DataFrames\"\n        )\n    ]\n    dataf = self._generate_features(dataf_list=dataf_list)\n    output_cols = self.get_feature_names_out()\n    return dataf[output_cols].to_numpy()\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.LagPreProcessor","title":"<code>LagPreProcessor</code>","text":"<p>             Bases: <code>BasePreProcessor</code></p> <p>Add lag features based on given windows.</p> <p>:param windows: All lag windows to process for all features. </p> <p>[5, 10, 15, 20] by default (4 weeks lookback)</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>class LagPreProcessor(BasePreProcessor):\n    \"\"\"\n    Add lag features based on given windows.\n\n    :param windows: All lag windows to process for all features. \\n\n    [5, 10, 15, 20] by default (4 weeks lookback) \\n\n    \"\"\"\n\n    def __init__(self, windows: list = None,):\n        super().__init__()\n        self.windows = windows if windows else [5, 10, 15, 20]\n\n    def fit(self, X: Union[np.array, pd.DataFrame], y=None, tickers: pd.Series = None):\n        return self\n\n    def transform(self, X: Union[np.array, pd.DataFrame], tickers: pd.Series) -&gt; np.array:\n        X = pd.DataFrame(X)\n        feature_cols = X.columns.tolist()\n        X[\"ticker\"] = tickers\n        ticker_groups = X.groupby(\"ticker\")\n        output_features = []\n        for feature in tqdm(feature_cols, desc=\"Lag feature generation\"):\n            feature_group = ticker_groups[feature]\n            for day in self.windows:\n                shifted = feature_group.shift(day)\n                X.loc[:, f\"{feature}_lag{day}\"] = shifted\n                output_features.append(f\"{feature}_lag{day}\")\n        self.output_features = output_features\n        return X[output_features].to_numpy()\n\n    def fit_transform(self, X: Union[np.array, pd.DataFrame], tickers: pd.Series):\n        self.fit(X=X, tickers=tickers)\n        return self.transform(X=X, tickers=tickers)\n\n    def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n        \"\"\"Return feature names.\"\"\"\n        return self.output_features if not input_features else input_features\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.LagPreProcessor.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Return feature names.</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n    \"\"\"Return feature names.\"\"\"\n    return self.output_features if not input_features else input_features\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.MinimumDataFilter","title":"<code>MinimumDataFilter</code>","text":"<p>             Bases: <code>BasePreProcessor</code></p> <p>Filter dates and tickers based on minimum data requirements.  NOTE: This step only works with DataFrame input.</p> <p>:param min_samples_date: Minimum number of samples per date. Defaults to 200. :param min_samples_ticker: Minimum number of samples per ticker. Defaults to 1200. :param blacklist_tickers: List of tickers to exclude from the dataset. Defaults to None. :param date_col: Column name for date. Defaults to \"date\". :param ticker_col: Column name for ticker. Defaults to \"bloomberg_ticker\".</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>class MinimumDataFilter(BasePreProcessor):\n    \"\"\" \n    Filter dates and tickers based on minimum data requirements. \n    NOTE: This step only works with DataFrame input.\n\n    :param min_samples_date: Minimum number of samples per date. Defaults to 200.\n    :param min_samples_ticker: Minimum number of samples per ticker. Defaults to 1200.\n    :param blacklist_tickers: List of tickers to exclude from the dataset. Defaults to None.\n    :param date_col: Column name for date. Defaults to \"date\".\n    :param ticker_col: Column name for ticker. Defaults to \"bloomberg_ticker\".\n    \"\"\"\n    def __init__(self, min_samples_date: int = 200, min_samples_ticker: int = 1200, blacklist_tickers: list = None, date_col=\"date\", ticker_col=\"bloomberg_ticker\"):\n        super().__init__()\n        self.min_samples_date = min_samples_date\n        self.min_samples_ticker = min_samples_ticker\n        self.blacklist_tickers = blacklist_tickers\n        self.date_col = date_col\n        self.ticker_col = ticker_col\n\n    def fit(self, X: pd.DataFrame, y=None):\n        self.feature_names_out_ = X.columns.tolist()\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; np.array:\n        \"\"\"\n        Filter dates and tickers based on minimum data requirements.\n        :param X: DataFrame with columns: [ticker_col, date_col, open, high, low, close, volume] (HLOCV)\n        :return: Array with filtered DataFrame\n        \"\"\"\n        filtered_data = X.groupby(self.date_col).filter(lambda x: len(x) &gt;= self.min_samples_date)\n        records_per_ticker = (\n            filtered_data.reset_index(drop=False)\n            .groupby(self.ticker_col)[self.date_col]\n            .nunique()\n            .reset_index()\n            .sort_values(by=self.date_col)\n        )\n        tickers_with_records = records_per_ticker.query(f\"{self.date_col} &gt;= {self.min_samples_ticker}\")[self.ticker_col].values\n        filtered_data = filtered_data.loc[filtered_data[self.ticker_col].isin(tickers_with_records)].reset_index(drop=True)\n\n        if self.blacklist_tickers:\n            filtered_data = filtered_data.loc[~filtered_data[self.ticker_col].isin(self.blacklist_tickers)]\n\n        return filtered_data.to_numpy()\n\n    def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n        check_is_fitted(self)\n        return self.feature_names_out_ if not input_features else input_features\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.MinimumDataFilter.transform","title":"<code>transform(X)</code>","text":"<p>Filter dates and tickers based on minimum data requirements. :param X: DataFrame with columns: [ticker_col, date_col, open, high, low, close, volume] (HLOCV) :return: Array with filtered DataFrame</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; np.array:\n    \"\"\"\n    Filter dates and tickers based on minimum data requirements.\n    :param X: DataFrame with columns: [ticker_col, date_col, open, high, low, close, volume] (HLOCV)\n    :return: Array with filtered DataFrame\n    \"\"\"\n    filtered_data = X.groupby(self.date_col).filter(lambda x: len(x) &gt;= self.min_samples_date)\n    records_per_ticker = (\n        filtered_data.reset_index(drop=False)\n        .groupby(self.ticker_col)[self.date_col]\n        .nunique()\n        .reset_index()\n        .sort_values(by=self.date_col)\n    )\n    tickers_with_records = records_per_ticker.query(f\"{self.date_col} &gt;= {self.min_samples_ticker}\")[self.ticker_col].values\n    filtered_data = filtered_data.loc[filtered_data[self.ticker_col].isin(tickers_with_records)].reset_index(drop=True)\n\n    if self.blacklist_tickers:\n        filtered_data = filtered_data.loc[~filtered_data[self.ticker_col].isin(self.blacklist_tickers)]\n\n    return filtered_data.to_numpy()\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.PandasTaFeatureGenerator","title":"<code>PandasTaFeatureGenerator</code>","text":"<p>             Bases: <code>BasePreProcessor</code></p> <p>Generate features with pandas-ta. https://github.com/twopirllc/pandas-ta Usage in Pipeline works only with Pandas API.  Run <code>.set_output(\"pandas\")</code> on your pipeline first.</p> <p>:param strategy: Valid Pandas Ta strategy. </p> <p>For more information on creating a strategy, see: </p> <p>https://github.com/twopirllc/pandas-ta#pandas-ta-strategy </p> <p>By default, a strategy with RSI(14) and RSI(60) is used. </p> <p>:param ticker_col: Column name for grouping by tickers. </p> <p>:param num_cores: Number of cores to use for multiprocessing. </p> <p>By default, all available cores are used.</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>class PandasTaFeatureGenerator(BasePreProcessor):\n    \"\"\"\n    Generate features with pandas-ta.\n    https://github.com/twopirllc/pandas-ta\n    Usage in Pipeline works only with Pandas API. \n    Run `.set_output(\"pandas\")` on your pipeline first.\n\n    :param strategy: Valid Pandas Ta strategy. \\n\n    For more information on creating a strategy, see: \\n\n    https://github.com/twopirllc/pandas-ta#pandas-ta-strategy \\n\n    By default, a strategy with RSI(14) and RSI(60) is used. \\n\n    :param ticker_col: Column name for grouping by tickers. \\n\n    :param num_cores: Number of cores to use for multiprocessing. \\n\n    By default, all available cores are used. \\n\n    \"\"\"\n    def __init__(self, \n                 strategy: ta.Strategy = None,\n                 ticker_col: str = \"ticker\",\n                 num_cores: int = None,\n    ):\n        super().__init__()\n        self.ticker_col = ticker_col\n        self.num_cores = num_cores if num_cores else os.cpu_count()\n        standard_strategy = ta.Strategy(name=\"standard\", \n                                        ta=[{\"kind\": \"rsi\", \"length\": 14, \"col_names\": (\"feature_RSI_14\")},\n                                            {\"kind\": \"rsi\", \"length\": 60, \"col_names\": (\"feature_RSI_60\")}])\n        self.strategy = strategy if strategy is not None else standard_strategy\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Main feature generation method. \\n \n        :param X: DataFrame with columns: [ticker, date, open, high, low, close, volume] \\n\n        :return: PandasTA features\n        \"\"\"\n        initial_features = X.columns.tolist()\n        dataf_list = [\n            x\n            for _, x in tqdm(\n                X.groupby(self.ticker_col), desc=\"Generating ticker DataFrames\"\n            )\n        ]\n        X = self._generate_features(dataf_list=dataf_list)\n        output_df = X.drop(columns=initial_features)\n        self.output_cols = output_df.columns.tolist()\n        return output_df\n\n    def _generate_features(self, dataf_list: List[pd.DataFrame]) -&gt; pd.DataFrame:\n        \"\"\"\n        Add features for list of ticker DataFrames and concatenate.\n        :param dataf_list: List of DataFrames for each ticker.\n        :return: Concatenated DataFrame for all full list with features added.\n        \"\"\"\n        with Pool(self.num_cores) as p:\n            feature_datafs = list(\n                tqdm(\n                    p.imap(self.add_features, dataf_list),\n                    desc=\"Generating pandas-ta features\",\n                    total=len(dataf_list),\n                )\n            )\n        return pd.concat(feature_datafs)\n\n    def add_features(self, ticker_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        The TA strategy is applied to the DataFrame here.\n        :param ticker_df: DataFrame for a single ticker.\n        :return: DataFrame with features added.\n        \"\"\"\n        # We use a different multiprocessing engine so shutting off pandas_ta's multiprocessing\n        ticker_df.ta.cores = 0\n        # Run strategy\n        ticker_df.ta.strategy(self.strategy)\n        return ticker_df\n\n    def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n        return self.output_cols if not input_features else input_features\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.PandasTaFeatureGenerator.add_features","title":"<code>add_features(ticker_df)</code>","text":"<p>The TA strategy is applied to the DataFrame here. :param ticker_df: DataFrame for a single ticker. :return: DataFrame with features added.</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>def add_features(self, ticker_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    The TA strategy is applied to the DataFrame here.\n    :param ticker_df: DataFrame for a single ticker.\n    :return: DataFrame with features added.\n    \"\"\"\n    # We use a different multiprocessing engine so shutting off pandas_ta's multiprocessing\n    ticker_df.ta.cores = 0\n    # Run strategy\n    ticker_df.ta.strategy(self.strategy)\n    return ticker_df\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.PandasTaFeatureGenerator.transform","title":"<code>transform(X)</code>","text":"<p>Main feature generation method. </p> <p>:param X: DataFrame with columns: [ticker, date, open, high, low, close, volume] </p> <p>:return: PandasTA features</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Main feature generation method. \\n \n    :param X: DataFrame with columns: [ticker, date, open, high, low, close, volume] \\n\n    :return: PandasTA features\n    \"\"\"\n    initial_features = X.columns.tolist()\n    dataf_list = [\n        x\n        for _, x in tqdm(\n            X.groupby(self.ticker_col), desc=\"Generating ticker DataFrames\"\n        )\n    ]\n    X = self._generate_features(dataf_list=dataf_list)\n    output_df = X.drop(columns=initial_features)\n    self.output_cols = output_df.columns.tolist()\n    return output_df\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.ReduceMemoryProcessor","title":"<code>ReduceMemoryProcessor</code>","text":"<p>             Bases: <code>BasePreProcessor</code></p> <p>Reduce memory usage as much as possible.</p> <p>Credits to kainsama and others for writing about memory usage reduction for Numerai data: https://forum.numer.ai/t/reducing-memory/313</p> <p>:param deep_mem_inspect: Introspect the data deeply by interrogating object dtypes. Yields a more accurate representation of memory usage if you have complex object columns. :param verbose: Print memory usage before and after optimization.</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>class ReduceMemoryProcessor(BasePreProcessor):\n    \"\"\"\n    Reduce memory usage as much as possible.\n\n    Credits to kainsama and others for writing about memory usage reduction for Numerai data:\n    https://forum.numer.ai/t/reducing-memory/313\n\n    :param deep_mem_inspect: Introspect the data deeply by interrogating object dtypes.\n    Yields a more accurate representation of memory usage if you have complex object columns.\n    :param verbose: Print memory usage before and after optimization.\n    \"\"\"\n\n    def __init__(self, deep_mem_inspect=False, verbose=True):\n        super().__init__()\n        self.deep_mem_inspect = deep_mem_inspect\n        self.verbose = verbose\n\n    def transform(self, dataf: Union[np.array, pd.DataFrame]) -&gt; np.array:\n        return self._reduce_mem_usage(dataf).to_numpy()\n\n    def _reduce_mem_usage(self, dataf: Union[np.array, pd.DataFrame]) -&gt; pd.DataFrame:\n        \"\"\"\n        Iterate through all columns and modify the numeric column types\n        to reduce memory usage.\n        \"\"\"\n        dataf = pd.DataFrame(dataf)\n        self.output_cols = dataf.columns.tolist()\n        start_memory_usage = (\n            dataf.memory_usage(deep=self.deep_mem_inspect).sum() / 1024**2\n        )\n        if self.verbose:\n            print(\n                f\"Memory usage of DataFrame is {round(start_memory_usage, 2)} MB\"\n            )\n\n        for col in dataf.columns:\n            col_type = dataf[col].dtype.name\n\n            if col_type not in [\n                \"object\",\n                \"category\",\n                \"datetime64[ns, UTC]\",\n                \"datetime64[ns]\",\n            ]:\n                c_min = dataf[col].min()\n                c_max = dataf[col].max()\n                if str(col_type)[:3] == \"int\":\n                    if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max:\n                        dataf[col] = dataf[col].astype(np.int16)\n                    elif (\n                        c_min &gt; np.iinfo(np.int16).min\n                        and c_max &lt; np.iinfo(np.int16).max\n                    ):\n                        dataf[col] = dataf[col].astype(np.int16)\n                    elif (\n                        c_min &gt; np.iinfo(np.int32).min\n                        and c_max &lt; np.iinfo(np.int32).max\n                    ):\n                        dataf[col] = dataf[col].astype(np.int32)\n                    elif (\n                        c_min &gt; np.iinfo(np.int64).min\n                        and c_max &lt; np.iinfo(np.int64).max\n                    ):\n                        dataf[col] = dataf[col].astype(np.int64)\n                else:\n                    if (\n                        c_min &gt; np.finfo(np.float16).min\n                        and c_max &lt; np.finfo(np.float16).max\n                    ):\n                        dataf[col] = dataf[col].astype(np.float16)\n                    elif (\n                        c_min &gt; np.finfo(np.float32).min\n                        and c_max &lt; np.finfo(np.float32).max\n                    ):\n                        dataf[col] = dataf[col].astype(np.float32)\n                    else:\n                        dataf[col] = dataf[col].astype(np.float64)\n\n        end_memory_usage = (\n            dataf.memory_usage(deep=self.deep_mem_inspect).sum() / 1024**2\n        )\n        if self.verbose:\n            print(\n                f\"Memory usage after optimization is: {round(end_memory_usage, 2)} MB\"\n            )\n            print(\n                f\"Usage decreased by {round(100 * (start_memory_usage - end_memory_usage) / start_memory_usage, 2)}%\"\n            )\n        return dataf\n\n    def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n        \"\"\"Return feature names.\"\"\"\n        return self.output_cols if not input_features else input_features\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.ReduceMemoryProcessor.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Return feature names.</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n    \"\"\"Return feature names.\"\"\"\n    return self.output_cols if not input_features else input_features\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.TickerMapper","title":"<code>TickerMapper</code>","text":"<p>             Bases: <code>BasePreProcessor</code></p> <p>Map ticker from one format to another. </p> <p>:param ticker_col: Column used for mapping. Must already be present in the input data. </p> <p>:param target_ticker_format: Format to map tickers to. Must be present in the ticker map. </p> <p>For default mapper supported ticker formats are: ['ticker', 'bloomberg_ticker', 'yahoo'] </p> <p>:param mapper_path: Path to CSV file containing at least ticker_col and target_ticker_format columns. </p> <p>Can be either a web link of local path. Numerai Signals mapping by default.</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>class TickerMapper(BasePreProcessor):\n    \"\"\"\n    Map ticker from one format to another. \\n\n    :param ticker_col: Column used for mapping. Must already be present in the input data. \\n\n    :param target_ticker_format: Format to map tickers to. Must be present in the ticker map. \\n\n    For default mapper supported ticker formats are: ['ticker', 'bloomberg_ticker', 'yahoo'] \\n\n    :param mapper_path: Path to CSV file containing at least ticker_col and target_ticker_format columns. \\n\n    Can be either a web link of local path. Numerai Signals mapping by default.\n    \"\"\"\n\n    def __init__(\n        self, ticker_col: str = \"ticker\", target_ticker_format: str = \"bloomberg_ticker\",\n        mapper_path: str = \"https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_ticker_map_w_bbg.csv\"\n    ):\n        super().__init__()\n        self.ticker_col = ticker_col\n        self.target_ticker_format = target_ticker_format\n\n        self.signals_map_path = mapper_path\n        self.ticker_map = pd.read_csv(self.signals_map_path)\n\n        assert (\n            self.ticker_col in self.ticker_map.columns\n        ), f\"Ticker column '{self.ticker_col}' is not available in ticker mapping.\"\n        assert (\n            self.target_ticker_format in self.ticker_map.columns\n        ), f\"Target ticker column '{self.target_ticker_format}' is not available in ticker mapping.\"\n\n        self.mapping = dict(\n            self.ticker_map[[self.ticker_col, self.target_ticker_format]].values\n        )\n\n    def transform(self, X: Union[np.array, pd.Series]) -&gt; np.array:\n        \"\"\"\n        Transform ticker column.\n        :param X: Ticker column\n        :return tickers: Mapped tickers\n        \"\"\"\n        tickers = pd.DataFrame(X, columns=[self.ticker_col])[self.ticker_col].map(self.mapping)\n        return tickers.to_numpy()\n\n    def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n        return [self.target_ticker_format] if not input_features else input_features\n</code></pre>"},{"location":"api/#numerblox.preprocessing.signals.TickerMapper.transform","title":"<code>transform(X)</code>","text":"<p>Transform ticker column. :param X: Ticker column :return tickers: Mapped tickers</p> Source code in <code>numerblox/preprocessing/signals.py</code> <pre><code>def transform(self, X: Union[np.array, pd.Series]) -&gt; np.array:\n    \"\"\"\n    Transform ticker column.\n    :param X: Ticker column\n    :return tickers: Mapped tickers\n    \"\"\"\n    tickers = pd.DataFrame(X, columns=[self.ticker_col])[self.ticker_col].map(self.mapping)\n    return tickers.to_numpy()\n</code></pre>"},{"location":"api/#numerblox.targets.BaseTargetProcessor","title":"<code>BaseTargetProcessor</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Common functionality for preprocessors and postprocessors.</p> Source code in <code>numerblox/targets.py</code> <pre><code>class BaseTargetProcessor(BaseEstimator, TransformerMixin):\n    \"\"\"Common functionality for preprocessors and postprocessors.\"\"\"\n\n    def __init__(self):\n        ...\n\n    def fit(self, X, y=None):\n        return self\n\n    @abstractmethod\n    def transform(\n        self, X: Union[np.array, pd.DataFrame], y=None, **kwargs\n    ) -&gt; pd.DataFrame:\n        ...\n\n    def __call__(\n        self, X: Union[np.array, pd.DataFrame], y=None, **kwargs\n    ) -&gt; pd.DataFrame:\n        return self.transform(X=X, y=y, **kwargs)\n\n    @abstractmethod\n    def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n        ...\n</code></pre>"},{"location":"api/#numerblox.targets.BayesianGMMTargetProcessor","title":"<code>BayesianGMMTargetProcessor</code>","text":"<p>             Bases: <code>BaseTargetProcessor</code></p> <p>Generate synthetic (fake) target using a Bayesian Gaussian Mixture model. </p> <p>Based on Michael Oliver's GitHub Gist implementation: </p> <p>https://gist.github.com/the-moliver/dcdd2862dc2c78dda600f1b449071c93</p> <p>:param n_components: Number of components for fitting Bayesian Gaussian Mixture Model.</p> Source code in <code>numerblox/targets.py</code> <pre><code>class BayesianGMMTargetProcessor(BaseTargetProcessor):\n    \"\"\"\n    Generate synthetic (fake) target using a Bayesian Gaussian Mixture model. \\n\n    Based on Michael Oliver's GitHub Gist implementation: \\n\n    https://gist.github.com/the-moliver/dcdd2862dc2c78dda600f1b449071c93\n\n    :param n_components: Number of components for fitting Bayesian Gaussian Mixture Model.\n    \"\"\"\n    def __init__(\n        self,\n        n_components: int = 3,\n    ):\n        super().__init__()\n        self.n_components = n_components\n        self.ridge = Ridge(fit_intercept=False)\n        self.bins = [0, 0.05, 0.25, 0.75, 0.95, 1]\n\n    def fit(self, X: pd.DataFrame, y: pd.Series, eras: pd.Series):\n        \"\"\"\n        Fit Bayesian Gaussian Mixture model on coefficients and normalize.\n        :param X: DataFrame containing features.\n        :param y: Series containing real target.\n        :param eras: Series containing era information.\n        \"\"\"\n        bgmm = BayesianGaussianMixture(n_components=self.n_components)\n        coefs = self._get_coefs(dataf=X, y=y, eras=eras)\n        bgmm.fit(coefs)\n        # make probability of sampling each component equal to better balance rare regimes\n        bgmm.weights_[:] = 1 / self.n_components\n        self.bgmm_ = bgmm\n        return self\n\n    def transform(self, X: pd.DataFrame, eras: pd.Series) -&gt; np.array:\n        \"\"\"\n        Main method for generating fake target.\n        :param X: DataFrame containing features.\n        :param eras: Series containing era information.\n        \"\"\"\n        check_is_fitted(self, \"bgmm_\")\n        assert len(X) == len(eras), \"X and eras must be same length.\"\n        all_eras = eras.unique().tolist()\n        # Scale data between 0 and 1\n        X = X.astype(float)\n        X /= X.max()\n        X -= 0.5\n        X.loc[:, 'era'] = eras\n\n        fake_target = self._generate_target(dataf=X, all_eras=all_eras)\n        return fake_target\n\n    def _get_coefs(self, dataf: pd.DataFrame, y: pd.Series, eras: pd.Series) -&gt; np.ndarray:\n        \"\"\"\n        Generate coefficients for BGMM.\n        :param dataf: DataFrame containing features.\n        :param y: Series containing real target.\n        \"\"\"\n        coefs = []\n        dataf.loc[:, 'era'] = eras\n        dataf.loc[:, 'target'] = y\n        all_eras = dataf['era'].unique().tolist()\n        for era in all_eras:\n            era_df = dataf[dataf['era'] == era]\n            era_y = era_df.loc[:, 'target']\n            era_df = era_df.drop(columns=[\"era\", \"target\"])\n            self.ridge.fit(era_df, era_y)\n            coefs.append(self.ridge.coef_)\n        stacked_coefs = np.vstack(coefs)\n        return stacked_coefs\n\n    def _generate_target(\n        self, dataf: pd.DataFrame, all_eras: list\n    ) -&gt; np.ndarray:\n        \"\"\"Generate fake target using Bayesian Gaussian Mixture model.\"\"\"\n        fake_target = []\n        for era in tqdm(all_eras, desc=\"Generating fake target\"):\n            features = dataf[dataf['era'] == era]\n            features = features.drop(columns=[\"era\", \"target\"])\n            # Sample a set of weights from GMM\n            beta, _ = self.bgmm_.sample(1)\n            # Create fake continuous target\n            fake_targ = features @ beta[0]\n            # Bin fake target like real target\n            fake_targ = (rankdata(fake_targ) - 0.5) / len(fake_targ)\n            fake_targ = (np.digitize(fake_targ, self.bins) - 1) / 4\n            fake_target.append(fake_targ)\n        return np.concatenate(fake_target)\n\n    def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n        \"\"\"Return feature names.\"\"\"\n        return [\"fake_target\"] if not input_features else input_features\n</code></pre>"},{"location":"api/#numerblox.targets.BayesianGMMTargetProcessor.fit","title":"<code>fit(X, y, eras)</code>","text":"<p>Fit Bayesian Gaussian Mixture model on coefficients and normalize. :param X: DataFrame containing features. :param y: Series containing real target. :param eras: Series containing era information.</p> Source code in <code>numerblox/targets.py</code> <pre><code>def fit(self, X: pd.DataFrame, y: pd.Series, eras: pd.Series):\n    \"\"\"\n    Fit Bayesian Gaussian Mixture model on coefficients and normalize.\n    :param X: DataFrame containing features.\n    :param y: Series containing real target.\n    :param eras: Series containing era information.\n    \"\"\"\n    bgmm = BayesianGaussianMixture(n_components=self.n_components)\n    coefs = self._get_coefs(dataf=X, y=y, eras=eras)\n    bgmm.fit(coefs)\n    # make probability of sampling each component equal to better balance rare regimes\n    bgmm.weights_[:] = 1 / self.n_components\n    self.bgmm_ = bgmm\n    return self\n</code></pre>"},{"location":"api/#numerblox.targets.BayesianGMMTargetProcessor.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Return feature names.</p> Source code in <code>numerblox/targets.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n    \"\"\"Return feature names.\"\"\"\n    return [\"fake_target\"] if not input_features else input_features\n</code></pre>"},{"location":"api/#numerblox.targets.BayesianGMMTargetProcessor.transform","title":"<code>transform(X, eras)</code>","text":"<p>Main method for generating fake target. :param X: DataFrame containing features. :param eras: Series containing era information.</p> Source code in <code>numerblox/targets.py</code> <pre><code>def transform(self, X: pd.DataFrame, eras: pd.Series) -&gt; np.array:\n    \"\"\"\n    Main method for generating fake target.\n    :param X: DataFrame containing features.\n    :param eras: Series containing era information.\n    \"\"\"\n    check_is_fitted(self, \"bgmm_\")\n    assert len(X) == len(eras), \"X and eras must be same length.\"\n    all_eras = eras.unique().tolist()\n    # Scale data between 0 and 1\n    X = X.astype(float)\n    X /= X.max()\n    X -= 0.5\n    X.loc[:, 'era'] = eras\n\n    fake_target = self._generate_target(dataf=X, all_eras=all_eras)\n    return fake_target\n</code></pre>"},{"location":"api/#numerblox.targets.SignalsTargetProcessor","title":"<code>SignalsTargetProcessor</code>","text":"<p>             Bases: <code>BaseTargetProcessor</code></p> <p>Engineer targets for Numerai Signals. </p> <p>More information on implements Numerai Signals targets: </p> <p>https://forum.numer.ai/t/decoding-the-signals-target/2501</p> <p>:param price_col: Column from which target will be derived. </p> <p>:param windows: Timeframes to use for engineering targets. 10 and 20-day by default. </p> <p>:param bins: Binning used to create group targets. Nomi binning by default. </p> <p>:param labels: Scaling for binned target. Must be same length as resulting bins (bins-1). Numerai labels by default.</p> Source code in <code>numerblox/targets.py</code> <pre><code>class SignalsTargetProcessor(BaseTargetProcessor):\n    \"\"\"\n    Engineer targets for Numerai Signals. \\n\n    More information on implements Numerai Signals targets: \\n\n    https://forum.numer.ai/t/decoding-the-signals-target/2501\n\n    :param price_col: Column from which target will be derived. \\n\n    :param windows: Timeframes to use for engineering targets. 10 and 20-day by default. \\n\n    :param bins: Binning used to create group targets. Nomi binning by default. \\n\n    :param labels: Scaling for binned target. Must be same length as resulting bins (bins-1). Numerai labels by default.\n    \"\"\"\n\n    def __init__(\n        self,\n        price_col: str = \"close\",\n        windows: list = None,\n        bins: list = None,\n        labels: list = None,\n    ):\n        super().__init__()\n        self.price_col = price_col\n        self.windows = windows if windows else [10, 20]\n        self.bins = bins if bins else [0, 0.05, 0.25, 0.75, 0.95, 1]\n        self.labels = labels if labels else [0, 0.25, 0.50, 0.75, 1]\n\n    def transform(self, dataf: pd.DataFrame, eras: pd.Series) -&gt; np.array:\n        for window in tqdm(self.windows, desc=\"Signals target engineering windows\"):\n            dataf.loc[:, f\"target_{window}d_raw\"] = (\n                dataf[self.price_col].pct_change(periods=window).shift(-window)\n            )\n            era_groups = dataf.groupby(eras)\n\n            dataf.loc[:, f\"target_{window}d_rank\"] = era_groups[\n                f\"target_{window}d_raw\"\n            ].rank(pct=True, method=\"first\")\n            dataf.loc[:, f\"target_{window}d_group\"] = era_groups[\n                f\"target_{window}d_rank\"\n            ].transform(\n                lambda group: pd.cut(\n                    group, bins=self.bins, labels=self.labels, include_lowest=True\n                )\n            )\n        output_cols = self.get_feature_names_out()\n        return dataf[output_cols].to_numpy()\n\n    def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n        \"\"\"Return feature names of Signals targets. \"\"\"\n        if not input_features:\n            feature_names = []\n            for window in self.windows:\n                feature_names.append(f\"target_{window}d_raw\")\n                feature_names.append(f\"target_{window}d_rank\")\n                feature_names.append(f\"target_{window}d_group\")\n        else:\n            feature_names = input_features\n        return feature_names\n</code></pre>"},{"location":"api/#numerblox.targets.SignalsTargetProcessor.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Return feature names of Signals targets.</p> Source code in <code>numerblox/targets.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n    \"\"\"Return feature names of Signals targets. \"\"\"\n    if not input_features:\n        feature_names = []\n        for window in self.windows:\n            feature_names.append(f\"target_{window}d_raw\")\n            feature_names.append(f\"target_{window}d_rank\")\n            feature_names.append(f\"target_{window}d_group\")\n    else:\n        feature_names = input_features\n    return feature_names\n</code></pre>"},{"location":"api/#numerblox.meta.CrossValEstimator","title":"<code>CrossValEstimator</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Split your data into multiple folds and fit an estimator on each fold. For transforms predictions are concatenated into a 2D array. :param cv: Cross validation object that follows scikit-learn conventions. :param estimator: Estimator to fit on each fold. :param evaluation_func: Custom evaluation logic that is executed on validation data for each fold. Must accepts as input y_true and y_pred. For example, evaluation_func can handle logging metrics for each fold. Anything that evaluation_func returns is stored in <code>self.eval_results_</code>. :param predict_func: Name of the function that will be used for prediction. Must be one of 'predict', 'predict_proba', 'predict_log_proba'. For example, XGBRegressor has 'predict' and 'predict_proba' functions. :param verbose: Whether to print progress.</p> Source code in <code>numerblox/meta.py</code> <pre><code>class CrossValEstimator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Split your data into multiple folds and fit an estimator on each fold.\n    For transforms predictions are concatenated into a 2D array.\n    :param cv: Cross validation object that follows scikit-learn conventions.\n    :param estimator: Estimator to fit on each fold.\n    :param evaluation_func: Custom evaluation logic that is executed on validation data for each fold. Must accepts as input y_true and y_pred.\n    For example, evaluation_func can handle logging metrics for each fold.\n    Anything that evaluation_func returns is stored in `self.eval_results_`.\n    :param predict_func: Name of the function that will be used for prediction.\n    Must be one of 'predict', 'predict_proba', 'predict_log_proba'.\n    For example, XGBRegressor has 'predict' and 'predict_proba' functions.\n    :param verbose: Whether to print progress.\n    \"\"\"\n    def __init__(self, estimator: BaseEstimator, cv: BaseCrossValidator, evaluation_func=None, predict_func=\"predict\", verbose=False):\n        super().__init__()\n        self.cv = cv\n        if not hasattr(self.cv, \"split\") or isinstance(self.cv, str):\n            raise ValueError(\"cv must be a valid sklearn cv object withat least a 'split' function.\")\n        self.estimator = estimator\n        self.estimator_name = estimator.__class__.__name__\n        self.evaluation_func = evaluation_func\n\n        if predict_func not in [\"predict\", \"predict_proba\", \"predict_log_proba\"]:\n            raise ValueError(\"predict_func must be 'predict', 'predict_proba', or 'predict_log_proba'.\")\n        self.predict_func = predict_func\n        assert hasattr(self.estimator, self.predict_func), f\"Estimator {self.estimator_name} does not have {self.predict_func} function.\"\n        self.verbose = verbose\n\n    def fit(self, X: Union[np.array, pd.DataFrame], y: Union[np.array, pd.Series], **kwargs):\n        \"\"\" Use cross validation object to fit estimators. \"\"\"\n        self.estimators_ = []\n        self.eval_results_ = []\n        if isinstance(X, (pd.Series, pd.DataFrame)):\n            X = X.reset_index(drop=True).values\n        if isinstance(y, (pd.Series, pd.DataFrame)):\n            y = y.reset_index(drop=True).values\n        for i, (train_idx, val_idx) in tqdm(enumerate(self.cv.split(X, y)), \n                                            desc=f\"CrossValEstimator Fitting. Estimator='{self.estimator_name}'\", \n                                            total=self.cv.get_n_splits(), \n                                            disable=not self.verbose):\n            estimator = clone(self.estimator)\n            if self.verbose:\n                print(f\"Fitting {self.estimator_name} on fold {len(self.estimators_)}\")\n\n\n            estimator.fit(X[train_idx], y[train_idx], **kwargs)\n\n            # Execute custom evaluation logic\n            if self.evaluation_func:\n                if self.verbose:\n                    print(f\"Running evaluation on fold {len(self.estimators_)}\")\n\n                y_pred = getattr(estimator, self.predict_func)(X[val_idx])\n                y_pred = self._postprocess_pred(y_pred)\n                eval_fold = self.evaluation_func(y[val_idx], y_pred)\n                if self.verbose:\n                    print(f\"CrossValEstimator (estimator='{self.estimator_name}'): Fold '{i}' evaluation results: '{eval_fold}'\")\n                self.eval_results_.append(eval_fold)\n\n            self.estimators_.append(estimator)\n\n            # Store output shape by doing inference on 1st sample of training set\n            if i == 0:\n                sample_prediction = getattr(estimator, self.predict_func)(X[train_idx][:1])\n                sample_prediction = self._postprocess_pred(sample_prediction)\n                self.output_shape_ = sample_prediction.shape[1:]\n                self.multi_output_ = len(y.shape) &gt; 1\n                self.n_outputs_per_model_ = np.prod(self.output_shape_).astype(int)\n        return self\n\n    def transform(self, X, model_idxs: List[int] = None, **kwargs) -&gt; np.array:\n        \"\"\" \n        Use cross validation object to transform estimators. \n        :param X: Input data for inference.\n        :param y: Target data for inference.\n        :param model_idxs: List of indices of models to use for inference. \n        By default, all fitted models are used.\n        :param kwargs: Additional arguments to pass to the estimator's predict function.\n        \"\"\"\n        check_is_fitted(self)        \n        inference_estimators = [self.estimators_[i] for i in model_idxs] if model_idxs else self.estimators_\n\n        # Create an empty array to store predictions\n        final_predictions = np.zeros((X.shape[0], len(inference_estimators) * self.n_outputs_per_model_))\n        # Iterate through models to get predictions\n        for idx, estimator in enumerate(inference_estimators):\n            pred = getattr(estimator, self.predict_func)(X, **kwargs)\n            pred = self._postprocess_pred(pred)\n\n            # Calculate where to place these predictions in the final array\n            start_idx = idx * self.n_outputs_per_model_\n            end_idx = (idx + 1) * self.n_outputs_per_model_\n\n            final_predictions[:, start_idx:end_idx] = pred\n\n        return final_predictions\n\n    def predict(self, X, model_idxs: List[int] = None, **kwargs) -&gt; np.array:\n        return self.transform(X, model_idxs, **kwargs)\n\n    def get_feature_names_out(self, input_features=None) -&gt; List[str]:\n        check_is_fitted(self)\n        base_str = f\"CrossValEstimator_{self.estimator_name}_{self.predict_func}\"\n        # Single-output case\n        if self.n_outputs_per_model_ == 1:\n            feature_names = [f\"{base_str}_{i}\" for i in range(len(self.estimators_))]\n        # Multi-output case\n        else:\n            feature_names = []\n            for i in range(len(self.estimators_)):\n                for j in range(self.n_outputs_per_model_):\n                    feature_names.append(f\"{base_str}_{i}_output_{j}\")\n        return feature_names\n\n    def _postprocess_pred(self, pred):\n        # Make sure predictions are 2D\n        if len(pred.shape) == 1:\n            pred = pred.reshape(-1, 1)\n        return pred\n\n    def __sklearn_is_fitted__(self) -&gt; bool:\n        \"\"\" Check fitted status. \"\"\"\n        # Must have a fitted estimator for each split.\n        return len(self.estimators_) == self.cv.get_n_splits()\n</code></pre>"},{"location":"api/#numerblox.meta.CrossValEstimator.__sklearn_is_fitted__","title":"<code>__sklearn_is_fitted__()</code>","text":"<p>Check fitted status.</p> Source code in <code>numerblox/meta.py</code> <pre><code>def __sklearn_is_fitted__(self) -&gt; bool:\n    \"\"\" Check fitted status. \"\"\"\n    # Must have a fitted estimator for each split.\n    return len(self.estimators_) == self.cv.get_n_splits()\n</code></pre>"},{"location":"api/#numerblox.meta.CrossValEstimator.fit","title":"<code>fit(X, y, **kwargs)</code>","text":"<p>Use cross validation object to fit estimators.</p> Source code in <code>numerblox/meta.py</code> <pre><code>def fit(self, X: Union[np.array, pd.DataFrame], y: Union[np.array, pd.Series], **kwargs):\n    \"\"\" Use cross validation object to fit estimators. \"\"\"\n    self.estimators_ = []\n    self.eval_results_ = []\n    if isinstance(X, (pd.Series, pd.DataFrame)):\n        X = X.reset_index(drop=True).values\n    if isinstance(y, (pd.Series, pd.DataFrame)):\n        y = y.reset_index(drop=True).values\n    for i, (train_idx, val_idx) in tqdm(enumerate(self.cv.split(X, y)), \n                                        desc=f\"CrossValEstimator Fitting. Estimator='{self.estimator_name}'\", \n                                        total=self.cv.get_n_splits(), \n                                        disable=not self.verbose):\n        estimator = clone(self.estimator)\n        if self.verbose:\n            print(f\"Fitting {self.estimator_name} on fold {len(self.estimators_)}\")\n\n\n        estimator.fit(X[train_idx], y[train_idx], **kwargs)\n\n        # Execute custom evaluation logic\n        if self.evaluation_func:\n            if self.verbose:\n                print(f\"Running evaluation on fold {len(self.estimators_)}\")\n\n            y_pred = getattr(estimator, self.predict_func)(X[val_idx])\n            y_pred = self._postprocess_pred(y_pred)\n            eval_fold = self.evaluation_func(y[val_idx], y_pred)\n            if self.verbose:\n                print(f\"CrossValEstimator (estimator='{self.estimator_name}'): Fold '{i}' evaluation results: '{eval_fold}'\")\n            self.eval_results_.append(eval_fold)\n\n        self.estimators_.append(estimator)\n\n        # Store output shape by doing inference on 1st sample of training set\n        if i == 0:\n            sample_prediction = getattr(estimator, self.predict_func)(X[train_idx][:1])\n            sample_prediction = self._postprocess_pred(sample_prediction)\n            self.output_shape_ = sample_prediction.shape[1:]\n            self.multi_output_ = len(y.shape) &gt; 1\n            self.n_outputs_per_model_ = np.prod(self.output_shape_).astype(int)\n    return self\n</code></pre>"},{"location":"api/#numerblox.meta.CrossValEstimator.transform","title":"<code>transform(X, model_idxs=None, **kwargs)</code>","text":"<p>Use cross validation object to transform estimators.  :param X: Input data for inference. :param y: Target data for inference. :param model_idxs: List of indices of models to use for inference.  By default, all fitted models are used. :param kwargs: Additional arguments to pass to the estimator's predict function.</p> Source code in <code>numerblox/meta.py</code> <pre><code>def transform(self, X, model_idxs: List[int] = None, **kwargs) -&gt; np.array:\n    \"\"\" \n    Use cross validation object to transform estimators. \n    :param X: Input data for inference.\n    :param y: Target data for inference.\n    :param model_idxs: List of indices of models to use for inference. \n    By default, all fitted models are used.\n    :param kwargs: Additional arguments to pass to the estimator's predict function.\n    \"\"\"\n    check_is_fitted(self)        \n    inference_estimators = [self.estimators_[i] for i in model_idxs] if model_idxs else self.estimators_\n\n    # Create an empty array to store predictions\n    final_predictions = np.zeros((X.shape[0], len(inference_estimators) * self.n_outputs_per_model_))\n    # Iterate through models to get predictions\n    for idx, estimator in enumerate(inference_estimators):\n        pred = getattr(estimator, self.predict_func)(X, **kwargs)\n        pred = self._postprocess_pred(pred)\n\n        # Calculate where to place these predictions in the final array\n        start_idx = idx * self.n_outputs_per_model_\n        end_idx = (idx + 1) * self.n_outputs_per_model_\n\n        final_predictions[:, start_idx:end_idx] = pred\n\n    return final_predictions\n</code></pre>"},{"location":"api/#numerblox.meta.MetaEstimator","title":"<code>MetaEstimator</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code>, <code>MetaEstimatorMixin</code></p> <p>Helper for NumeraiPipeline and NumeraiFeatureUnion to use a model as a transformer.</p> <p>:param estimator: Underlying estimator like XGBoost, Catboost, scikit-learn, etc. :param predict_func: Name of the function that will be used for prediction. Must be one of 'predict', 'predict_proba', 'predict_log_proba'. For example, XGBRegressor has 'predict' and 'predict_proba' functions. :param model_type: \"regressor\" or \"classifier\". Used to determine if the estimator is multi output.</p> Source code in <code>numerblox/meta.py</code> <pre><code>class MetaEstimator(BaseEstimator, TransformerMixin, MetaEstimatorMixin):\n    \"\"\"\n    Helper for NumeraiPipeline and NumeraiFeatureUnion to use a model as a transformer.\n\n    :param estimator: Underlying estimator like XGBoost, Catboost, scikit-learn, etc.\n    :param predict_func: Name of the function that will be used for prediction.\n    Must be one of 'predict', 'predict_proba', 'predict_log_proba'.\n    For example, XGBRegressor has 'predict' and 'predict_proba' functions.\n    :param model_type: \"regressor\" or \"classifier\". Used to determine if the estimator is multi output.\n    \"\"\"\n\n    def __init__(self, estimator, predict_func=\"predict\", model_type=\"regressor\"):\n        self.estimator = estimator\n        if predict_func not in [\"predict\", \"predict_proba\", \"predict_log_proba\", \"transform\"]:\n            raise ValueError(\"predict_func must be 'predict', 'predict_proba', 'predict_log_proba' or 'transform'.\")\n        self.predict_func = predict_func\n        assert model_type in [\"regressor\", \"classifier\"], \"model_type must be 'regressor' or 'classifier'.\"\n        assert hasattr(self.estimator, self.predict_func), f\"Estimator {self.estimator.__class__.__name__} does not have {self.predict_func} function.\"\n        self.model_type = model_type\n        # predict_proba for classifiers -&gt; multi output\n        self.proba_class_ = predict_func == \"predict_proba\" and model_type == \"classifier\"\n\n    def fit(self, X: Union[np.array, pd.DataFrame], y, **kwargs):\n        \"\"\"\n        Fit underlying estimator and set attributes.\n        \"\"\"\n        X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES, multi_output=True)\n        # Either multi target or outputs are probabilities\n        self.multi_output_ = len(y.shape) &gt; 1 or self.proba_class_\n        self.estimator_ = clone(self.estimator)\n        self.estimator_.fit(X, y, **kwargs)\n        return self\n\n    def transform(self, X: Union[np.array, pd.DataFrame], **kwargs) -&gt; np.array:\n        \"\"\"\n        Apply the `predict_func` on the fitted estimator.\n\n        Shape `(X.shape[0], )` if estimator is not multi output and else `(X.shape[0], y.shape[1])`.\n        All additional kwargs are passed to the underlying estimator's predict function.\n        \"\"\"\n        check_is_fitted(self, \"estimator_\")\n        output = getattr(self.estimator_, self.predict_func)(X, **kwargs)\n        return output if self.multi_output_ else output.reshape(-1, 1)\n\n    def predict(self, X: Union[np.array, pd.DataFrame], **kwargs) -&gt; np.array:\n        \"\"\" \n        For if a MetaEstimator happens to be the last step in the pipeline. Has same behavior as transform.\n        \"\"\"\n        return self.transform(X, **kwargs)\n\n    def get_feature_names_out(self, input_features = None) -&gt; List[str]:\n        check_is_fitted(self)\n        feature_names = [f\"{self.estimator.__class__.__name__}_{self.predict_func}_output\"]\n        return feature_names if not input_features else input_features\n</code></pre>"},{"location":"api/#numerblox.meta.MetaEstimator.fit","title":"<code>fit(X, y, **kwargs)</code>","text":"<p>Fit underlying estimator and set attributes.</p> Source code in <code>numerblox/meta.py</code> <pre><code>def fit(self, X: Union[np.array, pd.DataFrame], y, **kwargs):\n    \"\"\"\n    Fit underlying estimator and set attributes.\n    \"\"\"\n    X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES, multi_output=True)\n    # Either multi target or outputs are probabilities\n    self.multi_output_ = len(y.shape) &gt; 1 or self.proba_class_\n    self.estimator_ = clone(self.estimator)\n    self.estimator_.fit(X, y, **kwargs)\n    return self\n</code></pre>"},{"location":"api/#numerblox.meta.MetaEstimator.predict","title":"<code>predict(X, **kwargs)</code>","text":"<p>For if a MetaEstimator happens to be the last step in the pipeline. Has same behavior as transform.</p> Source code in <code>numerblox/meta.py</code> <pre><code>def predict(self, X: Union[np.array, pd.DataFrame], **kwargs) -&gt; np.array:\n    \"\"\" \n    For if a MetaEstimator happens to be the last step in the pipeline. Has same behavior as transform.\n    \"\"\"\n    return self.transform(X, **kwargs)\n</code></pre>"},{"location":"api/#numerblox.meta.MetaEstimator.transform","title":"<code>transform(X, **kwargs)</code>","text":"<p>Apply the <code>predict_func</code> on the fitted estimator.</p> <p>Shape <code>(X.shape[0], )</code> if estimator is not multi output and else <code>(X.shape[0], y.shape[1])</code>. All additional kwargs are passed to the underlying estimator's predict function.</p> Source code in <code>numerblox/meta.py</code> <pre><code>def transform(self, X: Union[np.array, pd.DataFrame], **kwargs) -&gt; np.array:\n    \"\"\"\n    Apply the `predict_func` on the fitted estimator.\n\n    Shape `(X.shape[0], )` if estimator is not multi output and else `(X.shape[0], y.shape[1])`.\n    All additional kwargs are passed to the underlying estimator's predict function.\n    \"\"\"\n    check_is_fitted(self, \"estimator_\")\n    output = getattr(self.estimator_, self.predict_func)(X, **kwargs)\n    return output if self.multi_output_ else output.reshape(-1, 1)\n</code></pre>"},{"location":"api/#numerblox.meta.MetaPipeline","title":"<code>MetaPipeline</code>","text":"<p>             Bases: <code>Pipeline</code></p> <p>Pipeline which turns all estimators into transformers by wrapping them in MetaEstimator. This allows to have pipeline steps after models. For example, a FeatureNeutralizer after an XGBRegressor.</p> <p>:param steps: List of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an instance of BaseNeutralizer. :param memory: Used to cache the fitted transformers of the pipeline. :param verbose: If True, the time elapsed while fitting each step will be printed as it is completed. :param predict_func: Name of the function that will be used for prediction.</p> Source code in <code>numerblox/meta.py</code> <pre><code>class MetaPipeline(Pipeline):\n    \"\"\"\n    Pipeline which turns all estimators into transformers by wrapping them in MetaEstimator.\n    This allows to have pipeline steps after models.\n    For example, a FeatureNeutralizer after an XGBRegressor.\n\n    :param steps: List of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an instance of BaseNeutralizer.\n    :param memory: Used to cache the fitted transformers of the pipeline.\n    :param verbose: If True, the time elapsed while fitting each step will be printed as it is completed.\n    :param predict_func: Name of the function that will be used for prediction.\n    \"\"\"\n    def __init__(self, steps, memory=None, verbose=False, predict_func=\"predict\"):\n        self.predict_func = predict_func\n        self.modified_steps = self.wrap_estimators_as_transformers(steps)\n        self.steps = self.modified_steps\n        self.memory = memory\n        self.verbose = verbose\n\n    def wrap_estimators_as_transformers(self, steps):\n        \"\"\"\n        Converts all estimator steps (except the last step) into transformers by wrapping them in MetaEstimator.\n        :param steps: List of (name, transform) tuples specifying the pipeline steps.\n        :return: Modified steps with all estimators wrapped as transformers.\n        \"\"\"\n        transformed_steps = []\n        for i, step_tuple in enumerate(steps):\n            is_last_step = i == len(steps) - 1\n\n            if len(step_tuple) == 3:\n                name, step, columns = step_tuple\n                transformed_steps.append(self._wrap_step(name, step, columns, is_last_step))\n            else:\n                name, step = step_tuple\n                transformed_steps.append(self._wrap_step(name, step, is_last_step=is_last_step))\n\n        return transformed_steps\n\n    def _wrap_step(self, name, step, columns=None, is_last_step=False):\n            \"\"\" Recursive function to wrap steps \"\"\"\n            # Recursive call\n            if isinstance(step, (Pipeline, FeatureUnion, ColumnTransformer)):\n                if isinstance(step, Pipeline):\n                    transformed = step.__class__(self.wrap_estimators_as_transformers(step.steps))\n                elif isinstance(step, FeatureUnion):\n                    transformed = FeatureUnion(self.wrap_estimators_as_transformers(step.transformer_list))\n                elif isinstance(step, ColumnTransformer):\n                    transformed_transformers = self.wrap_estimators_as_transformers(step.transformers)\n                    transformed = ColumnTransformer(transformed_transformers)\n                return (name, transformed, columns) if columns else (name, transformed)\n\n            # If it's the last step and it doesn't have a transform method, don't wrap it\n            if is_last_step and not hasattr(step, 'transform'):\n                return (name, step, columns) if columns else (name, step)\n\n            # Wrap estimator that has the predict function but not the transform function\n            elif hasattr(step, self.predict_func) and not hasattr(step, 'transform'):\n                return (name, MetaEstimator(step, predict_func=self.predict_func))\n\n            return (name, step, columns) if columns else (name, step)\n\n    def _fit(self, X, y=None, **fit_params_steps):\n        # shallow copy of steps - this should really be steps_\n        self.steps = list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory = check_memory(self.memory)\n\n        for step_idx, name, transformer in self._iter(\n            with_final=False, filter_passthrough=False\n        ):\n            if transformer is None or transformer == \"passthrough\":\n                with _print_elapsed_time(\"Pipeline\", self._log_message(step_idx)):\n                    continue\n\n            if hasattr(memory, \"location\") and memory.location is None:\n                # we do not clone when caching is disabled to\n                # preserve backward compatibility\n                cloned_transformer = transformer\n            else:\n                cloned_transformer = clone(transformer)\n            # Fit or load from cache the current transformer\n            X, fitted_transformer = _fit_transform_one(\n                cloned_transformer,\n                X,\n                y,\n                None,\n                message_clsname=\"Pipeline\",\n                message=self._log_message(step_idx),\n                **fit_params_steps[name],\n            )\n            # Replace the transformer of the step with the fitted\n            # transformer. This is necessary when loading the transformer\n            # from the cache.\n            self.steps[step_idx] = (name, fitted_transformer)\n        return X\n\n    @available_if(_final_estimator_has(\"predict\"))\n    def predict(self, X, **predict_params):\n        \"\"\" Overrides predict method in Pipeline so it also parses arguments to transforms.\"\"\"\n        Xt = X\n        for _, name, transform in self._iter(with_final=False):\n            # Check if predict params is needed in transform. If so parse along\n            sig = inspect.signature(transform.transform)\n            params_needed = {k: v for k, v in predict_params.items() if k in sig.parameters}\n            # Use the needed parameters when calling transform\n            Xt = transform.transform(Xt, **params_needed)\n        return self.steps[-1][1].predict(Xt, **predict_params)\n</code></pre>"},{"location":"api/#numerblox.meta.MetaPipeline.predict","title":"<code>predict(X, **predict_params)</code>","text":"<p>Overrides predict method in Pipeline so it also parses arguments to transforms.</p> Source code in <code>numerblox/meta.py</code> <pre><code>@available_if(_final_estimator_has(\"predict\"))\ndef predict(self, X, **predict_params):\n    \"\"\" Overrides predict method in Pipeline so it also parses arguments to transforms.\"\"\"\n    Xt = X\n    for _, name, transform in self._iter(with_final=False):\n        # Check if predict params is needed in transform. If so parse along\n        sig = inspect.signature(transform.transform)\n        params_needed = {k: v for k, v in predict_params.items() if k in sig.parameters}\n        # Use the needed parameters when calling transform\n        Xt = transform.transform(Xt, **params_needed)\n    return self.steps[-1][1].predict(Xt, **predict_params)\n</code></pre>"},{"location":"api/#numerblox.meta.MetaPipeline.wrap_estimators_as_transformers","title":"<code>wrap_estimators_as_transformers(steps)</code>","text":"<p>Converts all estimator steps (except the last step) into transformers by wrapping them in MetaEstimator. :param steps: List of (name, transform) tuples specifying the pipeline steps. :return: Modified steps with all estimators wrapped as transformers.</p> Source code in <code>numerblox/meta.py</code> <pre><code>def wrap_estimators_as_transformers(self, steps):\n    \"\"\"\n    Converts all estimator steps (except the last step) into transformers by wrapping them in MetaEstimator.\n    :param steps: List of (name, transform) tuples specifying the pipeline steps.\n    :return: Modified steps with all estimators wrapped as transformers.\n    \"\"\"\n    transformed_steps = []\n    for i, step_tuple in enumerate(steps):\n        is_last_step = i == len(steps) - 1\n\n        if len(step_tuple) == 3:\n            name, step, columns = step_tuple\n            transformed_steps.append(self._wrap_step(name, step, columns, is_last_step))\n        else:\n            name, step = step_tuple\n            transformed_steps.append(self._wrap_step(name, step, is_last_step=is_last_step))\n\n    return transformed_steps\n</code></pre>"},{"location":"api/#numerblox.meta.make_meta_pipeline","title":"<code>make_meta_pipeline(*steps, memory=None, verbose=False)</code>","text":"<p>Convenience function for creating a MetaPipeline.  :param steps: List of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an instance of BaseNeutralizer. :param memory: Used to cache the fitted transformers of the pipeline. :param verbose: If True, the time elapsed while fitting each step will be printed as it is completed.</p> Source code in <code>numerblox/meta.py</code> <pre><code>def make_meta_pipeline(*steps, memory=None, verbose=False) -&gt; MetaPipeline:\n    \"\"\" \n    Convenience function for creating a MetaPipeline. \n    :param steps: List of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an instance of BaseNeutralizer.\n    :param memory: Used to cache the fitted transformers of the pipeline.\n    :param verbose: If True, the time elapsed while fitting each step will be printed as it is completed.\n    \"\"\"\n    return MetaPipeline(_name_estimators(steps), memory=memory, verbose=verbose)\n</code></pre>"},{"location":"api/#numerblox.neutralizers.BaseNeutralizer","title":"<code>BaseNeutralizer</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Base class for neutralization so it is compatible with scikit-learn. :param new_col_name: Name of new neutralized column.</p> Source code in <code>numerblox/neutralizers.py</code> <pre><code>class BaseNeutralizer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Base class for neutralization so it is compatible with scikit-learn.\n    :param new_col_name: Name of new neutralized column.\n    \"\"\"\n    def __init__(self, new_col_names: list):\n        self.new_col_names = new_col_names\n        super().__init__()\n\n    def fit(self, X=None, y=None, **kwargs):\n        return self\n\n    @abstractmethod\n    def transform(\n        self, X: Union[np.array, pd.DataFrame], \n        features: pd.DataFrame, eras: pd.Series, **kwargs\n    ) -&gt; np.array:\n        ...\n\n    def predict(self, X: np.array, features: pd.DataFrame, eras: Union[np.array, pd.Series]) -&gt; np.array:\n        \"\"\" Convenience function for scikit-learn compatibility. \"\"\"\n        return self.transform(X=X, features=features, eras=eras)\n\n    def fit_transform(self, X: np.array, features: pd.DataFrame, eras: Union[np.array, pd.Series]) -&gt; np.array:\n        \"\"\" \n        Convenience function for scikit-learn compatibility.\n        Needed because fit and transform except different arguments here.\n        \"\"\"\n        return self.fit().transform(X=X, features=features, eras=eras)\n\n    def __call__(\n        self, X: Union[np.array, pd.DataFrame],\n        features: pd.DataFrame, eras: pd.Series, **kwargs\n    ) -&gt; np.array:\n        return self.predict(X=X, features=features, eras=eras, **kwargs)\n\n    def get_feature_names_out(self, input_features: list = None) -&gt; list:\n        \"\"\" \n        Get feature names for neutralized output.\n\n        :param input_features: Optional list of input feature names.\n        :return: List of feature names for neutralized output.\n        \"\"\"\n        return input_features if input_features else self.new_col_names\n</code></pre>"},{"location":"api/#numerblox.neutralizers.BaseNeutralizer.fit_transform","title":"<code>fit_transform(X, features, eras)</code>","text":"<p>Convenience function for scikit-learn compatibility. Needed because fit and transform except different arguments here.</p> Source code in <code>numerblox/neutralizers.py</code> <pre><code>def fit_transform(self, X: np.array, features: pd.DataFrame, eras: Union[np.array, pd.Series]) -&gt; np.array:\n    \"\"\" \n    Convenience function for scikit-learn compatibility.\n    Needed because fit and transform except different arguments here.\n    \"\"\"\n    return self.fit().transform(X=X, features=features, eras=eras)\n</code></pre>"},{"location":"api/#numerblox.neutralizers.BaseNeutralizer.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Get feature names for neutralized output.</p> <p>:param input_features: Optional list of input feature names. :return: List of feature names for neutralized output.</p> Source code in <code>numerblox/neutralizers.py</code> <pre><code>def get_feature_names_out(self, input_features: list = None) -&gt; list:\n    \"\"\" \n    Get feature names for neutralized output.\n\n    :param input_features: Optional list of input feature names.\n    :return: List of feature names for neutralized output.\n    \"\"\"\n    return input_features if input_features else self.new_col_names\n</code></pre>"},{"location":"api/#numerblox.neutralizers.BaseNeutralizer.predict","title":"<code>predict(X, features, eras)</code>","text":"<p>Convenience function for scikit-learn compatibility.</p> Source code in <code>numerblox/neutralizers.py</code> <pre><code>def predict(self, X: np.array, features: pd.DataFrame, eras: Union[np.array, pd.Series]) -&gt; np.array:\n    \"\"\" Convenience function for scikit-learn compatibility. \"\"\"\n    return self.transform(X=X, features=features, eras=eras)\n</code></pre>"},{"location":"api/#numerblox.neutralizers.FeatureNeutralizer","title":"<code>FeatureNeutralizer</code>","text":"<p>             Bases: <code>BaseNeutralizer</code></p> <p>Classic feature neutralization by subtracting a linear model.</p> <p>:param pred_name: Name of prediction column. For creating the new column name.  :param proportion: Number in range [0...1] indicating how much to neutralize. :param suffix: Optional suffix that is added to new column name. :param num_cores: Number of cores to use for parallel processing. By default, all CPU cores are used.</p> Source code in <code>numerblox/neutralizers.py</code> <pre><code>class FeatureNeutralizer(BaseNeutralizer):\n    \"\"\"\n    Classic feature neutralization by subtracting a linear model.\n\n    :param pred_name: Name of prediction column. For creating the new column name. \n    :param proportion: Number in range [0...1] indicating how much to neutralize.\n    :param suffix: Optional suffix that is added to new column name.\n    :param num_cores: Number of cores to use for parallel processing.\n    By default, all CPU cores are used.\n    \"\"\"\n    def __init__(\n        self,\n        pred_name: Union[str, list] = \"prediction\",\n        proportion: Union[float, List[float]] = 0.5,\n        suffix: str = None,\n        num_cores: int = -1\n    ):\n        self.pred_name = [pred_name] if isinstance(pred_name, str) else pred_name\n        self.proportion = [proportion] if isinstance(proportion, float) else proportion\n        assert len(self.pred_name) == len(set(self.pred_name)), \"Duplicate 'pred_names' found. Make sure all names are unique.\"\n        assert len(self.proportion) == len(set(self.proportion)), \"Duplicate 'proportions' found. Make sure all proportions are unique.\"\n        for prop in self.proportion:\n            assert (\n                0.0 &lt;= prop &lt;= 1.0\n            ), f\"'proportion' should be a float in range [0...1]. Got '{prop}'.\"\n\n        new_col_names = []\n        for pred_name in self.pred_name:\n            for prop in self.proportion:\n                new_col_names.append(\n                    f\"{pred_name}_neutralized_{prop}_{suffix}\" if suffix else f\"{pred_name}_neutralized_{prop}\"\n                )\n        super().__init__(new_col_names=new_col_names)\n        self.suffix = suffix\n        self.num_cores = num_cores\n\n    def transform(self, X: Union[np.array, pd.Series, pd.DataFrame], \n                  features: pd.DataFrame, eras: Union[np.array, pd.Series]) -&gt; np.array:\n        \"\"\"\n        Main transform function.\n        :param X: Input predictions to neutralize. \\n\n        :param features: DataFrame with features for neutralization. \\n\n        :param eras: Series with era labels for each row in features. \\n\n        Features, eras and the prediction column must all have the same length.\n        :return: Neutralized predictions NumPy array.\n        \"\"\"\n        assert len(X) == len(features), \"Input predictions must have same length as features.\"\n        assert len(X) == len(eras), \"Input predictions must have same length as eras.\"\n        df = features.copy()\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n        # Ensure X is a 2D array and has the same number of columns as pred_name\n        if X.ndim == 1:\n            assert len(self.pred_name) == 1, \"Only one prediction column found. Please input a 2D array or define one column for 'pred_name'.\"\n            X = X.reshape(-1, 1)\n        else:\n            assert len(self.pred_name) == X.shape[1], \"Number of prediction columns given in X does not match 'pred_name'.\"\n        for i, pred_name in enumerate(self.pred_name):\n            df[pred_name] = X[:, i]\n        df[\"era\"] = eras\n\n        feature_cols = list(features.columns)\n        tasks = [\n            delayed(self._process_pred_name)(df, pred_name, proportion, feature_cols)\n            for pred_name in tqdm(self.pred_name, desc=\"Processing feature neutralizations\") \n            for proportion in self.proportion\n        ]\n        neutralized_results = Parallel(n_jobs=self.num_cores)(tasks)\n        neutralized_preds = pd.concat(neutralized_results, axis=1).to_numpy()\n        return neutralized_preds\n\n    def _process_pred_name(self, df: pd.DataFrame, pred_name: str, proportion: float, feature_cols: List[str]) -&gt; pd.DataFrame:\n        \"\"\" \n        Process one combination of prediction and proportion.\n        :param df: DataFrame with features and predictions.\n        :param pred_name: Name of prediction column.\n        :param proportion: Proportion to neutralize.\n        :param feature_cols: List of feature column names.\n        :return: Neutralized predictions.\n        Neutralized predictions are scaled to [0...1].\n        \"\"\"\n        neutralized_pred = df.groupby(\"era\", group_keys=False).apply(\n            lambda x: self.normalize_and_neutralize(x, [pred_name], feature_cols, proportion)\n        )\n        return pd.DataFrame(MinMaxScaler().fit_transform(neutralized_pred))\n\n    def neutralize(self, dataf: pd.DataFrame, columns: list, by: list, proportion: float) -&gt; pd.DataFrame:\n        \"\"\" \n        Neutralize on CPU. \n        :param dataf: DataFrame with features and predictions.\n        :param columns: List of prediction column names.\n        :param by: List of feature column names.\n        :param proportion: Proportion to neutralize.\n        :return: Neutralized predictions.\n        \"\"\"\n        scores = dataf[columns]\n        exposures = dataf[by].values\n        scores = scores - proportion * self._get_raw_exposures(exposures, scores)\n        return scores / scores.std()\n\n    @staticmethod\n    def normalize(dataf: pd.DataFrame) -&gt; np.ndarray:\n        \"\"\" Normalize predictions.\n        1. Rank predictions.\n        2. Normalize ranks.\n        3. Gaussianize ranks.\n        :param dataf: DataFrame with predictions.\n        :return: Gaussianized rank predictions.\n        \"\"\"\n        normalized_ranks = (dataf.rank(method=\"first\") - 0.5) / len(dataf)\n        # Gaussianized ranks\n        return sp.norm.ppf(normalized_ranks)\n\n    def normalize_and_neutralize(\n        self, dataf: pd.DataFrame, columns: list, by: list, proportion: float\n    ) -&gt; pd.DataFrame:\n        \"\"\" \n        Gaussianize predictions and neutralize with one combination of prediction and proportion. \n        :param dataf: DataFrame with features and predictions.\n        :param columns: List of prediction column names.\n        :param by: List of feature column names.\n        :param proportion: Proportion to neutralize.\n        :return: Neutralized predictions DataFrame.\n        \"\"\"\n        dataf[columns] = self.normalize(dataf[columns])\n        dataf[columns] = self.neutralize(dataf, columns, by, proportion)\n        return dataf[columns]\n\n    @staticmethod\n    def _get_raw_exposures(exposures: np.array, scores: pd.DataFrame) -&gt; np.array:\n        \"\"\" \n        Get raw feature exposures.\n        Make sure predictions are normalized!\n        :param exposures: Exposures for each era. \n        :param scores: DataFrame with predictions.\n        :return: Raw exposures for each era.\n        \"\"\"\n        return exposures.dot(np.linalg.pinv(exposures).dot(scores))\n</code></pre>"},{"location":"api/#numerblox.neutralizers.FeatureNeutralizer.neutralize","title":"<code>neutralize(dataf, columns, by, proportion)</code>","text":"<p>Neutralize on CPU.  :param dataf: DataFrame with features and predictions. :param columns: List of prediction column names. :param by: List of feature column names. :param proportion: Proportion to neutralize. :return: Neutralized predictions.</p> Source code in <code>numerblox/neutralizers.py</code> <pre><code>def neutralize(self, dataf: pd.DataFrame, columns: list, by: list, proportion: float) -&gt; pd.DataFrame:\n    \"\"\" \n    Neutralize on CPU. \n    :param dataf: DataFrame with features and predictions.\n    :param columns: List of prediction column names.\n    :param by: List of feature column names.\n    :param proportion: Proportion to neutralize.\n    :return: Neutralized predictions.\n    \"\"\"\n    scores = dataf[columns]\n    exposures = dataf[by].values\n    scores = scores - proportion * self._get_raw_exposures(exposures, scores)\n    return scores / scores.std()\n</code></pre>"},{"location":"api/#numerblox.neutralizers.FeatureNeutralizer.normalize","title":"<code>normalize(dataf)</code>  <code>staticmethod</code>","text":"<p>Normalize predictions. 1. Rank predictions. 2. Normalize ranks. 3. Gaussianize ranks. :param dataf: DataFrame with predictions. :return: Gaussianized rank predictions.</p> Source code in <code>numerblox/neutralizers.py</code> <pre><code>@staticmethod\ndef normalize(dataf: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\" Normalize predictions.\n    1. Rank predictions.\n    2. Normalize ranks.\n    3. Gaussianize ranks.\n    :param dataf: DataFrame with predictions.\n    :return: Gaussianized rank predictions.\n    \"\"\"\n    normalized_ranks = (dataf.rank(method=\"first\") - 0.5) / len(dataf)\n    # Gaussianized ranks\n    return sp.norm.ppf(normalized_ranks)\n</code></pre>"},{"location":"api/#numerblox.neutralizers.FeatureNeutralizer.normalize_and_neutralize","title":"<code>normalize_and_neutralize(dataf, columns, by, proportion)</code>","text":"<p>Gaussianize predictions and neutralize with one combination of prediction and proportion.  :param dataf: DataFrame with features and predictions. :param columns: List of prediction column names. :param by: List of feature column names. :param proportion: Proportion to neutralize. :return: Neutralized predictions DataFrame.</p> Source code in <code>numerblox/neutralizers.py</code> <pre><code>def normalize_and_neutralize(\n    self, dataf: pd.DataFrame, columns: list, by: list, proportion: float\n) -&gt; pd.DataFrame:\n    \"\"\" \n    Gaussianize predictions and neutralize with one combination of prediction and proportion. \n    :param dataf: DataFrame with features and predictions.\n    :param columns: List of prediction column names.\n    :param by: List of feature column names.\n    :param proportion: Proportion to neutralize.\n    :return: Neutralized predictions DataFrame.\n    \"\"\"\n    dataf[columns] = self.normalize(dataf[columns])\n    dataf[columns] = self.neutralize(dataf, columns, by, proportion)\n    return dataf[columns]\n</code></pre>"},{"location":"api/#numerblox.neutralizers.FeatureNeutralizer.transform","title":"<code>transform(X, features, eras)</code>","text":"<p>Main transform function. :param X: Input predictions to neutralize. </p> <p>:param features: DataFrame with features for neutralization. </p> <p>:param eras: Series with era labels for each row in features. </p> <p>Features, eras and the prediction column must all have the same length. :return: Neutralized predictions NumPy array.</p> Source code in <code>numerblox/neutralizers.py</code> <pre><code>def transform(self, X: Union[np.array, pd.Series, pd.DataFrame], \n              features: pd.DataFrame, eras: Union[np.array, pd.Series]) -&gt; np.array:\n    \"\"\"\n    Main transform function.\n    :param X: Input predictions to neutralize. \\n\n    :param features: DataFrame with features for neutralization. \\n\n    :param eras: Series with era labels for each row in features. \\n\n    Features, eras and the prediction column must all have the same length.\n    :return: Neutralized predictions NumPy array.\n    \"\"\"\n    assert len(X) == len(features), \"Input predictions must have same length as features.\"\n    assert len(X) == len(eras), \"Input predictions must have same length as eras.\"\n    df = features.copy()\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    # Ensure X is a 2D array and has the same number of columns as pred_name\n    if X.ndim == 1:\n        assert len(self.pred_name) == 1, \"Only one prediction column found. Please input a 2D array or define one column for 'pred_name'.\"\n        X = X.reshape(-1, 1)\n    else:\n        assert len(self.pred_name) == X.shape[1], \"Number of prediction columns given in X does not match 'pred_name'.\"\n    for i, pred_name in enumerate(self.pred_name):\n        df[pred_name] = X[:, i]\n    df[\"era\"] = eras\n\n    feature_cols = list(features.columns)\n    tasks = [\n        delayed(self._process_pred_name)(df, pred_name, proportion, feature_cols)\n        for pred_name in tqdm(self.pred_name, desc=\"Processing feature neutralizations\") \n        for proportion in self.proportion\n    ]\n    neutralized_results = Parallel(n_jobs=self.num_cores)(tasks)\n    neutralized_preds = pd.concat(neutralized_results, axis=1).to_numpy()\n    return neutralized_preds\n</code></pre>"},{"location":"api/#numerblox.penalizers.BasePenalizer","title":"<code>BasePenalizer</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Base class for penalization so it is compatible with scikit-learn. :param new_col_name: Name of new neutralized column.</p> Source code in <code>numerblox/penalizers.py</code> <pre><code>class BasePenalizer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Base class for penalization so it is compatible with scikit-learn.\n    :param new_col_name: Name of new neutralized column.\n    \"\"\"\n    def __init__(self, new_col_name: str):\n        self.new_col_name = new_col_name\n        super().__init__()\n\n    def fit(self, X=None, y=None, **kwargs):\n        return self\n\n    @abstractmethod\n    def transform(\n        self, X: Union[np.array, pd.DataFrame], \n        features: pd.DataFrame, eras: pd.Series, **kwargs\n    ) -&gt; np.array:\n        ...\n\n    def predict(self, X: np.array, features: pd.DataFrame, eras: Union[np.array, pd.Series]) -&gt; np.array:\n        \"\"\" Convenience function for scikit-learn compatibility. \"\"\"\n        return self.transform(X=X, features=features, eras=eras)\n\n    def fit_transform(self, X: np.array, features: pd.DataFrame, eras: Union[np.array, pd.Series]) -&gt; np.array:\n        \"\"\" \n        Convenience function for scikit-learn compatibility.\n        Needed because fit and transform except different arguments here.\n        \"\"\"\n        return self.fit().transform(X=X, features=features, eras=eras)\n\n    def __call__(\n        self, X: Union[np.array, pd.DataFrame],\n        features: pd.DataFrame, eras: pd.Series, **kwargs\n    ) -&gt; np.array:\n        return self.predict(X=X, features=features, eras=eras, **kwargs)\n\n    def get_feature_names_out(self, input_features: list = None) -&gt; list:\n        \"\"\" \n        Get feature names for neutralized output.\n\n        :param input_features: Optional list of input feature names.\n        :return: List of feature names for neutralized output.\n        \"\"\"\n        return input_features if input_features else [self.new_col_name]\n</code></pre>"},{"location":"api/#numerblox.penalizers.BasePenalizer.fit_transform","title":"<code>fit_transform(X, features, eras)</code>","text":"<p>Convenience function for scikit-learn compatibility. Needed because fit and transform except different arguments here.</p> Source code in <code>numerblox/penalizers.py</code> <pre><code>def fit_transform(self, X: np.array, features: pd.DataFrame, eras: Union[np.array, pd.Series]) -&gt; np.array:\n    \"\"\" \n    Convenience function for scikit-learn compatibility.\n    Needed because fit and transform except different arguments here.\n    \"\"\"\n    return self.fit().transform(X=X, features=features, eras=eras)\n</code></pre>"},{"location":"api/#numerblox.penalizers.BasePenalizer.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Get feature names for neutralized output.</p> <p>:param input_features: Optional list of input feature names. :return: List of feature names for neutralized output.</p> Source code in <code>numerblox/penalizers.py</code> <pre><code>def get_feature_names_out(self, input_features: list = None) -&gt; list:\n    \"\"\" \n    Get feature names for neutralized output.\n\n    :param input_features: Optional list of input feature names.\n    :return: List of feature names for neutralized output.\n    \"\"\"\n    return input_features if input_features else [self.new_col_name]\n</code></pre>"},{"location":"api/#numerblox.penalizers.BasePenalizer.predict","title":"<code>predict(X, features, eras)</code>","text":"<p>Convenience function for scikit-learn compatibility.</p> Source code in <code>numerblox/penalizers.py</code> <pre><code>def predict(self, X: np.array, features: pd.DataFrame, eras: Union[np.array, pd.Series]) -&gt; np.array:\n    \"\"\" Convenience function for scikit-learn compatibility. \"\"\"\n    return self.transform(X=X, features=features, eras=eras)\n</code></pre>"},{"location":"api/#numerblox.penalizers.FeaturePenalizer","title":"<code>FeaturePenalizer</code>","text":"<p>             Bases: <code>BasePenalizer</code></p> <p>Feature penalization with TensorFlow.</p> <p>Source (by jrb): https://github.com/jonrtaylor/twitch/blob/master/FE_Clipping_Script.ipynb</p> <p>Source of first PyTorch implementation (by Michael Oliver / mdo): https://forum.numer.ai/t/model-diagnostics-feature-exposure/899/12</p> <p>:param max_exposure: Number in range [0...1] indicating how much to reduce max feature exposure to. :param pred_name: Prediction column name. Used for new column name. </p> <p>:param suffix: Optional suffix that is added to new column name.</p> Source code in <code>numerblox/penalizers.py</code> <pre><code>class FeaturePenalizer(BasePenalizer):\n    \"\"\"\n    Feature penalization with TensorFlow.\n\n    Source (by jrb): https://github.com/jonrtaylor/twitch/blob/master/FE_Clipping_Script.ipynb\n\n    Source of first PyTorch implementation (by Michael Oliver / mdo): https://forum.numer.ai/t/model-diagnostics-feature-exposure/899/12\n\n    :param max_exposure: Number in range [0...1] indicating how much to reduce max feature exposure to.\n    :param pred_name: Prediction column name. Used for new column name. \\n\n    :param suffix: Optional suffix that is added to new column name.\n    \"\"\"\n    def __init__(\n        self,\n        max_exposure: float,\n        pred_name: str = \"prediction\",\n        suffix: str = None,\n    ):\n        self.max_exposure = max_exposure\n        self.pred_name = pred_name\n        assert (\n            0.0 &lt;= max_exposure &lt;= 1.0\n        ), f\"'max_exposure' should be a float in range [0...1]. Got '{self.max_exposure}'.\"\n        new_col_name = (\n            f\"{self.pred_name}_penalized_{self.max_exposure}_{suffix}\"\n            if suffix\n            else f\"{self.pred_name}_penalized_{self.max_exposure}\"\n        )\n        super().__init__(new_col_name=new_col_name)\n        self.suffix = suffix\n\n    def transform(self, X: pd.DataFrame, features: pd.DataFrame, eras: pd.Series) -&gt; np.array:\n        \"\"\"\n        Main transform method.\n        :param X: Input predictions to neutralize. \n        :param features: DataFrame with features for neutralization. \n        :param eras: Series with era labels for each row in features. \n        Features, eras and the prediction column must all have the same length.\n        :return: Penalized predictions.\n        \"\"\"\n        assert len(X) == len(features), \"Input predictions must have same length as features.\"\n        assert len(X) == len(eras), \"Input predictions must have same length as eras.\"\n        df = features.copy()\n        df[\"prediction\"] = X\n        df[\"era\"] = eras\n        penalized_data = self._reduce_all_exposures(\n            dataf=df, column=self.pred_name, neutralizers=list(features.columns)\n        )\n        return penalized_data\n\n    def _reduce_all_exposures(\n        self,\n        dataf: pd.DataFrame,\n        column: str = \"prediction\",\n        neutralizers: list = None,\n        normalize=True,\n        gaussianize=True,\n    ) -&gt; pd.DataFrame:\n        neutralized = []\n\n        for era in tqdm(dataf[\"era\"].unique()):\n            dataf_era = dataf[dataf[\"era\"] == era]\n            scores = dataf_era[[column]].values\n            exposure_values = dataf_era[neutralizers].values\n\n            if normalize:\n                scores2 = []\n                for x in scores.T:\n                    x = (scipy.stats.rankdata(x, method=\"ordinal\") - 0.5) / len(x)\n                    if gaussianize:\n                        x = scipy.stats.norm.ppf(x)\n                    scores2.append(x)\n                scores = np.array(scores2)[0]\n\n            scores, _ = self._reduce_exposure(\n                scores, exposure_values, len(neutralizers), None\n            )\n\n            scores /= tf.math.reduce_std(scores)\n            scores -= tf.reduce_min(scores)\n            scores /= tf.reduce_max(scores)\n            neutralized.append(scores.numpy())\n\n        predictions = pd.DataFrame(\n            np.concatenate(neutralized), columns=[column], index=dataf.index\n        )\n        return predictions\n\n    def _reduce_exposure(self, prediction, features, input_size=50, weights=None):\n        model = tf.keras.models.Sequential(\n            [\n                tf.keras.layers.Input(input_size),\n                tf.keras.experimental.LinearModel(use_bias=False),\n            ]\n        )\n        feats = tf.convert_to_tensor(features - 0.5, dtype=tf.float32)\n        pred = tf.convert_to_tensor(prediction, dtype=tf.float32)\n        if weights is None:\n            optimizer = tf.keras.optimizers.Adamax()\n            start_exp = self.__exposures(feats, pred[:, None])\n            target_exps = tf.clip_by_value(\n                start_exp, -self.max_exposure, self.max_exposure\n            )\n            self._train_loop(model, optimizer, feats, pred, target_exps)\n        else:\n            model.set_weights(weights)\n        return pred[:, None] - model(feats), model.get_weights()\n\n    def _train_loop(self, model, optimizer, feats, pred, target_exps):\n        for _ in range(1000000):\n            loss, grads = self.__train_loop_body(model, feats, pred, target_exps)\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n            if loss &lt; 1e-7:\n                break\n\n    @tf.function(experimental_relax_shapes=True)\n    def __train_loop_body(self, model, feats, pred, target_exps):\n        with tf.GradientTape() as tape:\n            exps = self.__exposures(feats, pred[:, None] - model(feats, training=True))\n            loss = tf.reduce_sum(\n                tf.nn.relu(tf.nn.relu(exps) - tf.nn.relu(target_exps))\n                + tf.nn.relu(tf.nn.relu(-exps) - tf.nn.relu(-target_exps))\n            )\n        return loss, tape.gradient(loss, model.trainable_variables)\n\n    @staticmethod\n    @tf.function(experimental_relax_shapes=True, experimental_compile=True)\n    def __exposures(x, y):\n        x = x - tf.math.reduce_mean(x, axis=0)\n        x = x / tf.norm(x, axis=0)\n        y = y - tf.math.reduce_mean(y, axis=0)\n        y = y / tf.norm(y, axis=0)\n        return tf.matmul(x, y, transpose_a=True)\n</code></pre>"},{"location":"api/#numerblox.penalizers.FeaturePenalizer.transform","title":"<code>transform(X, features, eras)</code>","text":"<p>Main transform method. :param X: Input predictions to neutralize.  :param features: DataFrame with features for neutralization.  :param eras: Series with era labels for each row in features.  Features, eras and the prediction column must all have the same length. :return: Penalized predictions.</p> Source code in <code>numerblox/penalizers.py</code> <pre><code>def transform(self, X: pd.DataFrame, features: pd.DataFrame, eras: pd.Series) -&gt; np.array:\n    \"\"\"\n    Main transform method.\n    :param X: Input predictions to neutralize. \n    :param features: DataFrame with features for neutralization. \n    :param eras: Series with era labels for each row in features. \n    Features, eras and the prediction column must all have the same length.\n    :return: Penalized predictions.\n    \"\"\"\n    assert len(X) == len(features), \"Input predictions must have same length as features.\"\n    assert len(X) == len(eras), \"Input predictions must have same length as eras.\"\n    df = features.copy()\n    df[\"prediction\"] = X\n    df[\"era\"] = eras\n    penalized_data = self._reduce_all_exposures(\n        dataf=df, column=self.pred_name, neutralizers=list(features.columns)\n    )\n    return penalized_data\n</code></pre>"},{"location":"api/#numerblox.prediction_loaders.BasePredictionLoader","title":"<code>BasePredictionLoader</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Shared functionality for all Prediction Loaders.</p> Source code in <code>numerblox/prediction_loaders.py</code> <pre><code>class BasePredictionLoader(BaseEstimator, TransformerMixin):\n    \"\"\" Shared functionality for all Prediction Loaders. \"\"\"\n    def __init__(self):\n        ...\n\n    def fit(self, X=None, y=None):\n        return self\n\n    @abstractmethod\n    def transform(self, X=None, y=None) -&gt; pd.DataFrame:\n        \"\"\" Return Predictions generated by model. \"\"\"\n        ...\n\n    @abstractmethod\n    def get_feature_names_out(self, input_features=None):\n        \"\"\" Return feature names. \"\"\"\n        ...\n</code></pre>"},{"location":"api/#numerblox.prediction_loaders.BasePredictionLoader.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>  <code>abstractmethod</code>","text":"<p>Return feature names.</p> Source code in <code>numerblox/prediction_loaders.py</code> <pre><code>@abstractmethod\ndef get_feature_names_out(self, input_features=None):\n    \"\"\" Return feature names. \"\"\"\n    ...\n</code></pre>"},{"location":"api/#numerblox.prediction_loaders.BasePredictionLoader.transform","title":"<code>transform(X=None, y=None)</code>  <code>abstractmethod</code>","text":"<p>Return Predictions generated by model.</p> Source code in <code>numerblox/prediction_loaders.py</code> <pre><code>@abstractmethod\ndef transform(self, X=None, y=None) -&gt; pd.DataFrame:\n    \"\"\" Return Predictions generated by model. \"\"\"\n    ...\n</code></pre>"},{"location":"api/#numerblox.prediction_loaders.ExamplePredictions","title":"<code>ExamplePredictions</code>","text":"<p>             Bases: <code>BasePredictionLoader</code></p> <p>Load example predictions. :param file_name: File to download from NumerAPI. By default this is example predictions for v4.2 data. 'v4.2/live_example_preds.parquet' by default.  Example predictions in previous versions: - v4.2. validation examples -&gt; \"v4.2/validation_example_preds.parquet\" - v4.2. live benchmark models -&gt; \"v4.2/live_benchmark_models.parquet\" - v4.2. validation benchmark models -&gt; \"v4.2/validation_benchmark_models.parquet\" :param round_num: Optional round number. Downloads most recent round by default. :param keep_files: Whether to keep downloaded files. By default, files are deleted after the predictions are loaded.</p> Source code in <code>numerblox/prediction_loaders.py</code> <pre><code>class ExamplePredictions(BasePredictionLoader):\n    \"\"\"\n    Load example predictions.\n    :param file_name: File to download from NumerAPI.\n    By default this is example predictions for v4.2 data.\n    'v4.2/live_example_preds.parquet' by default. \n    Example predictions in previous versions:\n    - v4.2. validation examples -&gt; \"v4.2/validation_example_preds.parquet\"\n    - v4.2. live benchmark models -&gt; \"v4.2/live_benchmark_models.parquet\"\n    - v4.2. validation benchmark models -&gt; \"v4.2/validation_benchmark_models.parquet\"\n    :param round_num: Optional round number. Downloads most recent round by default.\n    :param keep_files: Whether to keep downloaded files.\n    By default, files are deleted after the predictions are loaded.\n    \"\"\"\n    def __init__(self, file_name: str = \"v4.2/live_example_preds.parquet\",\n                 round_num: int = None, keep_files: bool = False):\n        super().__init__()\n        self.file_name = file_name\n        self.round_num = round_num\n        self.keep_files = keep_files\n\n    def transform(self, X=None, y=None) -&gt; pd.DataFrame:\n        \"\"\" Return example predictions. \"\"\"\n        self._download_example_preds()\n        example_preds = self._load_example_preds()\n        if not self.keep_files:\n            self.downloader.remove_base_directory()\n        return example_preds\n\n    def _download_example_preds(self):\n        data_directory = f\"example_predictions_loader_{uuid4()}\"\n        self.downloader = NumeraiClassicDownloader(directory_path=data_directory)\n        self.dest_path = f\"{str(self.downloader.dir)}/{self.file_name}\"\n        self.downloader.download_single_dataset(filename=self.file_name,\n                                                dest_path=self.dest_path,\n                                                round_num=self.round_num)\n\n    def _load_example_preds(self, *args, **kwargs):\n        return pd.read_parquet(self.dest_path, *args, **kwargs)\n\n    def get_feature_names_out(self, input_features=None):\n        return [Path(self.file_name).with_suffix('').as_posix()] if not input_features else input_features\n</code></pre>"},{"location":"api/#numerblox.prediction_loaders.ExamplePredictions.transform","title":"<code>transform(X=None, y=None)</code>","text":"<p>Return example predictions.</p> Source code in <code>numerblox/prediction_loaders.py</code> <pre><code>def transform(self, X=None, y=None) -&gt; pd.DataFrame:\n    \"\"\" Return example predictions. \"\"\"\n    self._download_example_preds()\n    example_preds = self._load_example_preds()\n    if not self.keep_files:\n        self.downloader.remove_base_directory()\n    return example_preds\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator","title":"<code>BaseEvaluator</code>","text":"<p>Evaluation functionality that is relevant for both Numerai Classic and Numerai Signals.</p> <p>Metrics include: - Mean, Standard Deviation and Sharpe (Corrv2) for era returns. - Max drawdown. - Annual Percentage Yield (APY). - Correlation with benchmark predictions. - Max feature exposure: https://forum.numer.ai/t/model-diagnostics-feature-exposure/899. - Feature Neutral Mean, Standard deviation and Sharpe: https://docs.numer.ai/tournament/feature-neutral-correlation. - Smart Sharpe - Exposure Dissimilarity: https://forum.numer.ai/t/true-contribution-details/5128/4. - Autocorrelation (1st order). - Calmar Ratio. - Performance vs. Benchmark predictions. - Mean, Standard Deviation and Sharpe for TB200 (Buy top 200 stocks and sell bottom 200 stocks). - Mean, Standard Deviation and Sharpe for TB500 (Buy top 500 stocks and sell bottom 500 stocks).</p> <p>:param metrics_list: List of metrics to calculate. Default: FAST_METRICS. :param era_col: Column name pointing to eras. Most commonly \"era\" for Numerai Classic and \"friday_date\" for Numerai Signals. :param custom_functions: Additional functions called in evaluation. Check out the NumerBlox docs on evaluation for more info on using custom functions. :param show_detailed_progress_bar: Show detailed progress bar for evaluation of each prediction column.</p> <p>Note that we calculate the sample standard deviation with ddof=0. It may differ slightly from the standard Pandas calculation, but is consistent with how NumPy computes standard deviation. More info: https://stackoverflow.com/questions/24984178/different-std-in-pandas-vs-numpy</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>class BaseEvaluator:\n    \"\"\"\n    Evaluation functionality that is relevant for both\n    Numerai Classic and Numerai Signals.\n\n    Metrics include:\n    - Mean, Standard Deviation and Sharpe (Corrv2) for era returns.\n    - Max drawdown.\n    - Annual Percentage Yield (APY).\n    - Correlation with benchmark predictions.\n    - Max feature exposure: https://forum.numer.ai/t/model-diagnostics-feature-exposure/899.\n    - Feature Neutral Mean, Standard deviation and Sharpe: https://docs.numer.ai/tournament/feature-neutral-correlation.\n    - Smart Sharpe\n    - Exposure Dissimilarity: https://forum.numer.ai/t/true-contribution-details/5128/4.\n    - Autocorrelation (1st order).\n    - Calmar Ratio.\n    - Performance vs. Benchmark predictions.\n    - Mean, Standard Deviation and Sharpe for TB200 (Buy top 200 stocks and sell bottom 200 stocks).\n    - Mean, Standard Deviation and Sharpe for TB500 (Buy top 500 stocks and sell bottom 500 stocks).\n\n    :param metrics_list: List of metrics to calculate. Default: FAST_METRICS.\n    :param era_col: Column name pointing to eras. Most commonly \"era\" for Numerai Classic and \"friday_date\" for Numerai Signals.\n    :param custom_functions: Additional functions called in evaluation.\n    Check out the NumerBlox docs on evaluation for more info on using custom functions.\n    :param show_detailed_progress_bar: Show detailed progress bar for evaluation of each prediction column.\n\n    Note that we calculate the sample standard deviation with ddof=0.\n    It may differ slightly from the standard Pandas calculation, but\n    is consistent with how NumPy computes standard deviation.\n    More info:\n    https://stackoverflow.com/questions/24984178/different-std-in-pandas-vs-numpy\n    \"\"\"\n\n    def __init__(\n        self,\n        metrics_list: List[str],\n        era_col: str,\n        custom_functions: Dict[str, Dict[str, Any]],\n        show_detailed_progress_bar: bool,\n    ):\n        self.era_col = era_col\n        self.metrics_list = metrics_list\n        self.custom_functions = custom_functions\n        if self.custom_functions is not None:\n            self.check_custom_functions()\n        self.show_detailed_progress_bar = show_detailed_progress_bar\n\n    def full_evaluation(\n        self,\n        dataf: pd.DataFrame,\n        pred_cols: List[str],\n        target_col: str = \"target\",\n        benchmark_cols: list = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Perform evaluation for each prediction column in pred_cols.\n        By default only the \"prediction\" column is evaluated.\n        Evaluation is done against given target and benchmark prediction column.\n        :param dataf: DataFrame containing era_col, pred_cols, target_col and optional benchmark_cols.\n        :param pred_cols: List of prediction columns to calculate evaluation metrics for.\n        :param target_col: Target column to evaluate against.\n        :param benchmark_cols: Optional list of benchmark columns to calculate evaluation metrics for.\n        \"\"\"\n        val_stats = pd.DataFrame()\n        feature_cols = [col for col in dataf.columns if col.startswith(\"feature\")]\n        cat_cols = (\n            dataf[feature_cols].select_dtypes(include=[\"category\"]).columns.to_list()\n        )\n        if cat_cols:\n            print(\n                f\"WARNING: Categorical features detected that cannot be used for neutralization. Removing columns: '{cat_cols}' for evaluation.\"\n            )\n            dataf.loc[:, feature_cols] = dataf[feature_cols].select_dtypes(\n                exclude=[\"category\"]\n            )\n        dataf = dataf.fillna(0.5)\n        for col in tqdm(pred_cols, desc=\"Evaluation: \"):\n            col_stats = self.evaluation_one_col(\n                dataf=dataf,\n                pred_col=col,\n                feature_cols=feature_cols,\n                target_col=target_col,\n                benchmark_cols=benchmark_cols,\n            )\n            val_stats = pd.concat([val_stats, col_stats], axis=0)\n        return val_stats\n\n    def evaluation_one_col(\n        self,\n        dataf: pd.DataFrame,\n        feature_cols: list,\n        pred_col: str,\n        target_col: str,\n        benchmark_cols: list = None,\n    ):\n        \"\"\"\n        Perform evaluation for one prediction column\n        against given target and benchmark column(s).\n        \"\"\"\n        assert (\n            self.era_col in dataf.columns\n        ), f\"Era column '{self.era_col}' not found in DataFrame. Make sure to set the correct era_col.\"\n        assert (\n                pred_col in dataf.columns\n            ), f\"Prediction column '{pred_col}' not found in DataFrame. Make sure to set the correct pred_col.\"\n        assert (\n            target_col in dataf.columns\n        ), f\"Target column '{target_col}' not found in DataFrame. Make sure to set the correct target_col.\"\n        if benchmark_cols:\n            for col in benchmark_cols:\n                assert (\n                    col in dataf.columns\n                ), f\"Benchmark column '{col}' not found in DataFrame. Make sure to set the correct benchmark_cols.\"\n\n        # Check that all values are between 0 and 1\n        assert (\n            dataf[pred_col].min().min() &gt;= 0 and dataf[pred_col].max().max() &lt;= 1\n        ), \"All predictions should be between 0 and 1 (inclusive).\"\n        assert (\n            dataf[target_col].min() &gt;= 0 and dataf[target_col].max() &lt;= 1\n        ), \"All targets should be between 0 and 1 (inclusive).\"\n        if benchmark_cols is not None:\n            for col in benchmark_cols:\n                assert (\n                    dataf[col].min() &gt;= 0 and dataf[col].max() &lt;= 1\n                ), f\"All predictions for '{col}' should be between 0 and 1 (inclusive).\"\n\n        if self.show_detailed_progress_bar:\n            len_metrics_list = len(self.metrics_list)\n            len_benchmark_cols = 0 if benchmark_cols is None else len(benchmark_cols)\n            len_custom_functions = 0 if self.custom_functions is None else len(list(self.custom_functions.keys()))\n            len_pbar = len_metrics_list + len_benchmark_cols + len_custom_functions\n            pbar = tqdm(total=len_pbar, desc=\"Evaluation\")\n\n        col_stats = {}\n        col_stats[\"target\"] = target_col\n\n        # Compute stats per era (only if needed)\n        per_era_numerai_corrs = self.per_era_numerai_corrs(\n            dataf=dataf, pred_col=pred_col, target_col=target_col\n        )\n\n        # check if mean, std, or sharpe are in metrics_list\n        if \"mean_std_sharpe\" in self.metrics_list:\n            if self.show_detailed_progress_bar:\n                pbar.set_description_str(f\"mean_std_sharpe for evaluation\")\n                pbar.update(1)\n            mean, std, sharpe = self.mean_std_sharpe(era_corrs=per_era_numerai_corrs)\n            col_stats[\"mean\"] = mean\n            col_stats[\"std\"] = std\n            col_stats[\"sharpe\"] = sharpe\n\n        if \"legacy_mean_std_sharpe\" in self.metrics_list:\n            if self.show_detailed_progress_bar:\n                pbar.set_description_str(f\"legacy_mean_std_sharpe for evaluation\")\n                pbar.update(1)\n            per_era_corrs = self.per_era_corrs(\n                dataf=dataf, pred_col=pred_col, target_col=target_col\n            )\n            legacy_mean, legacy_std, legacy_sharpe = self.mean_std_sharpe(\n                era_corrs=per_era_corrs\n            )\n            col_stats[\"legacy_mean\"] = legacy_mean\n            col_stats[\"legacy_std\"] = legacy_std\n            col_stats[\"legacy_sharpe\"] = legacy_sharpe\n\n        if \"max_drawdown\" in self.metrics_list:\n            if self.show_detailed_progress_bar:\n                pbar.set_description_str(f\"max_drawdown for evaluation\")\n                pbar.update(1)\n            col_stats[\"max_drawdown\"] = self.max_drawdown(\n                era_corrs=per_era_numerai_corrs\n            )\n\n        if \"apy\":\n            if self.show_detailed_progress_bar:\n                pbar.set_description_str(f\"apy for evaluation\")\n                pbar.update(1)\n            col_stats[\"apy\"] = self.apy(era_corrs=per_era_numerai_corrs)\n\n        if \"calmar_ratio\" in self.metrics_list:\n            if self.show_detailed_progress_bar:\n                pbar.set_description_str(f\"calmar_ratio for evaluation\")\n                pbar.update(1)\n            if not \"max_drawdown\" in self.metrics_list:\n                col_stats[\"max_drawdown\"] = self.max_drawdown(\n                    era_corrs=per_era_numerai_corrs\n                )\n            if not \"apy\" in self.metrics_list:\n                col_stats[\"apy\"] = self.apy(era_corrs=per_era_numerai_corrs)\n            col_stats[\"calmar_ratio\"] = (\n                np.nan\n                if col_stats[\"max_drawdown\"] == 0\n                else col_stats[\"apy\"] / -col_stats[\"max_drawdown\"]\n            )\n\n        if \"autocorrelation\" in self.metrics_list:\n            if self.show_detailed_progress_bar:\n                pbar.set_description(f\"autocorrelation for evaluation\")\n                pbar.update(1)\n            col_stats[\"autocorrelation\"] = self.autocorr1(per_era_numerai_corrs)\n\n        if \"max_feature_exposure\" in self.metrics_list:\n            if self.show_detailed_progress_bar:\n                pbar.set_description_str(f\"max_feature_exposure for evaluation\")\n                pbar.update(1)\n            col_stats[\"max_feature_exposure\"] = self.max_feature_exposure(\n                dataf=dataf, feature_cols=feature_cols, pred_col=pred_col\n            )\n\n        if \"smart_sharpe\" in self.metrics_list:\n            if self.show_detailed_progress_bar:\n                pbar.set_description_str(f\"smart_sharpe for evaluation\")\n                pbar.update(1)\n            col_stats[\"smart_sharpe\"] = self.smart_sharpe(\n                era_corrs=per_era_numerai_corrs\n            )\n\n        if benchmark_cols is not None:\n            for bench_col in benchmark_cols:\n                if self.show_detailed_progress_bar:\n                    pbar.set_description_str(f\"Evaluation for benchmark column: '{bench_col}'\")\n                    pbar.update(1)\n\n                per_era_bench_corrs = self.per_era_numerai_corrs(\n                    dataf=dataf, pred_col=bench_col, target_col=target_col\n                )\n\n                if \"mean_std_sharpe\" in self.metrics_list:\n                    if self.show_detailed_progress_bar:\n                        pbar.set_description_str(f\"mean_std_sharpe for benchmark column: '{bench_col}'\")\n                    bench_mean, bench_std, bench_sharpe = self.mean_std_sharpe(\n                        era_corrs=per_era_bench_corrs\n                    )\n                    col_stats[f\"mean_vs_{bench_col}\"] = mean - bench_mean\n                    col_stats[f\"std_vs_{bench_col}\"] = std - bench_std\n                    col_stats[f\"sharpe_vs_{bench_col}\"] = sharpe - bench_sharpe\n\n                if \"mc_mean_std_sharpe\" in self.metrics_list:\n                    if self.show_detailed_progress_bar:\n                        pbar.set_description_str(f\"mc_mean_std_sharpe for benchmark column: '{bench_col}'\")\n                    mc_scores = self.contributive_correlation(\n                        dataf=dataf,\n                        pred_col=pred_col,\n                        target_col=target_col,\n                        other_col=bench_col,\n                    )\n                    col_stats[f\"mc_mean_{bench_col}\"] = np.nanmean(mc_scores)\n                    col_stats[f\"mc_std_{bench_col}\"] = np.nanstd(mc_scores)\n                    col_stats[f\"mc_sharpe_{bench_col}\"] = (\n                        np.nan\n                        if col_stats[f\"mc_std_{bench_col}\"] == 0\n                        else col_stats[f\"mc_mean_{bench_col}\"]\n                        / col_stats[f\"mc_std_{bench_col}\"]\n                    )\n\n                if \"corr_with\" in self.metrics_list:\n                    if self.show_detailed_progress_bar:\n                        pbar.set_description_str(f\"corr_with for benchmark column: '{bench_col}'\")\n                    col_stats[f\"corr_with_{bench_col}\"] = self.cross_correlation(\n                        dataf=dataf, pred_col=bench_col, other_col=bench_col\n                    )\n\n                if \"legacy_mc_mean_std_sharpe\" in self.metrics_list:\n                    if self.show_detailed_progress_bar:\n                        pbar.set_description_str(f\"legacy_mc_mean_std_sharpe for benchmark column: '{bench_col}'\")\n                    legacy_mc_scores = self.legacy_contribution(\n                        dataf=dataf,\n                        pred_col=pred_col,\n                        target_col=target_col,\n                        other_col=bench_col,\n                    )\n                    col_stats[f\"legacy_mc_mean_{bench_col}\"] = np.nanmean(\n                        legacy_mc_scores\n                    )\n                    col_stats[f\"legacy_mc_std_{bench_col}\"] = np.nanstd(\n                        legacy_mc_scores\n                    )\n                    col_stats[f\"legacy_mc_sharpe_{bench_col}\"] = (\n                        np.nan\n                        if col_stats[f\"legacy_mc_std_{bench_col}\"] == 0\n                        else col_stats[f\"legacy_mc_mean_{bench_col}\"]\n                        / col_stats[f\"legacy_mc_std_{bench_col}\"]\n                    )\n\n                if \"ex_diss\" in self.metrics_list or \"ex_diss_pearson\" in self.metrics_list:\n                    if self.show_detailed_progress_bar:\n                        pbar.set_description_str(f\"ex_diss_pearson for benchmark column: '{bench_col}'\")\n                    col_stats[\n                        f\"exposure_dissimilarity_pearson_{bench_col}\"\n                    ] = self.exposure_dissimilarity(\n                        dataf=dataf, pred_col=pred_col, other_col=bench_col,\n                        corr_method=\"pearson\"\n                    )\n                if \"ex_diss_spearman\" in self.metrics_list:\n                    if self.show_detailed_progress_bar:\n                        pbar.set_description_str(f\"ex_diss_spearman for benchmark column: '{bench_col}'\")\n                    col_stats[\n                        f\"exposure_dissimilarity_spearman_{bench_col}\"\n                    ] = self.exposure_dissimilarity(\n                        dataf=dataf, pred_col=pred_col, other_col=bench_col,\n                        corr_method=\"spearman\"\n                    )\n\n        # Compute intensive stats\n        if \"fn_mean_std_sharpe\" in self.metrics_list:\n            if self.show_detailed_progress_bar:\n                pbar.set_description_str(f\"fn_mean_std_sharpe for evaluation\")\n                pbar.update(1)\n            fn_mean, fn_std, fn_sharpe = self.feature_neutral_mean_std_sharpe(\n                dataf=dataf,\n                pred_col=pred_col,\n                target_col=target_col,\n                feature_names=feature_cols,\n            )\n            col_stats[\"feature_neutral_mean\"] = fn_mean\n            col_stats[\"feature_neutral_std\"] = fn_std\n            col_stats[\"feature_neutral_sharpe\"] = fn_sharpe\n\n        if \"tb200_mean_std_sharpe\" in self.metrics_list:\n            if self.show_detailed_progress_bar:\n                pbar.set_description_str(f\"tb200_mean_std_sharpe for evaluation\")\n                pbar.update(1)\n            tb200_mean, tb200_std, tb200_sharpe = self.tbx_mean_std_sharpe(\n                dataf=dataf, pred_col=pred_col, target_col=target_col, tb=200\n            )\n            col_stats[\"tb200_mean\"] = tb200_mean\n            col_stats[\"tb200_std\"] = tb200_std\n            col_stats[\"tb200_sharpe\"] = tb200_sharpe\n\n        if \"tb500_mean_std_sharpe\" in self.metrics_list:\n            if self.show_detailed_progress_bar:\n                pbar.set_description_str(f\"tb500_mean_std_sharpe for evaluation\")\n                pbar.update(1)\n            tb500_mean, tb500_std, tb500_sharpe = self.tbx_mean_std_sharpe(\n                dataf=dataf, pred_col=pred_col, target_col=target_col, tb=500\n            )\n            col_stats[\"tb500_mean\"] = tb500_mean\n            col_stats[\"tb500_std\"] = tb500_std\n            col_stats[\"tb500_sharpe\"] = tb500_sharpe\n\n        # Custom functions\n        if self.custom_functions is not None:\n            local_vars = locals()\n            for func_name, func_info in self.custom_functions.items():\n                if self.show_detailed_progress_bar:\n                    pbar.set_description_str(f\"custom function: '{func_name}' for evaluation\")\n                    pbar.update(1)\n                func = func_info['func']\n                args = func_info['args']\n                local_args = func_info['local_args']\n                resolved_args = {}\n                for k, v in args.items():\n                    # Resolve variables defined as local args\n                    if isinstance(v, str) and v in local_args:\n                        if v not in local_vars:\n                            raise ValueError(f\"Variable '{v}' was defined in 'local_args', but was not found in local variables. Make sure to set the correct local_args.\")\n                        else:\n                            resolved_args[k] = local_vars[v]\n                    else:\n                        resolved_args[k] = v\n                col_stats[func_name] = func(**resolved_args)\n\n        col_stats_df = pd.DataFrame(col_stats, index=[pred_col])\n        if self.show_detailed_progress_bar:\n            pbar.update(1)\n            pbar.close()\n        return col_stats_df\n\n    def per_era_corrs(\n        self, dataf: pd.DataFrame, pred_col: str, target_col: str\n    ) -&gt; pd.Series:\n        \"\"\"Correlation between prediction and target for each era.\"\"\"\n        return dataf.groupby(self.era_col).apply(\n            lambda d: self._normalize_uniform(d[pred_col].fillna(0.5)).corr(\n                d[target_col]\n            )\n        )\n\n    def per_era_numerai_corrs(\n        self, dataf: pd.DataFrame, pred_col: str, target_col: str\n    ) -&gt; pd.Series:\n        \"\"\"Numerai Corr between prediction and target for each era.\"\"\"\n        return dataf.groupby(self.era_col).apply(\n            lambda d: self.numerai_corr(d.fillna(0.5), pred_col, target_col)\n        )\n\n    def mean_std_sharpe(\n        self, era_corrs: pd.Series\n    ) -&gt; Tuple[np.float64, np.float64, np.float64]:\n        \"\"\"\n        Average, standard deviation and Sharpe ratio for\n        correlations per era.\n        \"\"\"\n        mean = pd.Series(era_corrs.mean()).item()\n        std = pd.Series(era_corrs.std(ddof=0)).item()\n        sharpe = np.nan if std == 0 else mean / std\n        return mean, std, sharpe\n\n    def numerai_corr(\n        self, dataf: pd.DataFrame, pred_col: str, target_col: str\n    ) -&gt; np.float64:\n        \"\"\"\n        Computes 'Numerai Corr' aka 'Corrv2'.\n        More info: https://forum.numer.ai/t/target-cyrus-new-primary-target/6303\n\n        Assumes original target col as input (i.e. in [0, 1] range).\n        \"\"\"\n        # Rank and gaussianize predictions\n        ranked_preds = self._normalize_uniform(\n            dataf[pred_col].fillna(0.5), method=\"average\"\n        )\n        gauss_ranked_preds = stats.norm.ppf(ranked_preds)\n        # Center target from [0...1] to [-0.5...0.5] range\n        targets = dataf[target_col]\n        centered_target = targets - targets.mean()\n        # Accentuate tails of predictions and targets\n        preds_p15 = np.sign(gauss_ranked_preds) * np.abs(gauss_ranked_preds) ** 1.5\n        target_p15 = np.sign(centered_target) * np.abs(centered_target) ** 1.5\n        # Pearson correlation\n        corr, _ = stats.pearsonr(preds_p15, target_p15)\n        return corr\n\n    @staticmethod\n    def max_drawdown(era_corrs: pd.Series) -&gt; np.float64:\n        \"\"\"Maximum drawdown per era.\"\"\"\n        # Arbitrarily large window\n        rolling_max = (\n            (era_corrs + 1).cumprod().rolling(window=9000, min_periods=1).max()\n        )\n        daily_value = (era_corrs + 1).cumprod()\n        max_drawdown = -((rolling_max - daily_value) / rolling_max).max()\n        return max_drawdown\n\n    @staticmethod\n    def apy(era_corrs: pd.Series, stake_compounding_lag: int = 4) -&gt; np.float64:\n        \"\"\"\n        Annual percentage yield.\n        :param era_corrs: Correlation scores by era\n        :param stake_compounding_lag: Compounding lag for Numerai rounds (4 for Numerai Classic)\n        \"\"\"\n        payout_scores = era_corrs.clip(-0.25, 0.25)\n        payout_product = (payout_scores + 1).prod()\n        return (\n            payout_product\n            ** (\n                # 52 weeks of compounding minus n for stake compounding lag\n                (52 - stake_compounding_lag)\n                / len(payout_scores)\n            )\n            - 1\n        ) * 100\n\n    def cross_correlation(self, dataf: pd.DataFrame, pred_col: str, other_col: str):\n        \"\"\"\n        Corrv2 correlation with other predictions (like another model, example predictions or meta model prediction).\n        :param dataf: DataFrame containing both pred_col and other_col.\n        :param pred_col: Main Prediction.\n        :param other_col: Other prediction column to calculate correlation with pred_col.\n\n        :return: Correlation between Corrv2's of pred_col and other_col.\n        \"\"\"\n        return self.per_era_numerai_corrs(\n            dataf=dataf,\n            pred_col=pred_col,\n            target_col=other_col,\n        ).mean()\n\n    def max_feature_exposure(\n        self, dataf: pd.DataFrame, feature_cols: List[str], pred_col: str\n    ) -&gt; np.float64:\n        \"\"\"Maximum exposure over all features.\"\"\"\n        max_per_era = dataf.groupby(self.era_col).apply(\n            lambda d: d[feature_cols].corrwith(d[pred_col]).abs().max()\n        )\n        max_feature_exposure = max_per_era.mean(skipna=True)\n        return max_feature_exposure\n\n    def feature_neutral_mean_std_sharpe(\n        self, dataf: pd.DataFrame, pred_col: str, target_col: str, feature_names: list\n    ) -&gt; Tuple[np.float64, np.float64, np.float64]:\n        \"\"\"\n        Feature neutralized mean performance.\n        More info: https://docs.numer.ai/tournament/feature-neutral-correlation\n        \"\"\"\n        fn = FeatureNeutralizer(pred_name=pred_col, proportion=1.0)\n        neutralized_preds = fn.predict(\n            dataf[pred_col], features=dataf[feature_names], eras=dataf[self.era_col]\n        )\n        # Construct new DataFrame with era col, target col and preds\n        neutralized_dataf = pd.DataFrame(columns=[self.era_col, target_col, pred_col])\n        neutralized_dataf[self.era_col] = dataf[self.era_col]\n        neutralized_dataf[target_col] = dataf[target_col]\n        neutralized_dataf[pred_col] = neutralized_preds\n\n        neutral_corrs = self.per_era_numerai_corrs(\n            dataf=neutralized_dataf,\n            pred_col=pred_col,\n            target_col=target_col,\n        )\n        mean, std, sharpe = self.mean_std_sharpe(era_corrs=neutral_corrs)\n        return mean, std, sharpe\n\n    def tbx_mean_std_sharpe(\n        self, dataf: pd.DataFrame, pred_col: str, target_col: str, tb: int = 200\n    ) -&gt; Tuple[np.float64, np.float64, np.float64]:\n        \"\"\"\n        Calculate Mean, Standard deviation and Sharpe ratio\n        when we focus on the x top and x bottom predictions.\n        :param tb: How many of top and bottom predictions to focus on.\n        TB200 and TB500 are the most common situations.\n        \"\"\"\n        tb_val_corrs = self._score_by_date(\n            dataf=dataf, columns=[pred_col], target=target_col, tb=tb\n        )\n        return self.mean_std_sharpe(era_corrs=tb_val_corrs)\n\n    def exposure_dissimilarity(\n        self, dataf: pd.DataFrame, pred_col: str, other_col: str, corr_method: str = \"pearson\"\n    ) -&gt; np.float32:\n        \"\"\"\n        Model pattern of feature exposure to the another column.\n        See TC details forum post: https://forum.numer.ai/t/true-contribution-details/5128/4\n        :param dataf: DataFrame containing both pred_col and other_col.\n        :param pred_col: Main Prediction.\n        :param other_col: Other prediction column to calculate exposure dissimilarity against.\n        :param corr_method: Correlation method to use for calculating feature exposures.\n        corr_method should be one of ['pearson', 'kendall', 'spearman']. Default: 'pearson'.\n        \"\"\"\n        assert corr_method in [\"pearson\", \"kendall\", \"spearman\"], f\"corr_method should be one of ['pearson', 'kendall', 'spearman']. Got: '{corr_method}'\"\n        feature_cols = [col for col in dataf.columns if col.startswith(\"feature\")]\n        U = dataf[feature_cols].corrwith(dataf[pred_col], method=corr_method).values\n        E = dataf[feature_cols].corrwith(dataf[other_col], method=corr_method).values\n\n        denominator = np.dot(E, E)\n        if denominator == 0:\n            exp_dis = 0\n        else:\n            exp_dis = 1 - np.dot(U, E) / denominator\n        return exp_dis\n\n    @staticmethod\n    def _neutralize_series(\n        series: pd.Series, by: pd.Series, proportion=1.0\n    ) -&gt; pd.Series:\n        scores = series.values.reshape(-1, 1)\n        exposures = by.values.reshape(-1, 1)\n\n        # This line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n        exposures = np.hstack(\n            (exposures, np.array([np.nanmean(series)] * len(exposures)).reshape(-1, 1))\n        )\n\n        correction = proportion * (\n            exposures.dot(np.linalg.lstsq(exposures, scores, rcond=None)[0])\n        )\n        corrected_scores = scores - correction\n        neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n        return neutralized\n\n    @staticmethod\n    def _orthogonalize(v: np.ndarray, u: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Orthogonalizes v with respect to u by projecting v onto u,\n        then subtracting that projection from v.\n\n        This will reach the same result as the neutralize function when v and u\n        are single column vectors, but this is much faster.\n\n        Arguments:\n            v: np.ndarray - the vector to orthogonalize\n            u: np.ndarray - the vector to orthogonalize v against\n\n        Returns:\n            np.ndarray - the orthogonalized vector v\n        \"\"\"\n        # Calculate the dot product of u and v\n        dot_product = u.T @ v\n\n        # Calculate the projection of v onto u\n        projection = (dot_product / (u.T @ u)) * u\n\n        # Subtract the projection from v\n        return v - projection\n\n    def _score_by_date(\n        self, dataf: pd.DataFrame, columns: list, target: str, tb: int = None\n    ):\n        \"\"\"\n        Get era correlation based on given TB (x top and bottom predictions).\n        :param tb: How many of top and bottom predictions to focus on.\n        TB200 is the most common situation.\n        \"\"\"\n        unique_eras = dataf[self.era_col].unique()\n        computed = []\n        for u in unique_eras:\n            df_era = dataf[dataf[self.era_col] == u]\n            era_pred = np.float64(df_era[columns].values.T)\n            era_target = np.float64(df_era[target].values.T)\n\n            if tb is None:\n                ccs = np.corrcoef(era_target, era_pred)[0, 1:]\n            else:\n                tbidx = np.argsort(era_pred, axis=1)\n                tbidx = np.concatenate([tbidx[:, :tb], tbidx[:, -tb:]], axis=1)\n                ccs = [\n                    np.corrcoef(era_target[idx], pred[idx])[0, 1]\n                    for idx, pred in zip(tbidx, era_pred)\n                ]\n                ccs = np.array(ccs)\n            computed.append(ccs)\n        return pd.DataFrame(\n            np.array(computed), columns=columns, index=dataf[self.era_col].unique()\n        )\n\n    @staticmethod\n    def _normalize_uniform(df: pd.DataFrame, method: str = \"first\") -&gt; pd.Series:\n        \"\"\"\n        Normalize predictions uniformly using ranks.\n        NOTE: Make sure the range of predictions is [0, 1] (inclusive).\n        \"\"\"\n        x = (df.rank(method=method) - 0.5) / len(\n            df\n        )  # TODO: Evaluate if subtracting df.mean() is better\n        return pd.Series(x, index=df.index)\n\n    def get_feature_exposures_pearson(\n        self,\n        dataf: pd.DataFrame,\n        pred_col: str,\n        feature_list: List[str],\n        cpu_cores: int = -1,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate feature exposures for each era using Pearson correlation.\n\n        :param dataf: DataFrame containing predictions, features, and eras.\n        :param pred_col: Prediction column to calculate feature exposures for.\n        :param feature_list: List of feature columns in X.\n        :param cpu_cores: Number of CPU cores to use for parallelization.\n        :return: DataFrame with Pearson feature exposures by era for each feature.\n        \"\"\"\n\n        def calculate_era_pearson_exposure(\n            era, group, feature_list, pred_col_normalized\n        ):\n            data_matrix = group[feature_list + [pred_col_normalized]].values\n            correlations = np.corrcoef(data_matrix, rowvar=False)\n\n            # Get the correlations of all features with the predictions (which is the last column)\n            feature_correlations = correlations[:-1, -1]\n            return era, feature_correlations\n\n        normalized_ranks = (dataf[[pred_col]].rank(method=\"first\") - 0.5) / len(dataf)\n        dataf[f\"{pred_col}_normalized\"] = stats.norm.ppf(normalized_ranks)\n        feature_exposure_data = pd.DataFrame(\n            index=dataf[\"era\"].unique(), columns=feature_list\n        )\n\n        grouped_data = list(dataf.groupby(\"era\"))\n\n        results = Parallel(n_jobs=cpu_cores)(\n            delayed(calculate_era_pearson_exposure)(\n                era, group, feature_list, f\"{pred_col}_normalized\"\n            )\n            for era, group in grouped_data\n        )\n\n        for era, feature_correlations in results:\n            feature_exposure_data.loc[era, :] = feature_correlations\n        return feature_exposure_data\n\n    def get_feature_exposures_corrv2(\n        self,\n        dataf: pd.DataFrame,\n        pred_col: str,\n        feature_list: List[str],\n        cpu_cores: int = -1,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate feature exposures for each era using 'Numerai Corr'.\n        Results will be similar to get_feature_exposures() but more accurate.\n        This method will take longer to compute.\n\n        :param dataf: DataFrame containing predictions, features, and eras.\n        :param pred_col: Prediction column to calculate feature exposures for.\n        :param feature_list: List of feature columns in X.\n        :param cpu_cores: Number of CPU cores to use for parallelization.\n        Default: -1 (all cores).\n        :return: DataFrame with Corrv2 feature exposures by era for each feature.\n        \"\"\"\n\n        def calculate_era_feature_exposure(era, group, pred_col, feature_list):\n            exposures = {}\n            for feature in feature_list:\n                corr = self.numerai_corr(\n                    group, pred_col=f\"{pred_col}_normalized\", target_col=feature\n                )\n                exposures[feature] = corr\n            return era, exposures\n\n        normalized_ranks = (dataf[[pred_col]].rank(method=\"first\") - 0.5) / len(dataf)\n        dataf[f\"{pred_col}_normalized\"] = stats.norm.ppf(normalized_ranks)\n        feature_exposure_data = pd.DataFrame(\n            index=dataf[\"era\"].unique(), columns=feature_list\n        )\n\n        grouped_data = list(dataf.groupby(\"era\"))\n\n        results = Parallel(n_jobs=cpu_cores)(\n            delayed(calculate_era_feature_exposure)(era, group, pred_col, feature_list)\n            for era, group in grouped_data\n        )\n        for era, exposures in results:\n            feature_exposure_data.loc[era, :] = exposures\n        return feature_exposure_data\n\n    def smart_sharpe(self, era_corrs: pd.Series) -&gt; np.float64:\n        \"\"\"\n        Sharpe adjusted for autocorrelation.\n        :param era_corrs: Correlation scores by era\n        \"\"\"\n        return np.nanmean(era_corrs) / (\n            np.nanstd(era_corrs, ddof=1) * self.autocorr_penalty(era_corrs)\n        )\n\n    def autocorr_penalty(self, era_corrs: pd.Series) -&gt; np.float64:\n        \"\"\"\n        Adjusting factor for autocorrelation. Used in Smart Sharpe.\n        :param era_corrs: Correlation scores by era.\n        \"\"\"\n        n = len(era_corrs)\n        # 1st order autocorrelation\n        p = self.autocorr1(era_corrs)\n        return np.sqrt(1 + 2 * np.sum([((n - i) / n) * p**i for i in range(1, n)]))\n\n    def autocorr1(self, era_corrs: pd.Series) -&gt; np.float64:\n        \"\"\"\n        1st order autocorrelation.\n        :param era_corrs: Correlation scores by era.\n        \"\"\"\n        return np.corrcoef(era_corrs[:-1], era_corrs[1:])[0, 1]\n\n    def legacy_contribution(\n        self, dataf: pd.DataFrame, pred_col: str, target_col: str, other_col: str\n    ):\n        \"\"\"\n        Legacy contibution mean, standard deviation and sharpe ratio.\n        More info: https://forum.numer.ai/t/mmc2-announcement/93\n\n        :param dataf: DataFrame containing era_col, pred_col, target_col and other_col.\n        :param pred_col: Prediction column to calculate MMC for.\n        :param target_col: Target column to calculate MMC against.\n        :param other_col: Meta model column containing predictions to neutralize against.\n\n        :return: List of legacy contribution scores by era.\n        \"\"\"\n        legacy_mc_scores = []\n        # Standard deviation of a uniform distribution\n        COVARIANCE_FACTOR = 0.29**2\n        # Calculate MMC for each era\n        for _, x in dataf.groupby(self.era_col):\n            series = self._neutralize_series(\n                self._normalize_uniform(x[pred_col]), (x[other_col])\n            )\n            legacy_mc_scores.append(\n                np.cov(series, x[target_col])[0, 1] / COVARIANCE_FACTOR\n            )\n\n        return legacy_mc_scores\n\n    def contributive_correlation(\n        self, dataf: pd.DataFrame, pred_col: str, target_col: str, other_col: str\n    ) -&gt; np.array:\n        \"\"\"Calculate the contributive correlation of the given predictions\n        wrt the given meta model.\n        see: https://docs.numer.ai/numerai-tournament/scoring/meta-model-contribution-mmc-and-bmc\n\n        Uses Numerai's official scoring function for contribution under the hood.\n        See: https://github.com/numerai/numerai-tools/blob/master/numerai_tools/scoring.py\n\n        Calculate contributive correlation by:\n        1. tie-kept ranking each prediction and the meta model\n        2. gaussianizing each prediction and the meta model\n        3. orthogonalizing each prediction wrt the meta model\n        3.5. scaling the targets to buckets [-2, -1, 0, 1, 2]\n        4. dot product the orthogonalized predictions and the targets\n       then normalize by the length of the target (equivalent to covariance)\n\n        :param dataf: DataFrame containing era_col, pred_col, target_col and other_col.\n        :param pred_col: Prediction column to calculate MMC for.\n        :param target_col: Target column to calculate MMC against.\n        Make sure the range of targets is [0, 1] (inclusive). \n        If the function is called from full_evalation, this is guaranteed because of the checks.\n        :param other_col: Meta model column containing predictions to neutralize against.\n\n        :return: A 1D NumPy array of contributive correlations by era.\n        \"\"\"\n        mc_scores = []\n        for _, x in dataf.groupby(self.era_col):\n            mc = correlation_contribution(x[[pred_col]], \n                                          x[other_col], \n                                          x[target_col])\n            mc_scores.append(mc)\n        return np.array(mc_scores).ravel()\n\n    def check_custom_functions(self):\n        if not isinstance(self.custom_functions, dict):\n            raise ValueError(\"custom_functions must be a dictionary\")\n\n        for func_name, func_info in self.custom_functions.items():\n            if not isinstance(func_info, dict) or 'func' not in func_info or 'args' not in func_info:\n                raise ValueError(f\"Function {func_name} must have a 'func' and 'args' key\")\n\n            if not callable(func_info['func']):\n                raise ValueError(f\"The 'func' value for '{func_name}' in custom_functions must be a callable function.\")\n\n            if not isinstance(func_info['args'], dict):\n                raise ValueError(f\"'args' for '{func_name}' in custom_functions must be a dictionary\")\n\n            if \"local_args\" in func_info:\n                if not isinstance(func_info['local_args'], list):\n                    raise ValueError(f\"The 'local_args' key for {func_name} in custom_functionsmust be a list\")\n                for local_arg in func_info['local_args']:\n                    if not isinstance(local_arg, str):\n                        raise ValueError(f\"Local arg '{local_arg}' for '{func_name}' in custom_functions must be string.\")\n                    if local_arg not in list(func_info['args'].keys()):\n                        raise ValueError(f\"Local arg '{local_arg}' for '{func_name}' in custom_functions was not found in 'args'\")\n\n    def plot_correlations(\n        self,\n        dataf: pd.DataFrame,\n        pred_cols: List[str],\n        corr_cols: list = None,\n        target_col: str = \"target\",\n        roll_mean: int = 20,\n    ):\n        \"\"\"\n        Plot per era correlations over time.\n        :param dataf: DataFrame that contains at least all pred_cols, target_col and corr_cols.\n        :param pred_cols: List of prediction columns to calculate per era correlations for and plot.\n        :param corr_cols: Per era correlations already prepared to include in the plot.\n        This is optional for if you already have per era correlations prepared in your input dataf.\n        :param target_col: Target column name to compute per era correlations against.\n        :param roll_mean: How many eras should be averaged to compute a rolling score.\n        \"\"\"\n        validation_by_eras = pd.DataFrame()\n        # Compute per era correlation for each prediction column.\n        for pred_col in pred_cols:\n            per_era_corrs = self.per_era_numerai_corrs(\n                dataf, pred_col=pred_col, target_col=target_col\n            )\n            validation_by_eras.loc[:, pred_col] = per_era_corrs\n\n        # Add prepared per era correlation if any.\n        if corr_cols is not None:\n            for corr_col in corr_cols:\n                validation_by_eras.loc[:, corr_col] = dataf[corr_col]\n\n        validation_by_eras.rolling(roll_mean).mean().plot(\n            kind=\"line\",\n            marker=\"o\",\n            ms=4,\n            title=f\"Rolling Per Era Correlation Mean (rolling window size: {roll_mean})\",\n            figsize=(15, 5),\n        )\n        plt.legend(\n            loc=\"upper center\",\n            bbox_to_anchor=(0.5, -0.05),\n            fancybox=True,\n            shadow=True,\n            ncol=1,\n        )\n        plt.axhline(y=0.0, color=\"r\", linestyle=\"--\")\n        plt.show()\n\n        validation_by_eras.cumsum().plot(\n            title=\"Cumulative Sum of Era Correlations\", figsize=(15, 5)\n        )\n        plt.legend(\n            loc=\"upper center\",\n            bbox_to_anchor=(0.5, -0.05),\n            fancybox=True,\n            shadow=True,\n            ncol=1,\n        )\n        plt.axhline(y=0.0, color=\"r\", linestyle=\"--\")\n        plt.show()\n        return\n\n    @staticmethod\n    def plot_correlation_heatmap(dataf: pd.DataFrame, pred_cols: List[str]):\n        corr_matrix = dataf[pred_cols].corr().to_numpy()\n\n        plt.figure(figsize=(20, 20))\n\n        # Create heatmap\n        plt.imshow(corr_matrix, cmap=\"coolwarm\", interpolation=\"none\")\n        plt.colorbar()\n\n        # Add ticks and labels\n        ticks = np.arange(0, len(pred_cols), 1)\n        plt.xticks(ticks, pred_cols, rotation=90, fontsize=8)\n        plt.yticks(ticks, pred_cols, fontsize=8)\n\n        plt.show()\n        return\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.apy","title":"<code>apy(era_corrs, stake_compounding_lag=4)</code>  <code>staticmethod</code>","text":"<p>Annual percentage yield. :param era_corrs: Correlation scores by era :param stake_compounding_lag: Compounding lag for Numerai rounds (4 for Numerai Classic)</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>@staticmethod\ndef apy(era_corrs: pd.Series, stake_compounding_lag: int = 4) -&gt; np.float64:\n    \"\"\"\n    Annual percentage yield.\n    :param era_corrs: Correlation scores by era\n    :param stake_compounding_lag: Compounding lag for Numerai rounds (4 for Numerai Classic)\n    \"\"\"\n    payout_scores = era_corrs.clip(-0.25, 0.25)\n    payout_product = (payout_scores + 1).prod()\n    return (\n        payout_product\n        ** (\n            # 52 weeks of compounding minus n for stake compounding lag\n            (52 - stake_compounding_lag)\n            / len(payout_scores)\n        )\n        - 1\n    ) * 100\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.autocorr1","title":"<code>autocorr1(era_corrs)</code>","text":"<p>1st order autocorrelation. :param era_corrs: Correlation scores by era.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def autocorr1(self, era_corrs: pd.Series) -&gt; np.float64:\n    \"\"\"\n    1st order autocorrelation.\n    :param era_corrs: Correlation scores by era.\n    \"\"\"\n    return np.corrcoef(era_corrs[:-1], era_corrs[1:])[0, 1]\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.autocorr_penalty","title":"<code>autocorr_penalty(era_corrs)</code>","text":"<p>Adjusting factor for autocorrelation. Used in Smart Sharpe. :param era_corrs: Correlation scores by era.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def autocorr_penalty(self, era_corrs: pd.Series) -&gt; np.float64:\n    \"\"\"\n    Adjusting factor for autocorrelation. Used in Smart Sharpe.\n    :param era_corrs: Correlation scores by era.\n    \"\"\"\n    n = len(era_corrs)\n    # 1st order autocorrelation\n    p = self.autocorr1(era_corrs)\n    return np.sqrt(1 + 2 * np.sum([((n - i) / n) * p**i for i in range(1, n)]))\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.contributive_correlation","title":"<code>contributive_correlation(dataf, pred_col, target_col, other_col)</code>","text":"<p>Calculate the contributive correlation of the given predictions  wrt the given meta model.  see: https://docs.numer.ai/numerai-tournament/scoring/meta-model-contribution-mmc-and-bmc</p> <p>Uses Numerai's official scoring function for contribution under the hood.  See: https://github.com/numerai/numerai-tools/blob/master/numerai_tools/scoring.py</p> <p>Calculate contributive correlation by:  1. tie-kept ranking each prediction and the meta model  2. gaussianizing each prediction and the meta model  3. orthogonalizing each prediction wrt the meta model  3.5. scaling the targets to buckets [-2, -1, 0, 1, 2]  4. dot product the orthogonalized predictions and the targets then normalize by the length of the target (equivalent to covariance)</p> <p>:param dataf: DataFrame containing era_col, pred_col, target_col and other_col.  :param pred_col: Prediction column to calculate MMC for.  :param target_col: Target column to calculate MMC against.  Make sure the range of targets is [0, 1] (inclusive).   If the function is called from full_evalation, this is guaranteed because of the checks.  :param other_col: Meta model column containing predictions to neutralize against.</p> <p>:return: A 1D NumPy array of contributive correlations by era.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def contributive_correlation(\n    self, dataf: pd.DataFrame, pred_col: str, target_col: str, other_col: str\n) -&gt; np.array:\n    \"\"\"Calculate the contributive correlation of the given predictions\n    wrt the given meta model.\n    see: https://docs.numer.ai/numerai-tournament/scoring/meta-model-contribution-mmc-and-bmc\n\n    Uses Numerai's official scoring function for contribution under the hood.\n    See: https://github.com/numerai/numerai-tools/blob/master/numerai_tools/scoring.py\n\n    Calculate contributive correlation by:\n    1. tie-kept ranking each prediction and the meta model\n    2. gaussianizing each prediction and the meta model\n    3. orthogonalizing each prediction wrt the meta model\n    3.5. scaling the targets to buckets [-2, -1, 0, 1, 2]\n    4. dot product the orthogonalized predictions and the targets\n   then normalize by the length of the target (equivalent to covariance)\n\n    :param dataf: DataFrame containing era_col, pred_col, target_col and other_col.\n    :param pred_col: Prediction column to calculate MMC for.\n    :param target_col: Target column to calculate MMC against.\n    Make sure the range of targets is [0, 1] (inclusive). \n    If the function is called from full_evalation, this is guaranteed because of the checks.\n    :param other_col: Meta model column containing predictions to neutralize against.\n\n    :return: A 1D NumPy array of contributive correlations by era.\n    \"\"\"\n    mc_scores = []\n    for _, x in dataf.groupby(self.era_col):\n        mc = correlation_contribution(x[[pred_col]], \n                                      x[other_col], \n                                      x[target_col])\n        mc_scores.append(mc)\n    return np.array(mc_scores).ravel()\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.cross_correlation","title":"<code>cross_correlation(dataf, pred_col, other_col)</code>","text":"<p>Corrv2 correlation with other predictions (like another model, example predictions or meta model prediction). :param dataf: DataFrame containing both pred_col and other_col. :param pred_col: Main Prediction. :param other_col: Other prediction column to calculate correlation with pred_col.</p> <p>:return: Correlation between Corrv2's of pred_col and other_col.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def cross_correlation(self, dataf: pd.DataFrame, pred_col: str, other_col: str):\n    \"\"\"\n    Corrv2 correlation with other predictions (like another model, example predictions or meta model prediction).\n    :param dataf: DataFrame containing both pred_col and other_col.\n    :param pred_col: Main Prediction.\n    :param other_col: Other prediction column to calculate correlation with pred_col.\n\n    :return: Correlation between Corrv2's of pred_col and other_col.\n    \"\"\"\n    return self.per_era_numerai_corrs(\n        dataf=dataf,\n        pred_col=pred_col,\n        target_col=other_col,\n    ).mean()\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.evaluation_one_col","title":"<code>evaluation_one_col(dataf, feature_cols, pred_col, target_col, benchmark_cols=None)</code>","text":"<p>Perform evaluation for one prediction column against given target and benchmark column(s).</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def evaluation_one_col(\n    self,\n    dataf: pd.DataFrame,\n    feature_cols: list,\n    pred_col: str,\n    target_col: str,\n    benchmark_cols: list = None,\n):\n    \"\"\"\n    Perform evaluation for one prediction column\n    against given target and benchmark column(s).\n    \"\"\"\n    assert (\n        self.era_col in dataf.columns\n    ), f\"Era column '{self.era_col}' not found in DataFrame. Make sure to set the correct era_col.\"\n    assert (\n            pred_col in dataf.columns\n        ), f\"Prediction column '{pred_col}' not found in DataFrame. Make sure to set the correct pred_col.\"\n    assert (\n        target_col in dataf.columns\n    ), f\"Target column '{target_col}' not found in DataFrame. Make sure to set the correct target_col.\"\n    if benchmark_cols:\n        for col in benchmark_cols:\n            assert (\n                col in dataf.columns\n            ), f\"Benchmark column '{col}' not found in DataFrame. Make sure to set the correct benchmark_cols.\"\n\n    # Check that all values are between 0 and 1\n    assert (\n        dataf[pred_col].min().min() &gt;= 0 and dataf[pred_col].max().max() &lt;= 1\n    ), \"All predictions should be between 0 and 1 (inclusive).\"\n    assert (\n        dataf[target_col].min() &gt;= 0 and dataf[target_col].max() &lt;= 1\n    ), \"All targets should be between 0 and 1 (inclusive).\"\n    if benchmark_cols is not None:\n        for col in benchmark_cols:\n            assert (\n                dataf[col].min() &gt;= 0 and dataf[col].max() &lt;= 1\n            ), f\"All predictions for '{col}' should be between 0 and 1 (inclusive).\"\n\n    if self.show_detailed_progress_bar:\n        len_metrics_list = len(self.metrics_list)\n        len_benchmark_cols = 0 if benchmark_cols is None else len(benchmark_cols)\n        len_custom_functions = 0 if self.custom_functions is None else len(list(self.custom_functions.keys()))\n        len_pbar = len_metrics_list + len_benchmark_cols + len_custom_functions\n        pbar = tqdm(total=len_pbar, desc=\"Evaluation\")\n\n    col_stats = {}\n    col_stats[\"target\"] = target_col\n\n    # Compute stats per era (only if needed)\n    per_era_numerai_corrs = self.per_era_numerai_corrs(\n        dataf=dataf, pred_col=pred_col, target_col=target_col\n    )\n\n    # check if mean, std, or sharpe are in metrics_list\n    if \"mean_std_sharpe\" in self.metrics_list:\n        if self.show_detailed_progress_bar:\n            pbar.set_description_str(f\"mean_std_sharpe for evaluation\")\n            pbar.update(1)\n        mean, std, sharpe = self.mean_std_sharpe(era_corrs=per_era_numerai_corrs)\n        col_stats[\"mean\"] = mean\n        col_stats[\"std\"] = std\n        col_stats[\"sharpe\"] = sharpe\n\n    if \"legacy_mean_std_sharpe\" in self.metrics_list:\n        if self.show_detailed_progress_bar:\n            pbar.set_description_str(f\"legacy_mean_std_sharpe for evaluation\")\n            pbar.update(1)\n        per_era_corrs = self.per_era_corrs(\n            dataf=dataf, pred_col=pred_col, target_col=target_col\n        )\n        legacy_mean, legacy_std, legacy_sharpe = self.mean_std_sharpe(\n            era_corrs=per_era_corrs\n        )\n        col_stats[\"legacy_mean\"] = legacy_mean\n        col_stats[\"legacy_std\"] = legacy_std\n        col_stats[\"legacy_sharpe\"] = legacy_sharpe\n\n    if \"max_drawdown\" in self.metrics_list:\n        if self.show_detailed_progress_bar:\n            pbar.set_description_str(f\"max_drawdown for evaluation\")\n            pbar.update(1)\n        col_stats[\"max_drawdown\"] = self.max_drawdown(\n            era_corrs=per_era_numerai_corrs\n        )\n\n    if \"apy\":\n        if self.show_detailed_progress_bar:\n            pbar.set_description_str(f\"apy for evaluation\")\n            pbar.update(1)\n        col_stats[\"apy\"] = self.apy(era_corrs=per_era_numerai_corrs)\n\n    if \"calmar_ratio\" in self.metrics_list:\n        if self.show_detailed_progress_bar:\n            pbar.set_description_str(f\"calmar_ratio for evaluation\")\n            pbar.update(1)\n        if not \"max_drawdown\" in self.metrics_list:\n            col_stats[\"max_drawdown\"] = self.max_drawdown(\n                era_corrs=per_era_numerai_corrs\n            )\n        if not \"apy\" in self.metrics_list:\n            col_stats[\"apy\"] = self.apy(era_corrs=per_era_numerai_corrs)\n        col_stats[\"calmar_ratio\"] = (\n            np.nan\n            if col_stats[\"max_drawdown\"] == 0\n            else col_stats[\"apy\"] / -col_stats[\"max_drawdown\"]\n        )\n\n    if \"autocorrelation\" in self.metrics_list:\n        if self.show_detailed_progress_bar:\n            pbar.set_description(f\"autocorrelation for evaluation\")\n            pbar.update(1)\n        col_stats[\"autocorrelation\"] = self.autocorr1(per_era_numerai_corrs)\n\n    if \"max_feature_exposure\" in self.metrics_list:\n        if self.show_detailed_progress_bar:\n            pbar.set_description_str(f\"max_feature_exposure for evaluation\")\n            pbar.update(1)\n        col_stats[\"max_feature_exposure\"] = self.max_feature_exposure(\n            dataf=dataf, feature_cols=feature_cols, pred_col=pred_col\n        )\n\n    if \"smart_sharpe\" in self.metrics_list:\n        if self.show_detailed_progress_bar:\n            pbar.set_description_str(f\"smart_sharpe for evaluation\")\n            pbar.update(1)\n        col_stats[\"smart_sharpe\"] = self.smart_sharpe(\n            era_corrs=per_era_numerai_corrs\n        )\n\n    if benchmark_cols is not None:\n        for bench_col in benchmark_cols:\n            if self.show_detailed_progress_bar:\n                pbar.set_description_str(f\"Evaluation for benchmark column: '{bench_col}'\")\n                pbar.update(1)\n\n            per_era_bench_corrs = self.per_era_numerai_corrs(\n                dataf=dataf, pred_col=bench_col, target_col=target_col\n            )\n\n            if \"mean_std_sharpe\" in self.metrics_list:\n                if self.show_detailed_progress_bar:\n                    pbar.set_description_str(f\"mean_std_sharpe for benchmark column: '{bench_col}'\")\n                bench_mean, bench_std, bench_sharpe = self.mean_std_sharpe(\n                    era_corrs=per_era_bench_corrs\n                )\n                col_stats[f\"mean_vs_{bench_col}\"] = mean - bench_mean\n                col_stats[f\"std_vs_{bench_col}\"] = std - bench_std\n                col_stats[f\"sharpe_vs_{bench_col}\"] = sharpe - bench_sharpe\n\n            if \"mc_mean_std_sharpe\" in self.metrics_list:\n                if self.show_detailed_progress_bar:\n                    pbar.set_description_str(f\"mc_mean_std_sharpe for benchmark column: '{bench_col}'\")\n                mc_scores = self.contributive_correlation(\n                    dataf=dataf,\n                    pred_col=pred_col,\n                    target_col=target_col,\n                    other_col=bench_col,\n                )\n                col_stats[f\"mc_mean_{bench_col}\"] = np.nanmean(mc_scores)\n                col_stats[f\"mc_std_{bench_col}\"] = np.nanstd(mc_scores)\n                col_stats[f\"mc_sharpe_{bench_col}\"] = (\n                    np.nan\n                    if col_stats[f\"mc_std_{bench_col}\"] == 0\n                    else col_stats[f\"mc_mean_{bench_col}\"]\n                    / col_stats[f\"mc_std_{bench_col}\"]\n                )\n\n            if \"corr_with\" in self.metrics_list:\n                if self.show_detailed_progress_bar:\n                    pbar.set_description_str(f\"corr_with for benchmark column: '{bench_col}'\")\n                col_stats[f\"corr_with_{bench_col}\"] = self.cross_correlation(\n                    dataf=dataf, pred_col=bench_col, other_col=bench_col\n                )\n\n            if \"legacy_mc_mean_std_sharpe\" in self.metrics_list:\n                if self.show_detailed_progress_bar:\n                    pbar.set_description_str(f\"legacy_mc_mean_std_sharpe for benchmark column: '{bench_col}'\")\n                legacy_mc_scores = self.legacy_contribution(\n                    dataf=dataf,\n                    pred_col=pred_col,\n                    target_col=target_col,\n                    other_col=bench_col,\n                )\n                col_stats[f\"legacy_mc_mean_{bench_col}\"] = np.nanmean(\n                    legacy_mc_scores\n                )\n                col_stats[f\"legacy_mc_std_{bench_col}\"] = np.nanstd(\n                    legacy_mc_scores\n                )\n                col_stats[f\"legacy_mc_sharpe_{bench_col}\"] = (\n                    np.nan\n                    if col_stats[f\"legacy_mc_std_{bench_col}\"] == 0\n                    else col_stats[f\"legacy_mc_mean_{bench_col}\"]\n                    / col_stats[f\"legacy_mc_std_{bench_col}\"]\n                )\n\n            if \"ex_diss\" in self.metrics_list or \"ex_diss_pearson\" in self.metrics_list:\n                if self.show_detailed_progress_bar:\n                    pbar.set_description_str(f\"ex_diss_pearson for benchmark column: '{bench_col}'\")\n                col_stats[\n                    f\"exposure_dissimilarity_pearson_{bench_col}\"\n                ] = self.exposure_dissimilarity(\n                    dataf=dataf, pred_col=pred_col, other_col=bench_col,\n                    corr_method=\"pearson\"\n                )\n            if \"ex_diss_spearman\" in self.metrics_list:\n                if self.show_detailed_progress_bar:\n                    pbar.set_description_str(f\"ex_diss_spearman for benchmark column: '{bench_col}'\")\n                col_stats[\n                    f\"exposure_dissimilarity_spearman_{bench_col}\"\n                ] = self.exposure_dissimilarity(\n                    dataf=dataf, pred_col=pred_col, other_col=bench_col,\n                    corr_method=\"spearman\"\n                )\n\n    # Compute intensive stats\n    if \"fn_mean_std_sharpe\" in self.metrics_list:\n        if self.show_detailed_progress_bar:\n            pbar.set_description_str(f\"fn_mean_std_sharpe for evaluation\")\n            pbar.update(1)\n        fn_mean, fn_std, fn_sharpe = self.feature_neutral_mean_std_sharpe(\n            dataf=dataf,\n            pred_col=pred_col,\n            target_col=target_col,\n            feature_names=feature_cols,\n        )\n        col_stats[\"feature_neutral_mean\"] = fn_mean\n        col_stats[\"feature_neutral_std\"] = fn_std\n        col_stats[\"feature_neutral_sharpe\"] = fn_sharpe\n\n    if \"tb200_mean_std_sharpe\" in self.metrics_list:\n        if self.show_detailed_progress_bar:\n            pbar.set_description_str(f\"tb200_mean_std_sharpe for evaluation\")\n            pbar.update(1)\n        tb200_mean, tb200_std, tb200_sharpe = self.tbx_mean_std_sharpe(\n            dataf=dataf, pred_col=pred_col, target_col=target_col, tb=200\n        )\n        col_stats[\"tb200_mean\"] = tb200_mean\n        col_stats[\"tb200_std\"] = tb200_std\n        col_stats[\"tb200_sharpe\"] = tb200_sharpe\n\n    if \"tb500_mean_std_sharpe\" in self.metrics_list:\n        if self.show_detailed_progress_bar:\n            pbar.set_description_str(f\"tb500_mean_std_sharpe for evaluation\")\n            pbar.update(1)\n        tb500_mean, tb500_std, tb500_sharpe = self.tbx_mean_std_sharpe(\n            dataf=dataf, pred_col=pred_col, target_col=target_col, tb=500\n        )\n        col_stats[\"tb500_mean\"] = tb500_mean\n        col_stats[\"tb500_std\"] = tb500_std\n        col_stats[\"tb500_sharpe\"] = tb500_sharpe\n\n    # Custom functions\n    if self.custom_functions is not None:\n        local_vars = locals()\n        for func_name, func_info in self.custom_functions.items():\n            if self.show_detailed_progress_bar:\n                pbar.set_description_str(f\"custom function: '{func_name}' for evaluation\")\n                pbar.update(1)\n            func = func_info['func']\n            args = func_info['args']\n            local_args = func_info['local_args']\n            resolved_args = {}\n            for k, v in args.items():\n                # Resolve variables defined as local args\n                if isinstance(v, str) and v in local_args:\n                    if v not in local_vars:\n                        raise ValueError(f\"Variable '{v}' was defined in 'local_args', but was not found in local variables. Make sure to set the correct local_args.\")\n                    else:\n                        resolved_args[k] = local_vars[v]\n                else:\n                    resolved_args[k] = v\n            col_stats[func_name] = func(**resolved_args)\n\n    col_stats_df = pd.DataFrame(col_stats, index=[pred_col])\n    if self.show_detailed_progress_bar:\n        pbar.update(1)\n        pbar.close()\n    return col_stats_df\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.exposure_dissimilarity","title":"<code>exposure_dissimilarity(dataf, pred_col, other_col, corr_method='pearson')</code>","text":"<p>Model pattern of feature exposure to the another column. See TC details forum post: https://forum.numer.ai/t/true-contribution-details/5128/4 :param dataf: DataFrame containing both pred_col and other_col. :param pred_col: Main Prediction. :param other_col: Other prediction column to calculate exposure dissimilarity against. :param corr_method: Correlation method to use for calculating feature exposures. corr_method should be one of ['pearson', 'kendall', 'spearman']. Default: 'pearson'.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def exposure_dissimilarity(\n    self, dataf: pd.DataFrame, pred_col: str, other_col: str, corr_method: str = \"pearson\"\n) -&gt; np.float32:\n    \"\"\"\n    Model pattern of feature exposure to the another column.\n    See TC details forum post: https://forum.numer.ai/t/true-contribution-details/5128/4\n    :param dataf: DataFrame containing both pred_col and other_col.\n    :param pred_col: Main Prediction.\n    :param other_col: Other prediction column to calculate exposure dissimilarity against.\n    :param corr_method: Correlation method to use for calculating feature exposures.\n    corr_method should be one of ['pearson', 'kendall', 'spearman']. Default: 'pearson'.\n    \"\"\"\n    assert corr_method in [\"pearson\", \"kendall\", \"spearman\"], f\"corr_method should be one of ['pearson', 'kendall', 'spearman']. Got: '{corr_method}'\"\n    feature_cols = [col for col in dataf.columns if col.startswith(\"feature\")]\n    U = dataf[feature_cols].corrwith(dataf[pred_col], method=corr_method).values\n    E = dataf[feature_cols].corrwith(dataf[other_col], method=corr_method).values\n\n    denominator = np.dot(E, E)\n    if denominator == 0:\n        exp_dis = 0\n    else:\n        exp_dis = 1 - np.dot(U, E) / denominator\n    return exp_dis\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.feature_neutral_mean_std_sharpe","title":"<code>feature_neutral_mean_std_sharpe(dataf, pred_col, target_col, feature_names)</code>","text":"<p>Feature neutralized mean performance. More info: https://docs.numer.ai/tournament/feature-neutral-correlation</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def feature_neutral_mean_std_sharpe(\n    self, dataf: pd.DataFrame, pred_col: str, target_col: str, feature_names: list\n) -&gt; Tuple[np.float64, np.float64, np.float64]:\n    \"\"\"\n    Feature neutralized mean performance.\n    More info: https://docs.numer.ai/tournament/feature-neutral-correlation\n    \"\"\"\n    fn = FeatureNeutralizer(pred_name=pred_col, proportion=1.0)\n    neutralized_preds = fn.predict(\n        dataf[pred_col], features=dataf[feature_names], eras=dataf[self.era_col]\n    )\n    # Construct new DataFrame with era col, target col and preds\n    neutralized_dataf = pd.DataFrame(columns=[self.era_col, target_col, pred_col])\n    neutralized_dataf[self.era_col] = dataf[self.era_col]\n    neutralized_dataf[target_col] = dataf[target_col]\n    neutralized_dataf[pred_col] = neutralized_preds\n\n    neutral_corrs = self.per_era_numerai_corrs(\n        dataf=neutralized_dataf,\n        pred_col=pred_col,\n        target_col=target_col,\n    )\n    mean, std, sharpe = self.mean_std_sharpe(era_corrs=neutral_corrs)\n    return mean, std, sharpe\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.full_evaluation","title":"<code>full_evaluation(dataf, pred_cols, target_col='target', benchmark_cols=None)</code>","text":"<p>Perform evaluation for each prediction column in pred_cols. By default only the \"prediction\" column is evaluated. Evaluation is done against given target and benchmark prediction column. :param dataf: DataFrame containing era_col, pred_cols, target_col and optional benchmark_cols. :param pred_cols: List of prediction columns to calculate evaluation metrics for. :param target_col: Target column to evaluate against. :param benchmark_cols: Optional list of benchmark columns to calculate evaluation metrics for.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def full_evaluation(\n    self,\n    dataf: pd.DataFrame,\n    pred_cols: List[str],\n    target_col: str = \"target\",\n    benchmark_cols: list = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Perform evaluation for each prediction column in pred_cols.\n    By default only the \"prediction\" column is evaluated.\n    Evaluation is done against given target and benchmark prediction column.\n    :param dataf: DataFrame containing era_col, pred_cols, target_col and optional benchmark_cols.\n    :param pred_cols: List of prediction columns to calculate evaluation metrics for.\n    :param target_col: Target column to evaluate against.\n    :param benchmark_cols: Optional list of benchmark columns to calculate evaluation metrics for.\n    \"\"\"\n    val_stats = pd.DataFrame()\n    feature_cols = [col for col in dataf.columns if col.startswith(\"feature\")]\n    cat_cols = (\n        dataf[feature_cols].select_dtypes(include=[\"category\"]).columns.to_list()\n    )\n    if cat_cols:\n        print(\n            f\"WARNING: Categorical features detected that cannot be used for neutralization. Removing columns: '{cat_cols}' for evaluation.\"\n        )\n        dataf.loc[:, feature_cols] = dataf[feature_cols].select_dtypes(\n            exclude=[\"category\"]\n        )\n    dataf = dataf.fillna(0.5)\n    for col in tqdm(pred_cols, desc=\"Evaluation: \"):\n        col_stats = self.evaluation_one_col(\n            dataf=dataf,\n            pred_col=col,\n            feature_cols=feature_cols,\n            target_col=target_col,\n            benchmark_cols=benchmark_cols,\n        )\n        val_stats = pd.concat([val_stats, col_stats], axis=0)\n    return val_stats\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.get_feature_exposures_corrv2","title":"<code>get_feature_exposures_corrv2(dataf, pred_col, feature_list, cpu_cores=-1)</code>","text":"<p>Calculate feature exposures for each era using 'Numerai Corr'. Results will be similar to get_feature_exposures() but more accurate. This method will take longer to compute.</p> <p>:param dataf: DataFrame containing predictions, features, and eras. :param pred_col: Prediction column to calculate feature exposures for. :param feature_list: List of feature columns in X. :param cpu_cores: Number of CPU cores to use for parallelization. Default: -1 (all cores). :return: DataFrame with Corrv2 feature exposures by era for each feature.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def get_feature_exposures_corrv2(\n    self,\n    dataf: pd.DataFrame,\n    pred_col: str,\n    feature_list: List[str],\n    cpu_cores: int = -1,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate feature exposures for each era using 'Numerai Corr'.\n    Results will be similar to get_feature_exposures() but more accurate.\n    This method will take longer to compute.\n\n    :param dataf: DataFrame containing predictions, features, and eras.\n    :param pred_col: Prediction column to calculate feature exposures for.\n    :param feature_list: List of feature columns in X.\n    :param cpu_cores: Number of CPU cores to use for parallelization.\n    Default: -1 (all cores).\n    :return: DataFrame with Corrv2 feature exposures by era for each feature.\n    \"\"\"\n\n    def calculate_era_feature_exposure(era, group, pred_col, feature_list):\n        exposures = {}\n        for feature in feature_list:\n            corr = self.numerai_corr(\n                group, pred_col=f\"{pred_col}_normalized\", target_col=feature\n            )\n            exposures[feature] = corr\n        return era, exposures\n\n    normalized_ranks = (dataf[[pred_col]].rank(method=\"first\") - 0.5) / len(dataf)\n    dataf[f\"{pred_col}_normalized\"] = stats.norm.ppf(normalized_ranks)\n    feature_exposure_data = pd.DataFrame(\n        index=dataf[\"era\"].unique(), columns=feature_list\n    )\n\n    grouped_data = list(dataf.groupby(\"era\"))\n\n    results = Parallel(n_jobs=cpu_cores)(\n        delayed(calculate_era_feature_exposure)(era, group, pred_col, feature_list)\n        for era, group in grouped_data\n    )\n    for era, exposures in results:\n        feature_exposure_data.loc[era, :] = exposures\n    return feature_exposure_data\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.get_feature_exposures_pearson","title":"<code>get_feature_exposures_pearson(dataf, pred_col, feature_list, cpu_cores=-1)</code>","text":"<p>Calculate feature exposures for each era using Pearson correlation.</p> <p>:param dataf: DataFrame containing predictions, features, and eras. :param pred_col: Prediction column to calculate feature exposures for. :param feature_list: List of feature columns in X. :param cpu_cores: Number of CPU cores to use for parallelization. :return: DataFrame with Pearson feature exposures by era for each feature.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def get_feature_exposures_pearson(\n    self,\n    dataf: pd.DataFrame,\n    pred_col: str,\n    feature_list: List[str],\n    cpu_cores: int = -1,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate feature exposures for each era using Pearson correlation.\n\n    :param dataf: DataFrame containing predictions, features, and eras.\n    :param pred_col: Prediction column to calculate feature exposures for.\n    :param feature_list: List of feature columns in X.\n    :param cpu_cores: Number of CPU cores to use for parallelization.\n    :return: DataFrame with Pearson feature exposures by era for each feature.\n    \"\"\"\n\n    def calculate_era_pearson_exposure(\n        era, group, feature_list, pred_col_normalized\n    ):\n        data_matrix = group[feature_list + [pred_col_normalized]].values\n        correlations = np.corrcoef(data_matrix, rowvar=False)\n\n        # Get the correlations of all features with the predictions (which is the last column)\n        feature_correlations = correlations[:-1, -1]\n        return era, feature_correlations\n\n    normalized_ranks = (dataf[[pred_col]].rank(method=\"first\") - 0.5) / len(dataf)\n    dataf[f\"{pred_col}_normalized\"] = stats.norm.ppf(normalized_ranks)\n    feature_exposure_data = pd.DataFrame(\n        index=dataf[\"era\"].unique(), columns=feature_list\n    )\n\n    grouped_data = list(dataf.groupby(\"era\"))\n\n    results = Parallel(n_jobs=cpu_cores)(\n        delayed(calculate_era_pearson_exposure)(\n            era, group, feature_list, f\"{pred_col}_normalized\"\n        )\n        for era, group in grouped_data\n    )\n\n    for era, feature_correlations in results:\n        feature_exposure_data.loc[era, :] = feature_correlations\n    return feature_exposure_data\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.legacy_contribution","title":"<code>legacy_contribution(dataf, pred_col, target_col, other_col)</code>","text":"<p>Legacy contibution mean, standard deviation and sharpe ratio. More info: https://forum.numer.ai/t/mmc2-announcement/93</p> <p>:param dataf: DataFrame containing era_col, pred_col, target_col and other_col. :param pred_col: Prediction column to calculate MMC for. :param target_col: Target column to calculate MMC against. :param other_col: Meta model column containing predictions to neutralize against.</p> <p>:return: List of legacy contribution scores by era.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def legacy_contribution(\n    self, dataf: pd.DataFrame, pred_col: str, target_col: str, other_col: str\n):\n    \"\"\"\n    Legacy contibution mean, standard deviation and sharpe ratio.\n    More info: https://forum.numer.ai/t/mmc2-announcement/93\n\n    :param dataf: DataFrame containing era_col, pred_col, target_col and other_col.\n    :param pred_col: Prediction column to calculate MMC for.\n    :param target_col: Target column to calculate MMC against.\n    :param other_col: Meta model column containing predictions to neutralize against.\n\n    :return: List of legacy contribution scores by era.\n    \"\"\"\n    legacy_mc_scores = []\n    # Standard deviation of a uniform distribution\n    COVARIANCE_FACTOR = 0.29**2\n    # Calculate MMC for each era\n    for _, x in dataf.groupby(self.era_col):\n        series = self._neutralize_series(\n            self._normalize_uniform(x[pred_col]), (x[other_col])\n        )\n        legacy_mc_scores.append(\n            np.cov(series, x[target_col])[0, 1] / COVARIANCE_FACTOR\n        )\n\n    return legacy_mc_scores\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.max_drawdown","title":"<code>max_drawdown(era_corrs)</code>  <code>staticmethod</code>","text":"<p>Maximum drawdown per era.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>@staticmethod\ndef max_drawdown(era_corrs: pd.Series) -&gt; np.float64:\n    \"\"\"Maximum drawdown per era.\"\"\"\n    # Arbitrarily large window\n    rolling_max = (\n        (era_corrs + 1).cumprod().rolling(window=9000, min_periods=1).max()\n    )\n    daily_value = (era_corrs + 1).cumprod()\n    max_drawdown = -((rolling_max - daily_value) / rolling_max).max()\n    return max_drawdown\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.max_feature_exposure","title":"<code>max_feature_exposure(dataf, feature_cols, pred_col)</code>","text":"<p>Maximum exposure over all features.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def max_feature_exposure(\n    self, dataf: pd.DataFrame, feature_cols: List[str], pred_col: str\n) -&gt; np.float64:\n    \"\"\"Maximum exposure over all features.\"\"\"\n    max_per_era = dataf.groupby(self.era_col).apply(\n        lambda d: d[feature_cols].corrwith(d[pred_col]).abs().max()\n    )\n    max_feature_exposure = max_per_era.mean(skipna=True)\n    return max_feature_exposure\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.mean_std_sharpe","title":"<code>mean_std_sharpe(era_corrs)</code>","text":"<p>Average, standard deviation and Sharpe ratio for correlations per era.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def mean_std_sharpe(\n    self, era_corrs: pd.Series\n) -&gt; Tuple[np.float64, np.float64, np.float64]:\n    \"\"\"\n    Average, standard deviation and Sharpe ratio for\n    correlations per era.\n    \"\"\"\n    mean = pd.Series(era_corrs.mean()).item()\n    std = pd.Series(era_corrs.std(ddof=0)).item()\n    sharpe = np.nan if std == 0 else mean / std\n    return mean, std, sharpe\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.numerai_corr","title":"<code>numerai_corr(dataf, pred_col, target_col)</code>","text":"<p>Computes 'Numerai Corr' aka 'Corrv2'. More info: https://forum.numer.ai/t/target-cyrus-new-primary-target/6303</p> <p>Assumes original target col as input (i.e. in [0, 1] range).</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def numerai_corr(\n    self, dataf: pd.DataFrame, pred_col: str, target_col: str\n) -&gt; np.float64:\n    \"\"\"\n    Computes 'Numerai Corr' aka 'Corrv2'.\n    More info: https://forum.numer.ai/t/target-cyrus-new-primary-target/6303\n\n    Assumes original target col as input (i.e. in [0, 1] range).\n    \"\"\"\n    # Rank and gaussianize predictions\n    ranked_preds = self._normalize_uniform(\n        dataf[pred_col].fillna(0.5), method=\"average\"\n    )\n    gauss_ranked_preds = stats.norm.ppf(ranked_preds)\n    # Center target from [0...1] to [-0.5...0.5] range\n    targets = dataf[target_col]\n    centered_target = targets - targets.mean()\n    # Accentuate tails of predictions and targets\n    preds_p15 = np.sign(gauss_ranked_preds) * np.abs(gauss_ranked_preds) ** 1.5\n    target_p15 = np.sign(centered_target) * np.abs(centered_target) ** 1.5\n    # Pearson correlation\n    corr, _ = stats.pearsonr(preds_p15, target_p15)\n    return corr\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.per_era_corrs","title":"<code>per_era_corrs(dataf, pred_col, target_col)</code>","text":"<p>Correlation between prediction and target for each era.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def per_era_corrs(\n    self, dataf: pd.DataFrame, pred_col: str, target_col: str\n) -&gt; pd.Series:\n    \"\"\"Correlation between prediction and target for each era.\"\"\"\n    return dataf.groupby(self.era_col).apply(\n        lambda d: self._normalize_uniform(d[pred_col].fillna(0.5)).corr(\n            d[target_col]\n        )\n    )\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.per_era_numerai_corrs","title":"<code>per_era_numerai_corrs(dataf, pred_col, target_col)</code>","text":"<p>Numerai Corr between prediction and target for each era.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def per_era_numerai_corrs(\n    self, dataf: pd.DataFrame, pred_col: str, target_col: str\n) -&gt; pd.Series:\n    \"\"\"Numerai Corr between prediction and target for each era.\"\"\"\n    return dataf.groupby(self.era_col).apply(\n        lambda d: self.numerai_corr(d.fillna(0.5), pred_col, target_col)\n    )\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.plot_correlations","title":"<code>plot_correlations(dataf, pred_cols, corr_cols=None, target_col='target', roll_mean=20)</code>","text":"<p>Plot per era correlations over time. :param dataf: DataFrame that contains at least all pred_cols, target_col and corr_cols. :param pred_cols: List of prediction columns to calculate per era correlations for and plot. :param corr_cols: Per era correlations already prepared to include in the plot. This is optional for if you already have per era correlations prepared in your input dataf. :param target_col: Target column name to compute per era correlations against. :param roll_mean: How many eras should be averaged to compute a rolling score.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def plot_correlations(\n    self,\n    dataf: pd.DataFrame,\n    pred_cols: List[str],\n    corr_cols: list = None,\n    target_col: str = \"target\",\n    roll_mean: int = 20,\n):\n    \"\"\"\n    Plot per era correlations over time.\n    :param dataf: DataFrame that contains at least all pred_cols, target_col and corr_cols.\n    :param pred_cols: List of prediction columns to calculate per era correlations for and plot.\n    :param corr_cols: Per era correlations already prepared to include in the plot.\n    This is optional for if you already have per era correlations prepared in your input dataf.\n    :param target_col: Target column name to compute per era correlations against.\n    :param roll_mean: How many eras should be averaged to compute a rolling score.\n    \"\"\"\n    validation_by_eras = pd.DataFrame()\n    # Compute per era correlation for each prediction column.\n    for pred_col in pred_cols:\n        per_era_corrs = self.per_era_numerai_corrs(\n            dataf, pred_col=pred_col, target_col=target_col\n        )\n        validation_by_eras.loc[:, pred_col] = per_era_corrs\n\n    # Add prepared per era correlation if any.\n    if corr_cols is not None:\n        for corr_col in corr_cols:\n            validation_by_eras.loc[:, corr_col] = dataf[corr_col]\n\n    validation_by_eras.rolling(roll_mean).mean().plot(\n        kind=\"line\",\n        marker=\"o\",\n        ms=4,\n        title=f\"Rolling Per Era Correlation Mean (rolling window size: {roll_mean})\",\n        figsize=(15, 5),\n    )\n    plt.legend(\n        loc=\"upper center\",\n        bbox_to_anchor=(0.5, -0.05),\n        fancybox=True,\n        shadow=True,\n        ncol=1,\n    )\n    plt.axhline(y=0.0, color=\"r\", linestyle=\"--\")\n    plt.show()\n\n    validation_by_eras.cumsum().plot(\n        title=\"Cumulative Sum of Era Correlations\", figsize=(15, 5)\n    )\n    plt.legend(\n        loc=\"upper center\",\n        bbox_to_anchor=(0.5, -0.05),\n        fancybox=True,\n        shadow=True,\n        ncol=1,\n    )\n    plt.axhline(y=0.0, color=\"r\", linestyle=\"--\")\n    plt.show()\n    return\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.smart_sharpe","title":"<code>smart_sharpe(era_corrs)</code>","text":"<p>Sharpe adjusted for autocorrelation. :param era_corrs: Correlation scores by era</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def smart_sharpe(self, era_corrs: pd.Series) -&gt; np.float64:\n    \"\"\"\n    Sharpe adjusted for autocorrelation.\n    :param era_corrs: Correlation scores by era\n    \"\"\"\n    return np.nanmean(era_corrs) / (\n        np.nanstd(era_corrs, ddof=1) * self.autocorr_penalty(era_corrs)\n    )\n</code></pre>"},{"location":"api/#numerblox.evaluation.BaseEvaluator.tbx_mean_std_sharpe","title":"<code>tbx_mean_std_sharpe(dataf, pred_col, target_col, tb=200)</code>","text":"<p>Calculate Mean, Standard deviation and Sharpe ratio when we focus on the x top and x bottom predictions. :param tb: How many of top and bottom predictions to focus on. TB200 and TB500 are the most common situations.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def tbx_mean_std_sharpe(\n    self, dataf: pd.DataFrame, pred_col: str, target_col: str, tb: int = 200\n) -&gt; Tuple[np.float64, np.float64, np.float64]:\n    \"\"\"\n    Calculate Mean, Standard deviation and Sharpe ratio\n    when we focus on the x top and x bottom predictions.\n    :param tb: How many of top and bottom predictions to focus on.\n    TB200 and TB500 are the most common situations.\n    \"\"\"\n    tb_val_corrs = self._score_by_date(\n        dataf=dataf, columns=[pred_col], target=target_col, tb=tb\n    )\n    return self.mean_std_sharpe(era_corrs=tb_val_corrs)\n</code></pre>"},{"location":"api/#numerblox.evaluation.NumeraiClassicEvaluator","title":"<code>NumeraiClassicEvaluator</code>","text":"<p>             Bases: <code>BaseEvaluator</code></p> <p>Evaluator for all metrics that are relevant in Numerai Classic.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>class NumeraiClassicEvaluator(BaseEvaluator):\n    \"\"\"\n    Evaluator for all metrics that are relevant in Numerai Classic.\n    \"\"\"\n\n    def __init__(\n        self,\n        era_col: str = \"era\",\n        metrics_list: List[str] = FAST_METRICS,\n        custom_functions: Dict[str, Dict[str, Any]] = None,\n        show_detailed_progress_bar: bool = True,\n    ):\n        for metric in metrics_list:\n            assert (\n                metric in ALL_CLASSIC_METRICS\n            ), f\"Metric '{metric}' not found. Valid metrics: {ALL_CLASSIC_METRICS}.\"\n        super().__init__(\n            era_col=era_col, metrics_list=metrics_list, custom_functions=custom_functions,\n            show_detailed_progress_bar=show_detailed_progress_bar\n        )\n        self.fncv3_features = FNCV3_FEATURES\n\n    def full_evaluation(\n        self,\n        dataf: pd.DataFrame,\n        pred_cols: List[str],\n        target_col: str = \"target\",\n        benchmark_cols: list = None,\n    ) -&gt; pd.DataFrame:\n        val_stats = pd.DataFrame()\n        dataf = dataf.fillna(0.5)\n        feature_cols = [col for col in dataf.columns if col.startswith(\"feature\")]\n\n        # Check if sufficient columns are present in dataf to compute FNC\n        feature_set = set(dataf.columns)\n        if set(self.fncv3_features).issubset(feature_set):\n            print(\n                \"Using 'v4.2/features.json/fncv3_features' feature set to calculate FNC metrics.\"\n            )\n            valid_features = self.fncv3_features\n        else:\n            print(\n                \"WARNING: No suitable feature set defined for FNC. Skipping calculation of FNC.\"\n            )\n            valid_features = []\n\n        with tqdm(pred_cols, desc=\"Evaluation\") as pbar:\n            for col in pbar:\n                # Metrics that can be calculated for both Numerai Classic and Signals\n                col_stats = self.evaluation_one_col(\n                    dataf=dataf,\n                    feature_cols=feature_cols,\n                    pred_col=col,\n                    target_col=target_col,\n                    benchmark_cols=benchmark_cols,\n                )\n                # Numerai Classic specific metrics\n                if valid_features and \"fncv3_mean_std_sharpe\" in self.metrics_list:\n                    pbar.set_description_str(f\"fncv3_mean_std_sharpe for evaluation of '{col}'\")\n                    # Using only valid features defined in FNCV3_FEATURES\n                    fnc_v3, fn_std_v3, fn_sharpe_v3 = self.feature_neutral_mean_std_sharpe(\n                        dataf=dataf,\n                        pred_col=col,\n                        target_col=target_col,\n                        feature_names=valid_features,\n                    )\n                    col_stats.loc[col, \"feature_neutral_mean_v3\"] = fnc_v3\n                    col_stats.loc[col, \"feature_neutral_std_v3\"] = fn_std_v3\n                    col_stats.loc[col, \"feature_neutral_sharpe_v3\"] = fn_sharpe_v3\n\n                val_stats = pd.concat([val_stats, col_stats], axis=0)\n        return val_stats\n</code></pre>"},{"location":"api/#numerblox.evaluation.NumeraiSignalsEvaluator","title":"<code>NumeraiSignalsEvaluator</code>","text":"<p>             Bases: <code>BaseEvaluator</code></p> <p>Evaluator for all metrics that are relevant in Numerai Signals.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>class NumeraiSignalsEvaluator(BaseEvaluator):\n    \"\"\"Evaluator for all metrics that are relevant in Numerai Signals.\"\"\"\n    # Columns retrievable from Numerai Signals diagnostics.\n    # More info: https://forum.numer.ai/t/signals-diagnostics-guide/5950\n    VALID_DIAGNOSTICS_COLS = [\"validationCorrV4\", \"validationFncV4\", \"validationIcV2\", \"validationRic\"]\n\n    def __init__(\n        self,\n        era_col: str = \"friday_date\",\n        metrics_list: List[str] = FAST_METRICS,\n        custom_functions: Dict[str, Dict[str, Any]] = None,\n        show_detailed_progress_bar: bool = True,\n    ):\n        for metric in metrics_list:\n            assert (\n                metric in ALL_SIGNALS_METRICS\n            ), f\"Metric '{metric}' not found. Valid metrics: {ALL_SIGNALS_METRICS}.\"\n        super().__init__(\n            era_col=era_col, metrics_list=metrics_list, custom_functions=custom_functions,\n            show_detailed_progress_bar=show_detailed_progress_bar\n        )\n\n    def get_diagnostics(\n        self, val_dataf: pd.DataFrame, model_name: str, key: Key, timeout_min: int = 2,\n        col: Union[str, None] = \"validationFncV4\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Retrieved neutralized validation correlation by era. \\n\n        Calculated on Numerai servers. \\n\n        :param val_dataf: A DataFrame containing prediction, friday_date, ticker and data_type columns. \\n\n        data_type column should contain 'validation' instances. \\n\n        :param model_name: Any model name for which you have authentication credentials. \\n\n        :param key: Key object to authenticate upload of diagnostics. \\n\n        :param timeout_min: How many minutes to wait on diagnostics Computing on Numerai servers before timing out. \\n\n        :param col: Which column to return. Should be one of ['validationCorrV4', 'validationFncV4', 'validationIcV2', 'validationRic']. If None, all columns will be returned. \\n\n        2 minutes by default. \\n\n        :return: Pandas Series with era as index and neutralized validation correlations (validationCorr).\n        \"\"\"\n        assert col in self.VALID_DIAGNOSTICS_COLS or col is None, f\"corr_col should be one of {self.VALID_DIAGNOSTICS_COLS} or None. Got: '{col}'\"\n        api = SignalsAPI(public_id=key.pub_id, secret_key=key.secret_key)\n        model_id = api.get_models()[model_name]\n        diagnostics_id = api.upload_diagnostics(df=val_dataf, model_id=model_id)\n        data = self.__await_diagnostics(\n            api=api,\n            model_id=model_id,\n            diagnostics_id=diagnostics_id,\n            timeout_min=timeout_min,\n        )\n        diagnostics_df = pd.DataFrame(data[\"perEraDiagnostics\"]).set_index(\"era\")\n        diagnostics_df.index = pd.to_datetime(diagnostics_df.index)\n        return_cols = [col] if col is not None else self.VALID_DIAGNOSTICS_COLS\n        return diagnostics_df[return_cols]\n\n    @staticmethod\n    def __await_diagnostics(\n        api: SignalsAPI,\n        model_id: str,\n        diagnostics_id: str,\n        timeout_min: int,\n        interval_sec: int = 15,\n    ):\n        \"\"\"\n        Wait for diagnostics to be uploaded.\n        Try every 'interval_sec' seconds until 'timeout_min' minutes have passed.\n        \"\"\"\n        timeout = time.time() + 60 * timeout_min\n        data = {\"status\": \"not_done\"}\n        while time.time() &lt; timeout:\n            data = api.diagnostics(model_id=model_id, diagnostics_id=diagnostics_id)[0]\n            if not data[\"status\"] == \"done\":\n                print(\n                    f\"Diagnostics not processed yet. Sleeping for another {interval_sec} seconds.\"\n                )\n                time.sleep(interval_sec)\n            else:\n                break\n        if not data[\"status\"] == \"done\":\n            raise Exception(\n                f\"Diagnostics couldn't be retrieved within {timeout_min} minutes after uploading. Check if Numerai API is offline.\"\n            )\n        return data\n</code></pre>"},{"location":"api/#numerblox.evaluation.NumeraiSignalsEvaluator.__await_diagnostics","title":"<code>__await_diagnostics(api, model_id, diagnostics_id, timeout_min, interval_sec=15)</code>  <code>staticmethod</code>","text":"<p>Wait for diagnostics to be uploaded. Try every 'interval_sec' seconds until 'timeout_min' minutes have passed.</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>@staticmethod\ndef __await_diagnostics(\n    api: SignalsAPI,\n    model_id: str,\n    diagnostics_id: str,\n    timeout_min: int,\n    interval_sec: int = 15,\n):\n    \"\"\"\n    Wait for diagnostics to be uploaded.\n    Try every 'interval_sec' seconds until 'timeout_min' minutes have passed.\n    \"\"\"\n    timeout = time.time() + 60 * timeout_min\n    data = {\"status\": \"not_done\"}\n    while time.time() &lt; timeout:\n        data = api.diagnostics(model_id=model_id, diagnostics_id=diagnostics_id)[0]\n        if not data[\"status\"] == \"done\":\n            print(\n                f\"Diagnostics not processed yet. Sleeping for another {interval_sec} seconds.\"\n            )\n            time.sleep(interval_sec)\n        else:\n            break\n    if not data[\"status\"] == \"done\":\n        raise Exception(\n            f\"Diagnostics couldn't be retrieved within {timeout_min} minutes after uploading. Check if Numerai API is offline.\"\n        )\n    return data\n</code></pre>"},{"location":"api/#numerblox.evaluation.NumeraiSignalsEvaluator.get_diagnostics","title":"<code>get_diagnostics(val_dataf, model_name, key, timeout_min=2, col='validationFncV4')</code>","text":"<p>Retrieved neutralized validation correlation by era. </p> <p>Calculated on Numerai servers. </p> <p>:param val_dataf: A DataFrame containing prediction, friday_date, ticker and data_type columns. </p> <p>data_type column should contain 'validation' instances. </p> <p>:param model_name: Any model name for which you have authentication credentials. </p> <p>:param key: Key object to authenticate upload of diagnostics. </p> <p>:param timeout_min: How many minutes to wait on diagnostics Computing on Numerai servers before timing out. </p> <p>:param col: Which column to return. Should be one of ['validationCorrV4', 'validationFncV4', 'validationIcV2', 'validationRic']. If None, all columns will be returned. </p> <p>2 minutes by default. </p> <p>:return: Pandas Series with era as index and neutralized validation correlations (validationCorr).</p> Source code in <code>numerblox/evaluation.py</code> <pre><code>def get_diagnostics(\n    self, val_dataf: pd.DataFrame, model_name: str, key: Key, timeout_min: int = 2,\n    col: Union[str, None] = \"validationFncV4\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Retrieved neutralized validation correlation by era. \\n\n    Calculated on Numerai servers. \\n\n    :param val_dataf: A DataFrame containing prediction, friday_date, ticker and data_type columns. \\n\n    data_type column should contain 'validation' instances. \\n\n    :param model_name: Any model name for which you have authentication credentials. \\n\n    :param key: Key object to authenticate upload of diagnostics. \\n\n    :param timeout_min: How many minutes to wait on diagnostics Computing on Numerai servers before timing out. \\n\n    :param col: Which column to return. Should be one of ['validationCorrV4', 'validationFncV4', 'validationIcV2', 'validationRic']. If None, all columns will be returned. \\n\n    2 minutes by default. \\n\n    :return: Pandas Series with era as index and neutralized validation correlations (validationCorr).\n    \"\"\"\n    assert col in self.VALID_DIAGNOSTICS_COLS or col is None, f\"corr_col should be one of {self.VALID_DIAGNOSTICS_COLS} or None. Got: '{col}'\"\n    api = SignalsAPI(public_id=key.pub_id, secret_key=key.secret_key)\n    model_id = api.get_models()[model_name]\n    diagnostics_id = api.upload_diagnostics(df=val_dataf, model_id=model_id)\n    data = self.__await_diagnostics(\n        api=api,\n        model_id=model_id,\n        diagnostics_id=diagnostics_id,\n        timeout_min=timeout_min,\n    )\n    diagnostics_df = pd.DataFrame(data[\"perEraDiagnostics\"]).set_index(\"era\")\n    diagnostics_df.index = pd.to_datetime(diagnostics_df.index)\n    return_cols = [col] if col is not None else self.VALID_DIAGNOSTICS_COLS\n    return diagnostics_df[return_cols]\n</code></pre>"},{"location":"api/#numerblox.submission.BaseSubmitter","title":"<code>BaseSubmitter</code>","text":"<p>             Bases: <code>BaseIO</code></p> <p>Basic functionality for submitting to Numerai.  Uses numerapi under the hood. More info: https://numerapi.readthedocs.io/ </p> <p>:param directory_path: Directory to store and read submissions from.  :param api: NumerAPI or SignalsAPI :param max_retries: Maximum number of retries for uploading predictions to Numerai.  :param sleep_time: Time to sleep between uploading retries. :param fail_silently: Whether to skip uploading to Numerai without raising an error.  Useful for if you are uploading many models in a loop and want to skip models that fail to upload.</p> Source code in <code>numerblox/submission.py</code> <pre><code>class BaseSubmitter(BaseIO):\n    \"\"\"\n    Basic functionality for submitting to Numerai. \n    Uses numerapi under the hood.\n    More info: https://numerapi.readthedocs.io/ \n\n    :param directory_path: Directory to store and read submissions from. \n    :param api: NumerAPI or SignalsAPI\n    :param max_retries: Maximum number of retries for uploading predictions to Numerai. \n    :param sleep_time: Time to sleep between uploading retries.\n    :param fail_silently: Whether to skip uploading to Numerai without raising an error. \n    Useful for if you are uploading many models in a loop and want to skip models that fail to upload.\n    \"\"\"\n    def __init__(self, directory_path: str, api: Union[NumerAPI, SignalsAPI], max_retries: int, \n                 sleep_time: int, fail_silently: bool):\n        super().__init__(directory_path)\n        self.api = api\n        self.max_retries = max_retries\n        self.sleep_time = sleep_time\n        self.fail_silently = fail_silently\n\n    @abstractmethod\n    def save_csv(\n        self,\n        dataf: pd.DataFrame,\n        file_name: str,\n        cols: Union[str, list],\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        For Numerai Classic: Save index column + 'cols' (targets) to CSV.\n        For Numerai Signals: Save ticker, friday_date, data_type and signal columns to CSV.\n        \"\"\"\n        ...\n\n    def upload_predictions(self, file_name: str, model_name: str, *args, **kwargs):\n        \"\"\"\n        Upload CSV file to Numerai for given model name.\n        :param file_name: File name/path relative to directory_path.\n        :param model_name: Lowercase raw model name (For example, 'integration_test').\n        \"\"\"\n        full_path = str(self.dir / file_name)\n        model_id = self._get_model_id(model_name=model_name)\n        api_type = str(self.api.__class__.__name__)\n        print(\n            f\"{api_type}: Uploading predictions from '{full_path}' for model '{model_name}' (model_id='{model_id}')\"\n        )\n        for attempt in range(self.max_retries):\n            try:\n                self.api.upload_predictions(\n                    file_path=full_path, model_id=model_id, *args, **kwargs\n                )\n                print(\n                    f\"{api_type} submission of '{full_path}' for '{model_name}' is successful!\"\n                )\n                return\n            except Exception as e:\n                if attempt &lt; self.max_retries - 1:  # i.e. not the last attempt\n                    print(f\"Failed to upload '{full_path}' for '{model_name}' to Numerai. Retrying in {self.sleep_time} seconds...\")\n                    print(f\"Error: {e}\")\n                    time.sleep(self.sleep_time)\n                else:\n                    if self.fail_silently:\n                        print(f\"Failed to upload'{full_path}' for '{model_name}' to Numerai. Skipping...\")\n                        print(f\"Error: {e}\")\n                    else:\n                        print(f\"Failed to upload '{full_path}' for '{model_name}' to Numerai after {self.max_retries} attempts.\")\n                        raise e\n\n    def full_submission(\n        self,\n        dataf: pd.DataFrame,\n        model_name: str,\n        cols: Union[str, list],\n        file_name: str = 'submission.csv',\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Save DataFrame to csv and upload predictions through API.\n\n        :param dataf: Main DataFrame containing `cols`.\n        :param model_name: Lowercase Numerai model name.\n        :param file_name: path to save model to relative to base directory.\n        :param cols: Columns to be saved in submission file.\n        1 prediction column for Numerai Classic.\n        At least 1 prediction column and 1 ticker column for Numerai Signals.\n        *args, **kwargs are passed to numerapi API.\n        For example `version` argument in Numerai Classic submissions.\n        \"\"\"\n        self.save_csv(dataf=dataf, file_name=file_name, cols=cols)\n        self.upload_predictions(\n            file_name=file_name, model_name=model_name,\n            *args, **kwargs\n        )\n\n    def combine_csvs(self, csv_paths: list,\n                     aux_cols: list,\n                     era_col: str = None,\n                     pred_col: str = 'prediction') -&gt; pd.DataFrame:\n        \"\"\"\n        Read in csv files and combine all predictions with a rank mean. \\n\n        Multi-target predictions will be averaged out. \\n\n        :param csv_paths: List of full paths to .csv prediction files. \\n\n        :param aux_cols: ['id'] for Numerai Classic. \\n\n        ['ticker', 'last_friday', 'data_type'], for example, with Numerai Signals. \\n\n        :param era_col: Column indicating era ('era' or 'last_friday'). \\n\n        Will be used for Grouping the rank mean if given. Skip groupby if no era_col provided. \\n\n        :param pred_col: 'prediction' for Numerai Classic and 'signal' for Numerai Signals.\n        \"\"\"\n        all_datafs = [pd.read_csv(path, index_col=aux_cols) for path in tqdm(csv_paths)]\n        final_dataf = pd.concat(all_datafs, axis=\"columns\")\n        # Remove issue of duplicate columns\n        numeric_cols = final_dataf.select_dtypes(include=np.number).columns\n        final_dataf.rename({k: str(v) for k, v in zip(numeric_cols, range(len(numeric_cols)))},\n                           axis=1,\n                           inplace=True)\n        # Combine all numeric columns with rank mean\n        num_dataf = final_dataf.select_dtypes(include=np.number)\n        num_dataf = num_dataf.groupby(era_col) if era_col else num_dataf\n        final_dataf[pred_col] = num_dataf.rank(pct=True, method=\"first\").mean(axis=1)\n        return final_dataf[[pred_col]]\n\n    def _get_model_id(self, model_name: str) -&gt; str:\n        \"\"\"\n        Get ID needed for prediction uploading.\n        :param model_name: Raw lowercase model name\n        of Numerai model that you have access to.\n        \"\"\"\n        return self.get_model_mapping[model_name]\n\n    @property\n    def get_model_mapping(self) -&gt; dict:\n        \"\"\"Mapping between raw model names and model IDs.\"\"\"\n        return self.api.get_models()\n\n    def _check_value_range(self, dataf: pd.DataFrame, cols: Union[str, list]):\n        \"\"\" Check if all predictions are in range (0...1). \"\"\"\n        cols = [cols] if isinstance(cols, str) else cols\n        for col in cols:\n            if not dataf[col].between(0, 1).all():\n                min_val, max_val = dataf[col].min(), dataf[col].max()\n                raise ValueError(\n                    f\"Values must be between 0 and 1. \\\nFound min value of '{min_val}' and max value of '{max_val}' for column '{col}'.\"\n                )\n\n    def __call__(\n            self,\n            dataf: pd.DataFrame,\n            model_name: str,\n            file_name: str = \"submission.csv\",\n            cols: Union[str, list] = \"prediction\",\n            *args,\n            **kwargs,\n    ):\n        \"\"\"\n        The most common use case will be to create a CSV and submit it immediately after that.\n        full_submission handles this.\n        \"\"\"\n        self.full_submission(\n            dataf=dataf,\n            file_name=file_name,\n            model_name=model_name,\n            cols=cols,\n            *args,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/#numerblox.submission.BaseSubmitter.get_model_mapping","title":"<code>get_model_mapping: dict</code>  <code>property</code>","text":"<p>Mapping between raw model names and model IDs.</p>"},{"location":"api/#numerblox.submission.BaseSubmitter.__call__","title":"<code>__call__(dataf, model_name, file_name='submission.csv', cols='prediction', *args, **kwargs)</code>","text":"<p>The most common use case will be to create a CSV and submit it immediately after that. full_submission handles this.</p> Source code in <code>numerblox/submission.py</code> <pre><code>def __call__(\n        self,\n        dataf: pd.DataFrame,\n        model_name: str,\n        file_name: str = \"submission.csv\",\n        cols: Union[str, list] = \"prediction\",\n        *args,\n        **kwargs,\n):\n    \"\"\"\n    The most common use case will be to create a CSV and submit it immediately after that.\n    full_submission handles this.\n    \"\"\"\n    self.full_submission(\n        dataf=dataf,\n        file_name=file_name,\n        model_name=model_name,\n        cols=cols,\n        *args,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#numerblox.submission.BaseSubmitter.combine_csvs","title":"<code>combine_csvs(csv_paths, aux_cols, era_col=None, pred_col='prediction')</code>","text":"<p>Read in csv files and combine all predictions with a rank mean. </p> <p>Multi-target predictions will be averaged out. </p> <p>:param csv_paths: List of full paths to .csv prediction files. </p> <p>:param aux_cols: ['id'] for Numerai Classic. </p> <p>['ticker', 'last_friday', 'data_type'], for example, with Numerai Signals. </p> <p>:param era_col: Column indicating era ('era' or 'last_friday'). </p> <p>Will be used for Grouping the rank mean if given. Skip groupby if no era_col provided. </p> <p>:param pred_col: 'prediction' for Numerai Classic and 'signal' for Numerai Signals.</p> Source code in <code>numerblox/submission.py</code> <pre><code>def combine_csvs(self, csv_paths: list,\n                 aux_cols: list,\n                 era_col: str = None,\n                 pred_col: str = 'prediction') -&gt; pd.DataFrame:\n    \"\"\"\n    Read in csv files and combine all predictions with a rank mean. \\n\n    Multi-target predictions will be averaged out. \\n\n    :param csv_paths: List of full paths to .csv prediction files. \\n\n    :param aux_cols: ['id'] for Numerai Classic. \\n\n    ['ticker', 'last_friday', 'data_type'], for example, with Numerai Signals. \\n\n    :param era_col: Column indicating era ('era' or 'last_friday'). \\n\n    Will be used for Grouping the rank mean if given. Skip groupby if no era_col provided. \\n\n    :param pred_col: 'prediction' for Numerai Classic and 'signal' for Numerai Signals.\n    \"\"\"\n    all_datafs = [pd.read_csv(path, index_col=aux_cols) for path in tqdm(csv_paths)]\n    final_dataf = pd.concat(all_datafs, axis=\"columns\")\n    # Remove issue of duplicate columns\n    numeric_cols = final_dataf.select_dtypes(include=np.number).columns\n    final_dataf.rename({k: str(v) for k, v in zip(numeric_cols, range(len(numeric_cols)))},\n                       axis=1,\n                       inplace=True)\n    # Combine all numeric columns with rank mean\n    num_dataf = final_dataf.select_dtypes(include=np.number)\n    num_dataf = num_dataf.groupby(era_col) if era_col else num_dataf\n    final_dataf[pred_col] = num_dataf.rank(pct=True, method=\"first\").mean(axis=1)\n    return final_dataf[[pred_col]]\n</code></pre>"},{"location":"api/#numerblox.submission.BaseSubmitter.full_submission","title":"<code>full_submission(dataf, model_name, cols, file_name='submission.csv', *args, **kwargs)</code>","text":"<p>Save DataFrame to csv and upload predictions through API.</p> <p>:param dataf: Main DataFrame containing <code>cols</code>. :param model_name: Lowercase Numerai model name. :param file_name: path to save model to relative to base directory. :param cols: Columns to be saved in submission file. 1 prediction column for Numerai Classic. At least 1 prediction column and 1 ticker column for Numerai Signals. args, *kwargs are passed to numerapi API. For example <code>version</code> argument in Numerai Classic submissions.</p> Source code in <code>numerblox/submission.py</code> <pre><code>def full_submission(\n    self,\n    dataf: pd.DataFrame,\n    model_name: str,\n    cols: Union[str, list],\n    file_name: str = 'submission.csv',\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Save DataFrame to csv and upload predictions through API.\n\n    :param dataf: Main DataFrame containing `cols`.\n    :param model_name: Lowercase Numerai model name.\n    :param file_name: path to save model to relative to base directory.\n    :param cols: Columns to be saved in submission file.\n    1 prediction column for Numerai Classic.\n    At least 1 prediction column and 1 ticker column for Numerai Signals.\n    *args, **kwargs are passed to numerapi API.\n    For example `version` argument in Numerai Classic submissions.\n    \"\"\"\n    self.save_csv(dataf=dataf, file_name=file_name, cols=cols)\n    self.upload_predictions(\n        file_name=file_name, model_name=model_name,\n        *args, **kwargs\n    )\n</code></pre>"},{"location":"api/#numerblox.submission.BaseSubmitter.save_csv","title":"<code>save_csv(dataf, file_name, cols, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>For Numerai Classic: Save index column + 'cols' (targets) to CSV. For Numerai Signals: Save ticker, friday_date, data_type and signal columns to CSV.</p> Source code in <code>numerblox/submission.py</code> <pre><code>@abstractmethod\ndef save_csv(\n    self,\n    dataf: pd.DataFrame,\n    file_name: str,\n    cols: Union[str, list],\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    For Numerai Classic: Save index column + 'cols' (targets) to CSV.\n    For Numerai Signals: Save ticker, friday_date, data_type and signal columns to CSV.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#numerblox.submission.BaseSubmitter.upload_predictions","title":"<code>upload_predictions(file_name, model_name, *args, **kwargs)</code>","text":"<p>Upload CSV file to Numerai for given model name. :param file_name: File name/path relative to directory_path. :param model_name: Lowercase raw model name (For example, 'integration_test').</p> Source code in <code>numerblox/submission.py</code> <pre><code>def upload_predictions(self, file_name: str, model_name: str, *args, **kwargs):\n    \"\"\"\n    Upload CSV file to Numerai for given model name.\n    :param file_name: File name/path relative to directory_path.\n    :param model_name: Lowercase raw model name (For example, 'integration_test').\n    \"\"\"\n    full_path = str(self.dir / file_name)\n    model_id = self._get_model_id(model_name=model_name)\n    api_type = str(self.api.__class__.__name__)\n    print(\n        f\"{api_type}: Uploading predictions from '{full_path}' for model '{model_name}' (model_id='{model_id}')\"\n    )\n    for attempt in range(self.max_retries):\n        try:\n            self.api.upload_predictions(\n                file_path=full_path, model_id=model_id, *args, **kwargs\n            )\n            print(\n                f\"{api_type} submission of '{full_path}' for '{model_name}' is successful!\"\n            )\n            return\n        except Exception as e:\n            if attempt &lt; self.max_retries - 1:  # i.e. not the last attempt\n                print(f\"Failed to upload '{full_path}' for '{model_name}' to Numerai. Retrying in {self.sleep_time} seconds...\")\n                print(f\"Error: {e}\")\n                time.sleep(self.sleep_time)\n            else:\n                if self.fail_silently:\n                    print(f\"Failed to upload'{full_path}' for '{model_name}' to Numerai. Skipping...\")\n                    print(f\"Error: {e}\")\n                else:\n                    print(f\"Failed to upload '{full_path}' for '{model_name}' to Numerai after {self.max_retries} attempts.\")\n                    raise e\n</code></pre>"},{"location":"api/#numerblox.submission.NumerBaySubmitter","title":"<code>NumerBaySubmitter</code>","text":"<p>             Bases: <code>BaseSubmitter</code></p> <p>Submit to NumerBay to fulfill sale orders, in addition to submission to Numerai.</p> <p>:param tournament_submitter: Base tournament submitter (NumeraiClassicSubmitter or NumeraiSignalsSubmitter). This submitter will use the same directory path. :param upload_to_numerai: Whether to also submit to Numerai using the tournament submitter. Defaults to True, set to False to only upload to NumerBay. :param numerbay_username: NumerBay username :param numerbay_password: NumerBay password</p> Source code in <code>numerblox/submission.py</code> <pre><code>class NumerBaySubmitter(BaseSubmitter):\n    \"\"\"\n    Submit to NumerBay to fulfill sale orders, in addition to submission to Numerai.\n\n    :param tournament_submitter: Base tournament submitter (NumeraiClassicSubmitter or NumeraiSignalsSubmitter). This submitter will use the same directory path.\n    :param upload_to_numerai: Whether to also submit to Numerai using the tournament submitter. Defaults to True, set to False to only upload to NumerBay.\n    :param numerbay_username: NumerBay username\n    :param numerbay_password: NumerBay password\n    \"\"\"\n    def __init__(self,\n                 tournament_submitter: Union[NumeraiClassicSubmitter, NumeraiSignalsSubmitter],\n                 upload_to_numerai: bool = True,\n                 numerbay_username: str = None,\n                 numerbay_password: str = None):\n        super().__init__(\n            directory_path=str(tournament_submitter.dir), api=tournament_submitter.api,\n            max_retries=tournament_submitter.max_retries, sleep_time=tournament_submitter.sleep_time,\n            fail_silently=tournament_submitter.fail_silently\n        )\n        from numerbay import NumerBay\n        self.numerbay_api = NumerBay(username=numerbay_username, password=numerbay_password)\n        self.tournament_submitter = tournament_submitter\n        self.upload_to_numerai = upload_to_numerai\n\n    def upload_predictions(self,\n                           file_name: str,\n                           model_name: str,\n                           numerbay_product_full_name: str,\n                           *args,\n                           **kwargs):\n        \"\"\"\n        Upload CSV file to NumerBay (and Numerai if 'upload_to_numerai' is True) for given model name and NumerBay product full name.\n        :param file_name: File name/path relative to directory_path.\n        :param model_name: Lowercase raw model name (For example, 'integration_test').\n        :param numerbay_product_full_name: NumerBay product full name in the format of [category]-[product name], e.g. 'numerai-predictions-numerbay'\n        \"\"\"\n        if self.upload_to_numerai:\n            self.tournament_submitter.upload_predictions(file_name, model_name, *args, **kwargs)\n\n        full_path = str(self.dir / file_name)\n        api_type = str(self.numerbay_api.__class__.__name__)\n        print(\n            f\"{api_type}: Uploading predictions from '{full_path}' for NumerBay product '{numerbay_product_full_name}'\"\n        )\n        artifact = self.numerbay_api.upload_artifact(\n            str(full_path), product_full_name=numerbay_product_full_name\n        )\n        if artifact:\n            print(\n                f\"{api_type} submission of '{full_path}' for NumerBay product [bold blue]{numerbay_product_full_name} is successful!\"\n            )\n        else:\n            print(f\"\"\"WARNING: Upload skipped for NumerBay product '{numerbay_product_full_name}', \n                  the product uses buyer-side encryption but does not have any active sale order to upload for.\"\"\")\n\n    def full_submission(\n        self,\n        dataf: pd.DataFrame,\n        model_name: str,\n        cols: Union[str, list],\n        numerbay_product_full_name: str,\n        file_name: str = 'submission.csv',\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Save DataFrame to csv and upload predictions through API.\n\n        :param dataf: Main DataFrame containing `cols`.\n        :param model_name: Lowercase Numerai model name.\n        :param numerbay_product_full_name: NumerBay product full name in the format of [category]-[product name], e.g. 'numerai-predictions-numerbay'\n        :param file_name: path to save model to relative to base directory.\n        :param cols: Columns to be saved in submission file.\n        1 prediction column for Numerai Classic.\n        At least 1 prediction column and 1 ticker column for Numerai Signals.\n        *args, **kwargs are passed to numerapi API.\n        For example `version` argument in Numerai Classic submissions.\n        \"\"\"\n        self.save_csv(dataf=dataf, file_name=file_name, cols=cols)\n        self.upload_predictions(\n            file_name=file_name, model_name=model_name, numerbay_product_full_name=numerbay_product_full_name,\n            *args, **kwargs\n        )\n\n    def combine_csvs(self, *args,**kwargs) -&gt; pd.DataFrame:\n        return self.tournament_submitter.combine_csvs(*args,**kwargs)\n\n    def save_csv(self, *args, **kwargs):\n        self.tournament_submitter.save_csv(*args, **kwargs)\n\n    @property\n    def get_model_mapping(self) -&gt; dict:\n        return self.tournament_submitter.api.get_models()\n\n    def __call__(\n            self,\n            dataf: pd.DataFrame,\n            model_name: str,\n            numerbay_product_full_name: str,\n            file_name: str = \"submission.csv\",\n            cols: Union[str, list] = \"prediction\",\n            *args,\n            **kwargs,\n    ):\n        \"\"\"\n        The most common use case will be to create a CSV and submit it immediately after that.\n        full_submission handles this.\n        \"\"\"\n        self.full_submission(\n            dataf=dataf,\n            file_name=file_name,\n            model_name=model_name,\n            numerbay_product_full_name=numerbay_product_full_name,\n            cols=cols,\n            *args,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/#numerblox.submission.NumerBaySubmitter.__call__","title":"<code>__call__(dataf, model_name, numerbay_product_full_name, file_name='submission.csv', cols='prediction', *args, **kwargs)</code>","text":"<p>The most common use case will be to create a CSV and submit it immediately after that. full_submission handles this.</p> Source code in <code>numerblox/submission.py</code> <pre><code>def __call__(\n        self,\n        dataf: pd.DataFrame,\n        model_name: str,\n        numerbay_product_full_name: str,\n        file_name: str = \"submission.csv\",\n        cols: Union[str, list] = \"prediction\",\n        *args,\n        **kwargs,\n):\n    \"\"\"\n    The most common use case will be to create a CSV and submit it immediately after that.\n    full_submission handles this.\n    \"\"\"\n    self.full_submission(\n        dataf=dataf,\n        file_name=file_name,\n        model_name=model_name,\n        numerbay_product_full_name=numerbay_product_full_name,\n        cols=cols,\n        *args,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#numerblox.submission.NumerBaySubmitter.full_submission","title":"<code>full_submission(dataf, model_name, cols, numerbay_product_full_name, file_name='submission.csv', *args, **kwargs)</code>","text":"<p>Save DataFrame to csv and upload predictions through API.</p> <p>:param dataf: Main DataFrame containing <code>cols</code>. :param model_name: Lowercase Numerai model name. :param numerbay_product_full_name: NumerBay product full name in the format of [category]-[product name], e.g. 'numerai-predictions-numerbay' :param file_name: path to save model to relative to base directory. :param cols: Columns to be saved in submission file. 1 prediction column for Numerai Classic. At least 1 prediction column and 1 ticker column for Numerai Signals. args, *kwargs are passed to numerapi API. For example <code>version</code> argument in Numerai Classic submissions.</p> Source code in <code>numerblox/submission.py</code> <pre><code>def full_submission(\n    self,\n    dataf: pd.DataFrame,\n    model_name: str,\n    cols: Union[str, list],\n    numerbay_product_full_name: str,\n    file_name: str = 'submission.csv',\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Save DataFrame to csv and upload predictions through API.\n\n    :param dataf: Main DataFrame containing `cols`.\n    :param model_name: Lowercase Numerai model name.\n    :param numerbay_product_full_name: NumerBay product full name in the format of [category]-[product name], e.g. 'numerai-predictions-numerbay'\n    :param file_name: path to save model to relative to base directory.\n    :param cols: Columns to be saved in submission file.\n    1 prediction column for Numerai Classic.\n    At least 1 prediction column and 1 ticker column for Numerai Signals.\n    *args, **kwargs are passed to numerapi API.\n    For example `version` argument in Numerai Classic submissions.\n    \"\"\"\n    self.save_csv(dataf=dataf, file_name=file_name, cols=cols)\n    self.upload_predictions(\n        file_name=file_name, model_name=model_name, numerbay_product_full_name=numerbay_product_full_name,\n        *args, **kwargs\n    )\n</code></pre>"},{"location":"api/#numerblox.submission.NumerBaySubmitter.upload_predictions","title":"<code>upload_predictions(file_name, model_name, numerbay_product_full_name, *args, **kwargs)</code>","text":"<p>Upload CSV file to NumerBay (and Numerai if 'upload_to_numerai' is True) for given model name and NumerBay product full name. :param file_name: File name/path relative to directory_path. :param model_name: Lowercase raw model name (For example, 'integration_test'). :param numerbay_product_full_name: NumerBay product full name in the format of [category]-[product name], e.g. 'numerai-predictions-numerbay'</p> Source code in <code>numerblox/submission.py</code> <pre><code>def upload_predictions(self,\n                       file_name: str,\n                       model_name: str,\n                       numerbay_product_full_name: str,\n                       *args,\n                       **kwargs):\n    \"\"\"\n    Upload CSV file to NumerBay (and Numerai if 'upload_to_numerai' is True) for given model name and NumerBay product full name.\n    :param file_name: File name/path relative to directory_path.\n    :param model_name: Lowercase raw model name (For example, 'integration_test').\n    :param numerbay_product_full_name: NumerBay product full name in the format of [category]-[product name], e.g. 'numerai-predictions-numerbay'\n    \"\"\"\n    if self.upload_to_numerai:\n        self.tournament_submitter.upload_predictions(file_name, model_name, *args, **kwargs)\n\n    full_path = str(self.dir / file_name)\n    api_type = str(self.numerbay_api.__class__.__name__)\n    print(\n        f\"{api_type}: Uploading predictions from '{full_path}' for NumerBay product '{numerbay_product_full_name}'\"\n    )\n    artifact = self.numerbay_api.upload_artifact(\n        str(full_path), product_full_name=numerbay_product_full_name\n    )\n    if artifact:\n        print(\n            f\"{api_type} submission of '{full_path}' for NumerBay product [bold blue]{numerbay_product_full_name} is successful!\"\n        )\n    else:\n        print(f\"\"\"WARNING: Upload skipped for NumerBay product '{numerbay_product_full_name}', \n              the product uses buyer-side encryption but does not have any active sale order to upload for.\"\"\")\n</code></pre>"},{"location":"api/#numerblox.submission.NumeraiClassicSubmitter","title":"<code>NumeraiClassicSubmitter</code>","text":"<p>             Bases: <code>BaseSubmitter</code></p> <p>Submit for Numerai Classic.</p> <p>:param directory_path: Base directory to save and read prediction files from. </p> <p>:param key: Key object containing valid credentials for Numerai Classic. </p> <p>:param max_retries: Maximum number of retries for uploading predictions to Numerai.  :param sleep_time: Time to sleep between uploading retries. :param fail_silently: Whether to skip uploading to Numerai without raising an error.  Useful for if you are uploading many models in a loop and want to skip models that fail to upload. args, *kwargs will be passed to NumerAPI initialization.</p> Source code in <code>numerblox/submission.py</code> <pre><code>class NumeraiClassicSubmitter(BaseSubmitter):\n    \"\"\"\n    Submit for Numerai Classic.\n\n    :param directory_path: Base directory to save and read prediction files from. \\n\n    :param key: Key object containing valid credentials for Numerai Classic. \\n\n    :param max_retries: Maximum number of retries for uploading predictions to Numerai. \n    :param sleep_time: Time to sleep between uploading retries.\n    :param fail_silently: Whether to skip uploading to Numerai without raising an error. \n    Useful for if you are uploading many models in a loop and want to skip models that fail to upload.\n    *args, **kwargs will be passed to NumerAPI initialization.\n    \"\"\"\n    def __init__(self, directory_path: str, key: Key, \n                 max_retries: int = 2, sleep_time: int = 10, \n                 fail_silently=False, *args, **kwargs):\n        api = NumerAPI(public_id=key.pub_id, secret_key=key.secret_key, *args, **kwargs)\n        super().__init__(\n            directory_path=directory_path, api=api,\n            max_retries=max_retries, sleep_time=sleep_time, \n            fail_silently=fail_silently\n        )\n\n    def save_csv(\n            self,\n            dataf: pd.DataFrame,\n            file_name: str = \"submission.csv\",\n            cols: str = 'prediction',\n            *args,\n            **kwargs,\n    ):\n        \"\"\"\n        :param dataf: DataFrame which should have at least the following columns:\n        1. id (as index column)\n        2. cols (for example, 'prediction_mymodel'). Will be saved in 'prediction' column\n        :param file_name: .csv file path.\n        :param cols: Prediction column name.\n        For example, 'prediction' or 'prediction_mymodel'.\n        \"\"\"\n        sub_dataf = deepcopy(dataf)\n        self._check_value_range(dataf=sub_dataf, cols=cols)\n\n        full_path = str(self.dir / file_name)\n        print(\n            f\"Saving predictions CSV to '{full_path}'.\"\n        )\n        sub_dataf.loc[:, 'prediction'] = sub_dataf[cols]\n        sub_dataf.loc[:, 'prediction'].to_csv(full_path, *args, **kwargs)\n</code></pre>"},{"location":"api/#numerblox.submission.NumeraiClassicSubmitter.save_csv","title":"<code>save_csv(dataf, file_name='submission.csv', cols='prediction', *args, **kwargs)</code>","text":"<p>:param dataf: DataFrame which should have at least the following columns: 1. id (as index column) 2. cols (for example, 'prediction_mymodel'). Will be saved in 'prediction' column :param file_name: .csv file path. :param cols: Prediction column name. For example, 'prediction' or 'prediction_mymodel'.</p> Source code in <code>numerblox/submission.py</code> <pre><code>def save_csv(\n        self,\n        dataf: pd.DataFrame,\n        file_name: str = \"submission.csv\",\n        cols: str = 'prediction',\n        *args,\n        **kwargs,\n):\n    \"\"\"\n    :param dataf: DataFrame which should have at least the following columns:\n    1. id (as index column)\n    2. cols (for example, 'prediction_mymodel'). Will be saved in 'prediction' column\n    :param file_name: .csv file path.\n    :param cols: Prediction column name.\n    For example, 'prediction' or 'prediction_mymodel'.\n    \"\"\"\n    sub_dataf = deepcopy(dataf)\n    self._check_value_range(dataf=sub_dataf, cols=cols)\n\n    full_path = str(self.dir / file_name)\n    print(\n        f\"Saving predictions CSV to '{full_path}'.\"\n    )\n    sub_dataf.loc[:, 'prediction'] = sub_dataf[cols]\n    sub_dataf.loc[:, 'prediction'].to_csv(full_path, *args, **kwargs)\n</code></pre>"},{"location":"api/#numerblox.submission.NumeraiSignalsSubmitter","title":"<code>NumeraiSignalsSubmitter</code>","text":"<p>             Bases: <code>BaseSubmitter</code></p> <p>Submit for Numerai Signals.</p> <p>:param directory_path: Base directory to save and read prediction files from. </p> <p>:param key: Key object containing valid credentials for Numerai Signals. </p> <p>:param max_retries: Maximum number of retries for uploading predictions to Numerai.  :param sleep_time: Time to sleep between uploading retries. :param fail_silently: Whether to skip uploading to Numerai without raising an error.  Useful for if you are uploading many models in a loop and want to skip models that fail to upload. args, *kwargs will be passed to SignalsAPI initialization.</p> Source code in <code>numerblox/submission.py</code> <pre><code>class NumeraiSignalsSubmitter(BaseSubmitter):\n    \"\"\"\n    Submit for Numerai Signals.\n\n    :param directory_path: Base directory to save and read prediction files from. \\n\n    :param key: Key object containing valid credentials for Numerai Signals. \\n\n    :param max_retries: Maximum number of retries for uploading predictions to Numerai. \n    :param sleep_time: Time to sleep between uploading retries.\n    :param fail_silently: Whether to skip uploading to Numerai without raising an error. \n    Useful for if you are uploading many models in a loop and want to skip models that fail to upload.\n    *args, **kwargs will be passed to SignalsAPI initialization.\n    \"\"\"\n    def __init__(self, directory_path: str, key: Key, \n                 max_retries: int = 2, sleep_time: int = 10, \n                 fail_silently=False, *args, **kwargs):\n        api = SignalsAPI(\n            public_id=key.pub_id, secret_key=key.secret_key, *args, **kwargs\n        )\n        super().__init__(\n            directory_path=directory_path, api=api,\n            max_retries=max_retries, sleep_time=sleep_time,\n            fail_silently=fail_silently\n        )\n        self.supported_ticker_formats = [\n            \"cusip\",\n            \"sedol\",\n            \"ticker\",\n            \"numerai_ticker\",\n            \"bloomberg_ticker\",\n        ]\n\n    def save_csv(\n            self,\n            dataf: pd.DataFrame,\n            cols: list,\n            file_name: str = \"submission.csv\",\n            *args, **kwargs\n    ):\n        \"\"\"\n        :param dataf: DataFrame which should have at least the following columns:\n         1. One of supported ticker formats (cusip, sedol, ticker, numerai_ticker or bloomberg_ticker)\n         2. signal (Values between 0 and 1 (exclusive))\n         Additional columns for if you include validation data (optional):\n         3. friday_date (YYYYMMDD format date indication)\n         4. data_type ('val' and 'live' partitions)\n\n         :param cols: All cols that are saved in CSV.\n         cols should contain at least 1 ticker column and a 'signal' column.\n         For example: ['bloomberg_ticker', 'signal']\n         :param file_name: .csv file path.\n        \"\"\"\n        self._check_ticker_format(cols=cols)\n        self._check_value_range(dataf=dataf, cols=\"signal\")\n\n        full_path = str(self.dir / file_name)\n        print(\n            f\"Saving Signals predictions CSV to '{full_path}'.\"\n        )\n        dataf.loc[:, cols].reset_index(drop=True).to_csv(\n            full_path, index=False, *args, **kwargs\n        )\n\n    def _check_ticker_format(self, cols: list):\n        \"\"\" Check for valid ticker format. \"\"\"\n        valid_tickers = set(cols).intersection(set(self.supported_ticker_formats))\n        if not valid_tickers:\n            raise NotImplementedError(\n                f\"No supported ticker format in {cols}). \\\nSupported: '{self.supported_ticker_formats}'\"\n            )\n</code></pre>"},{"location":"api/#numerblox.submission.NumeraiSignalsSubmitter.save_csv","title":"<code>save_csv(dataf, cols, file_name='submission.csv', *args, **kwargs)</code>","text":"<p>:param dataf: DataFrame which should have at least the following columns:  1. One of supported ticker formats (cusip, sedol, ticker, numerai_ticker or bloomberg_ticker)  2. signal (Values between 0 and 1 (exclusive))  Additional columns for if you include validation data (optional):  3. friday_date (YYYYMMDD format date indication)  4. data_type ('val' and 'live' partitions)</p> <p>:param cols: All cols that are saved in CSV.  cols should contain at least 1 ticker column and a 'signal' column.  For example: ['bloomberg_ticker', 'signal']  :param file_name: .csv file path.</p> Source code in <code>numerblox/submission.py</code> <pre><code>def save_csv(\n        self,\n        dataf: pd.DataFrame,\n        cols: list,\n        file_name: str = \"submission.csv\",\n        *args, **kwargs\n):\n    \"\"\"\n    :param dataf: DataFrame which should have at least the following columns:\n     1. One of supported ticker formats (cusip, sedol, ticker, numerai_ticker or bloomberg_ticker)\n     2. signal (Values between 0 and 1 (exclusive))\n     Additional columns for if you include validation data (optional):\n     3. friday_date (YYYYMMDD format date indication)\n     4. data_type ('val' and 'live' partitions)\n\n     :param cols: All cols that are saved in CSV.\n     cols should contain at least 1 ticker column and a 'signal' column.\n     For example: ['bloomberg_ticker', 'signal']\n     :param file_name: .csv file path.\n    \"\"\"\n    self._check_ticker_format(cols=cols)\n    self._check_value_range(dataf=dataf, cols=\"signal\")\n\n    full_path = str(self.dir / file_name)\n    print(\n        f\"Saving Signals predictions CSV to '{full_path}'.\"\n    )\n    dataf.loc[:, cols].reset_index(drop=True).to_csv(\n        full_path, index=False, *args, **kwargs\n    )\n</code></pre>"},{"location":"contributing/","title":"How To Contribute","text":"<p>First, thank you for your consideration to contribute to <code>numerblox</code>! This document provides some general guidelines to streamline the contribution process.</p>"},{"location":"contributing/#installation","title":"Installation","text":"<p>If you haven't installed <code>numerblox</code> yet, clone the project into your favorite development environment and install the repo in editable mode and with all dev dependencies. </p> <pre><code>git clone https://github.com/crowdcent/numerblox.git\npip install poetry\ncd numerblox\npoetry install\n</code></pre>"},{"location":"contributing/#developing-considerations","title":"Developing considerations","text":""},{"location":"contributing/#1-building-a-new-component","title":"1. Building a new component","text":"<p>If you want to build a new component. Please consider the following steps:</p> <ol> <li>Place the new component in the appropriate section. Is it a Downloader (<code>download.py</code>), a Preprocessor (<code>preprocessing.py</code>) or a Submitting tool (<code>submission.py</code>)? Also check the documentation on that section to check for templates, conventions and how these blocks are constructed in general.</li> <li>Add tests for this new component in the appropriate test file. If you are introducing a new Downloader, add tests in <code>tests/test_downloader.py</code>. If you are introducing a new Preprocessor, add tests in <code>tests/test_preprocessing.py</code>. etc.</li> <li>When making a preprocessor or postprocessor, make sure the component follows scikit-learn conventions. The core things to implement are inheriting from <code>BaseEstimator</code> and implementing <code>fit</code>, <code>transform</code> and <code>get_feature_names_out</code> methods. </li> <li>If your component introduces new dependencies, make sure to add them to poetry with <code>poetry add &lt;library&gt;</code>.</li> </ol>"},{"location":"contributing/#2-fixing-bugs","title":"2. Fixing bugs","text":"<p>Even though most of the components in this library are tested, users will still likely run into issues. If you discover bugs, other issues or ideas for enhancements, do not hesitate to make a Github issue. Describe in the issue what code was run on what machine and background on the issue. Add stacktraces and screenshots if this is relevant for solving the issue. Also, please add appropriate labels for the Github issue.</p> <ul> <li>Ensure the bug was not already reported by searching on GitHub under Issues.</li> <li>If you're unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.</li> <li>Be sure to add the complete error messages.</li> <li>Be sure to add tests that fail without your patch, and pass with it.</li> </ul>"},{"location":"contributing/#3-creating-an-example-notebook","title":"3. Creating an example notebook","text":"<p>We welcome example notebooks that demonstrate the use of <code>numerblox</code>. If you want to create an example notebook, please make a notebook in the <code>examples/</code> folder. Make sure to add appropriate descriptions and explain the process of using the various components. Before committing please run the notebook from top to bottom. If it runs without errors, you can commit the notebook. Lastly, if the notebook uses additional libraries, please note this at the top of the notebook and create a code block with <code>!pip install &lt;library&gt;</code>.</p> <p>Example pip install cell:</p> <pre><code>!pip install scikit-lego plotly\n</code></pre>"},{"location":"contributing/#did-you-write-a-patch-that-fixes-a-bug","title":"Did you write a patch that fixes a bug?","text":"<ul> <li>Open a new GitHub pull request with the patch.</li> <li>Ensure that your PR includes a test that fails without your patch, and pass with it.</li> <li>Ensure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable.</li> </ul>"},{"location":"contributing/#pr-submission-guidelines","title":"PR submission guidelines","text":"<ul> <li>Keep each PR focused. While it's more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.</li> <li>Do not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.</li> <li>If, however, you submitted a PR and received a request for changes, you should proceed with commits inside that PR, so that the maintainer can see the incremental fixes and won't need to review the whole PR again. In the exception case where you realize it'll take many many commits to complete the requests, then it's probably best to close the PR, do the work and then submit it again. Use common sense where you'd choose one way over another.</li> </ul>"},{"location":"crowdcent/","title":"About CrowdCent","text":"<p>CrowdCent is on a mission to decentralize investment management by changing the way investment funds make decisions and allocate capital. We are the machine learning and coordination layer for online investment communities looking to turn their data into actionable, investable portfolios.</p> <p>More information about CrowdCent can be found on crowdcent.com.</p>"},{"location":"disclaimer/","title":"Disclaimer","text":"<p>Software is provided under an Apache 2.0 licence.  We can never guarantee that all computations will be correct. The software  is tested to the best of our ability, but we cannot guarantee correct results. Always verify results before using 3rd-party libraries like this in high stakes situations.</p> <p>Under no circumstances should any information provided in this software \u2014 or on associated distribution outlets \u2014 be construed as an offer soliciting the purchase or sale of any security or interest in any pooled investment vehicle sponsored, discussed, or mentioned by CrowdCent LLC or affiliates. Nor should it be construed as an offer to provide investment advisory services; an offer to invest in a CrowdCent investment vehicle will be made separately and only by means of the confidential offering documents of the specific pooled investment vehicles \u2014 which should be read in their entirety, and only to those who, among other requirements, meet certain qualifications under federal securities laws. Such investors, defined as accredited investors and qualified purchasers, are generally deemed capable of evaluating the merits and risks of prospective investments and financial matters. There can be no assurances that CrowdCent\u2019s investment objectives will be achieved or investment strategies will be successful. Any investment in a vehicle managed by CrowdCent involves a high degree of risk including the risk that the entire amount invested is lost. Any investments or portfolio companies mentioned, referred to, or described are not representative of all investments in vehicles managed by CrowdCent and there can be no assurance that the investments will be profitable or that other investments made in the future will have similar characteristics or results.</p>"},{"location":"download/","title":"Downloaders","text":""},{"location":"download/#numerai-classic","title":"Numerai Classic","text":"<p><code>NumeraiClassicDownloader</code> simplifies downloading of datasets from Numerai's API. It allows you to easily download data with a few lines and the data is automatically organized in directories.</p> <p>NOTE: Only int8 versions are available in this downloader. From v4.2 onwards, Numerai only provides int8 versions of the data.</p> <pre><code>from numerblox.download import NumeraiClassicDownloader\n\ndl = NumeraiClassicDownloader(directory_path=\"my_numerai_data_folder\")\n\n# Training and validation data\ndl.download_training_data(\"train_val\", version=\"4.3\")\n\n# Inference data (live)\ndl.download_inference_data(\"live\", version=\"4.3\")\n</code></pre> <p>Besides these common use cases you can also get feature sets and meta model predictions with <code>NumeraiClassicDownloader</code>. </p> <pre><code>from numerblox.download import NumeraiClassicDownloader\n\ndl = NumeraiClassicDownloader(directory_path=\"my_numerai_data_folder\")\n\n# Get feature sets (v4.3)\nfeatures = dl.get_classic_features()\n\n# Get meta model predictions\ndl.download_meta_model_preds()\nmeta_model_preds = pd.read_parquet(\"my_numerai_data_folder/meta_model.parquet\")\n</code></pre>"},{"location":"download/#numerai-signals","title":"Numerai Signals","text":"<p>For Numerai Signals we have several options implemented to download data. Numerai Signals Downloaders have similar methods as <code>NumeraiClassicDownloader</code>.</p>"},{"location":"download/#eod-historical-data","title":"EOD Historical Data","text":"<p>Download data from EOD historical data. A common data vendor used for Numerai Signals. </p> <p>More information: https://eodhistoricaldata.com</p> <p>Make sure you have the underlying Python package for EOD installed.</p> <pre><code>pip install eod\n</code></pre> <p>For EOD you also need to define credentials in the form of an API key.</p> <p>More information: https://eodhd.com/pricing</p> <pre><code>from numerblox.download import EODDownloader\n\neod_api_key = \"MY_EOD_API_KEY\"\ntickers = [\"AAPL.US\", \"MSFT.US\", \"GOOG.US\"]\ndl = EODDownloader(directory_path=\"my_numerai_signals_folder\",\nkey=eod_api_key, tickers=tickers)\n\n# Download full dataset\ndl.download_training_data(start=\"2008-01-01\")\n\n# load data directly into DataFrame from January 1st 2023 for inference.\nlive_data = dl.downloader_live_data(start=\"2023-01-01\")\n</code></pre>"},{"location":"download/#kaggle","title":"Kaggle","text":"<p>Some Numerai dataset are uploaded and maintained on Kaggle Datasets. NumerBlox offers a convenient API to download these datasets.</p> <p>For authentication, make sure you have a directory called .kaggle in your home directory with therein a kaggle.json file. kaggle.json should have the following structure: <code>{\"username\": USERNAME, \"key\": KAGGLE_API_KEY}</code></p> <p>More info on authentication: github.com/Kaggle/kaggle-api#api-credentials</p> <p>More info on the Kaggle Python API: kaggle.com/donkeys/kaggle-python-api</p> <p>Also make sure you have the <code>kaggle</code> Python package installed.</p> <pre><code>pip install kaggle\n</code></pre> <p>Below is a quickstart example using Katsu's starter dataset.</p> <pre><code>from numerblox.download import KaggleDownloader\n\nkd = KaggleDownloader(directory_path=\"my_numerai_signals_folder\")\n\n# A good example of Numerai Signals data on Kaggle Datasets is Katsu1110's yfinance price dataset.\nkd.download_inference_data(\"code1110/yfinance-stock-price-data-for-numerai-signals\")\n</code></pre>"},{"location":"download/#rolling-your-own-downloader","title":"Rolling your own downloader","text":"<p>We invite users to build out their own downloaders for Numerai Signals. The only requirements are that you inherit from <code>numerblox.download.BaseDownloader</code> and implement the <code>download_training_data</code> and <code>download_inference_data</code> methods. Below you will find a template for this.</p> <p>If you have a downloader that you would like to share with the community, please open a Pull Request in NumerBlox.</p> <pre><code>class AwesomeCustomDownloader(BaseDownloader):\n    \"\"\"\n    TEMPLATE -\n    Download awesome financial data for Numerai Signals from who knows where.\n\n    :param directory_path: Base folder to download files to.\n    \"\"\"\n    def __init__(self, directory_path: str):\n        super().__init__(directory_path=directory_path)\n\n    def download_inference_data(self, *args, **kwargs):\n        \"\"\" (minimal) weekly inference downloading here. \"\"\"\n        ...\n\n    def download_training_data(self, *args, **kwargs):\n        \"\"\" Training + validation dataset downloading here. \"\"\"\n        ...\n\n</code></pre>"},{"location":"end_to_end/","title":"End To End Examples","text":"<p>This section will show NumerBlox in action for some more advanced use cases. If you are looking for inspiration to leverage the power of NumerBlox, check out these examples.</p> <p>First we download the classic data with NumeraiClassicDownloader. We use a NumerFrame for convenience to parse the dataset.</p> <pre><code>from numerblox.numerframe import create_numerframe\nfrom numerblox.download import NumeraiClassicDownloader\ndl = NumeraiClassicDownloader(directory_path=\"my_numerai_data_folder\")\ndl.download_training_data(\"train_val\", version=\"4.2\", int8=True)\ndf = create_numerframe(\"my_numerai_data_folder/train_val/train_int8.parquet\")\nval_df = create_numerframe(\"my_numerai_data_folder/train_val/val_int8.parquet\")\n\nX, y = df.get_feature_target_pair(multi_target=False)\nfncv3_cols = df.get_fncv3_feature_data.columns.tolist()\n\nval_X, val_y = val_df.get_feature_target_pair(multi_target=False)\nval_features = val_df.get_feature_data\nval_eras = val_df.get_era_data\n</code></pre>"},{"location":"end_to_end/#1-neutralized-xgboost-pipeline","title":"1. Neutralized XGBoost pipeline.","text":"<p>Let's construct an end-to-end pipeline that does the following: - Augment FNCv3 features with group statistics features for the <code>sunshine</code> and <code>rain</code> data. - Fit 5 folds of XGBoost. - Ensemble them with a weighted average where the more recent folds get a higher weight. - Neutralize the prediction with respect to the original features.</p> <p>External libraries are xgboost and sklego. Make sure to have these dependencies installed.</p> <pre><code>!pip install xgboost sklego\n</code></pre> <pre><code>from xgboost import XGBRegressor\nfrom sklego.preprocessing import ColumnSelector\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.pipeline import make_union\nfrom sklearn.compose import make_column_transformer\n\nfrom numerblox.preprocessing import GroupStatsPreProcessor\nfrom numerblox.meta import CrossValEstimator, make_meta_pipeline\nfrom numerblox.ensemble import NumeraiEnsemble\nfrom numerblox.neutralizers import FeatureNeutralizer\n\n# Preprocessing\ngpp = GroupStatsPreProcessor(groups=['sunshine', 'rain'])\nfncv3_selector = ColumnSelector(fncv3_cols)\n\npreproc_pipe = make_union(gpp, fncv3_selector)\n\n# Model\nxgb = XGBRegressor()\ncve = CrossValEstimator(estimator=xgb, cv=TimeSeriesSplit(n_splits=5))\nens = NumeraiEnsemble()\nfn = FeatureNeutralizer(proportion=0.5)\nfull_pipe = make_meta_pipeline(preproc_pipe, cve, ens, fn)\n\n# Train full model\nfull_pipe.fit(X, y, numeraiensemble__eras=eras);\n\n# Inference on validation data\nval_preds = full_pipe.predict(val_X, eras=val_eras, features=val_features)\n</code></pre>"},{"location":"end_to_end/#2-multi-classification-ensemble","title":"2. Multi Classification Ensemble","text":"<p>This example shows a multiclass classification example where the Numerai target is transformed into integers (<code>[0, 0.25, 0.5, 0.75, 1.0] -&gt; [0, 1, 2, 3, 4]</code>) and treated as a classification problem. </p> <p>When we call <code>predict_proba</code> on a classifier the result will be a probability for every class, like for example <code>[0.1, 0.2, 0.3, 0.2, 0.2]</code>. In order to reduce these to one number we use the <code>PredictionReducer</code>, which takes the probabilities for every model and reduces it with a vector multiplication (Fro example, <code>[0.1, 0.2, 0.3, 0.2, 0.2] @ [0, 1, 2, 3, 4] = 2.2</code>). It does this for every model so the output of <code>PredictionReducer</code> has 3 columns. </p> <p>Because we set <code>donate_weighted=True</code> in <code>NumeraiEnsemble</code> 3 columns are reduced to one column using a weighted ensemble where the most recent fold get the highest weight. Lastly, the final prediction column is neutralized.</p> <pre><code>from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom numerblox.meta import CrossValEstimator, make_meta_pipeline\nfrom numerblox.ensemble import NumeraiEnsemble, PredictionReducer\nfrom numerblox.neutralizers import FeatureNeutralizer\n\nmodel = DecisionTreeClassifier()\ncrossval1 = CrossValEstimator(estimator=model, cv=TimeSeriesSplit(n_splits=3), predict_func='predict_proba')\npred_rud = PredictionReducer(n_models=3, n_classes=5)\nens2 = NumeraiEnsemble(donate_weighted=True)\nneut2 = FeatureNeutralizer(proportion=0.5)\nfull_pipe = make_meta_pipeline(preproc_pipe, crossval1, pred_rud, ens2, neut2)\n\nfull_pipe.fit(X, y, numeraiensemble__eras=eras)\n\npreds = full_pipe.predict(val_X, eras=val_eras, features=val_features)\n</code></pre>"},{"location":"end_to_end/#3-ensemble-of-ensemble-of-regressors","title":"3. Ensemble of ensemble of regressors","text":"<p>This object introduces a <code>ColumnTransformer</code> that contains 3 pipelines. Each pipeline can have a different set of arguments. Here we simplify by passing every pipeline with the same columns.  The output from all pipelines is concatenated, ensembled with <code>NumeraiEnsemble</code> and the final ensembles column is neutralized. Note that every fold here is equal weighted. If you want to give recent folds more weight set <code>weights</code> in <code>NumeraiEnsemble</code> for all <code>ColumnTransformer</code> output.</p> <pre><code>from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom numerblox.meta import CrossValEstimator, make_meta_pipeline\nfrom numerblox.ensemble import NumeraiEnsemble,\nfrom numerblox.neutralizers import FeatureNeutralizer\n\npipes = []\nfor i in range(3):\n    model = DecisionTreeRegressor()\n    crossval = CrossValEstimator(estimator=model, cv=TimeSeriesSplit(n_splits=5), predict_func='predict')\n    pipe = make_pipeline(crossval)\n    pipes.append(pipe)\n\nmodels = make_column_transformer(*[(pipe, features.columns.tolist()) for pipe in pipes])\nens_end = NumeraiEnsemble()\nneut = FeatureNeutralizer(proportion=0.5)\nfull_pipe = make_meta_pipeline(models, ens_end, neut)\n\nfull_pipe.fit(X, y, \n              columntransformer__eras=eras,\n              numeraiensemble__eras=eras)\n\npreds = full_pipe.predict(val_X, eras=val_eras, features=val_features)\n</code></pre>"},{"location":"evaluation/","title":"Evaluators","text":"<p>NumerBlox offers evaluators for both Numerai Classic and Numerai Signals.</p>"},{"location":"evaluation/#common-metrics","title":"Common Metrics","text":"<p>For both <code>NumeraiClassicEvaluator</code> and <code>NumeraiSignalsEvaluator</code> you can set a custom <code>metrics_list</code> with all metrics you want to compute.</p> <p>By default, metrics will include <code>[\"mean_std_sharpe\", \"apy\", \"max_drawdown\", \"calmar_ratio\"]</code></p> <p>All valid metrics for <code>metrics_list</code> are:</p> <ul> <li> <p>\"mean_std_sharpe\" -&gt; Mean, standard deviation and Sharpe ratio based on Corrv2 (Numerai Correlation).</p> </li> <li> <p>\"apy\" -&gt; Annual Percentage Yield.</p> </li> <li> <p>\"max_drawdown\" -&gt; Max drawdown.</p> </li> <li> <p>\"calmar_ratio\" -&gt; Calmar Ratio.</p> </li> <li> <p>\"autocorrelation\" -&gt; Autocorrelation (1st order).</p> </li> <li> <p>\"max_feature_exposure\" -&gt; Max feature exposure.</p> </li> <li> <p>\"smart_sharpe\" -&gt; Smart Sharpe.</p> </li> <li> <p>\"legacy_mean_std_sharpe\" -&gt; Mean, standard deviation and Sharpe ratio based on legacy model contribution.</p> </li> <li> <p>\"fn_mean_std_sharpe\" -&gt; Feature Neutral mean, standard deviation and Sharpe ratio (can take some time to compute).</p> </li> <li> <p>\"tb200_mean_std_sharpe\" -&gt; Mean, standard deviation and Sharpe ratio based on TB200.</p> </li> <li> <p>\"tb500_mean_std_sharpe\" -&gt; Mean, standard deviation and Sharpe ratio based on TB500.</p> </li> </ul> <p>The following metrics only work if <code>benchmark_cols</code> are defined in <code>full_evaluation</code>:</p> <ul> <li> <p>\"mc_mean_std_sharpe\" -&gt; Mean, standard deviation and Sharpe ratio based on model contribution.</p> </li> <li> <p>\"corr_with\" -&gt; Correlation with benchmark predictions.</p> </li> <li> <p>\"ex_diss_pearson\" (alias \"ex_diss\") -&gt; Exposure Dissimilarity to benchmark predictions using Pearson correlation.</p> </li> <li> <p>\"ex_diss_spearman\" -&gt; Exposure Dissimilarity to benchmark predictions using Spearman correlation. Will be slower compared to \"ex_diss_pearson\".</p> </li> </ul>"},{"location":"evaluation/#numerai-classic-specific-metrics","title":"Numerai Classic specific metrics","text":"<p><code>NumeraiClassicEvaluator</code> can also compute FNCv3. If you want to compute this add <code>fncv3_mean_std_sharpe</code> to the <code>metrics_list</code>.</p> <pre><code>from numerblox.evaluation import NumeraiClassicEvaluator, FAST_METRICS\n\n# Validation DataFrame to compute metrics on\n# Should have at least era_col, pred_cols and target_col columns.\nval_df = ...\n\nevaluator = NumeraiClassicEvaluator(era_col=\"era\", metrics_list=FAST_METRICS)\nmetrics = evaluator.full_evaluation(val_df, \n                                    pred_cols=[\"prediction\"], \n                                    target_col=\"target\",\n                                    benchmark_cols=[\"benchmark1\", \"benchmark2\"])\n</code></pre>"},{"location":"evaluation/#numerai-signals-specific-metrics","title":"Numerai Signals specific metrics","text":"<p><code>NumeraiSignalsEvaluator</code> offers Numerai Signals diagnostics scores. This is a special operation as it calls on Numerai servers and needs additional authentication, so it is not included in <code>full_evaluation</code>.</p> <p>Example of how to get diagnostic scores for Numerai Signals:</p> <pre><code>from numerblox.misc import Key\nfrom numerblox.evaluation import NumeraiSignalsEvaluator\n\nevaluator = NumeraiSignalsEvaluator()\n\n# A Numerai Signals model name you use.\nmodel_name = \"MY_MODEL\"\n# NumerBlox Key for accessing the Numerai API\nkey = Key(pub_id=\"Hello\", secret_key=\"World\")\n# DataFrame with validation data containing prediction, friday_date, ticker and data_type columns\nval_df = pd.DataFrame()\n\nevaluator.get_neutralized_corr(val, model_name=model_name, key=key, corr_col=\"validationRic\")\n# Returns a Pandas DataFrame with validationRic.\n</code></pre>"},{"location":"evaluation/#custom-functions","title":"Custom functions","text":"<p>In addition to the default metrics, evaluators can be augmented with custom metrics. This can be done by defining a dictionary of functions and arguments.</p> <p>The custom function dictionary should have the following structure:</p> <pre><code>{\n    \"func1\": # Metric name\n    {\n        \"func\": custom_function,  # Function to call\n        \"args\": { # General arguments (can be any type)\n            \"dataf\": \"dataf\",\n            \"some_arg\": \"some_arg\",\n        },\n        \"local_args\": [\"dataf\"]  # List of local variables to use/resolve\n    },\n    \"func2\":\n    {\n        \"func\": custom_function2,\n        \"args\": { \n            \"dataf\": \"dataf\",\n            \"some_arg\": \"some_arg\",\n        },\n        \"local_args\": [\"dataf\"]\n    },\n    (...)\n}\n</code></pre> <ul> <li> <p>The main keys (<code>func1</code> and <code>func2</code> in the example) will be the metric key names for the output evaluation DataFrame.</p> </li> <li> <p>The <code>func</code> key should be a function that takes in the arguments defined in <code>args</code> as keyword arguments. <code>func</code> should be a callable function or class (i.e. class that implements <code>__call__</code>).</p> </li> <li> <p>The <code>args</code> key should be a dictionary with arguments to pass to <code>func</code>. The values of the dictionary can be any type. Arguments that you want resolved as local variables should be defined as strings (see <code>local_args</code> explanation).</p> </li> <li> <p>The <code>local_args</code> key should be a list of strings that refer to variables that exist locally in the evaluation_one_col function. These local variables will be resolved to local variables for <code>func</code>. This allows you to use evaluation_one_col variables like <code>dataf</code>, <code>pred_col</code>, <code>target_col</code>, <code>col_stats</code>, <code>mean</code>, <code>per_era_numerai_corrs</code>, etc.</p> </li> </ul> <p>Example of how to use custom functions in <code>NumeraiClassicEvaluator</code>:</p> <pre><code>from numerblox.evaluation import NumeraiClassicEvaluator\n\ndef residuals(dataf, target_col, pred_col, val: int):\n    \"\"\" Simple dummy func: mean of residuals. \"\"\"\n    return np.mean(dataf[target_col] - dataf[pred_col] + val)\n\ncustom_functions = {\n        \"residuals\": {\n            # Callable function\n            \"func\": residuals,\n            \"args\": {\n                # String referring to local variables\n                \"dataf\": \"dataf\", \n                \"pred_col\": \"pred_col\",\n                \"target_col\": \"target_col\",\n                # Static argument\n                \"val\": 0.0001,\n            },\n             # List of local variables to use/resolve\n            \"local_args\": [\"dataf\", \"pred_col\", \"target_col\"] \n        },\n}\n\nevaluator = NumeraiClassicEvaluator(custom_functions=custom_functions)\n\n# In evaluator residuals(dataf=dataf, pred_col=\"prediction\", target_col=\"target\", val=\"0.0001) is called.\nmetrics = evaluator.full_evaluation(val_df, \n                                    pred_cols=[\"prediction\"], \n                                    target_col=\"target\")\n# metrics will contain a \"residuals\" column.\n</code></pre>"},{"location":"meta/","title":"Meta Estimators","text":"<p>Meta estimator wrap existing scikit-learn estimators to provide additional functionality. Currently, the following meta estimators are available:</p> <ul> <li>CrossValEstimator</li> <li>MetaPipeline</li> </ul>"},{"location":"meta/#crossvalestimator","title":"CrossValEstimator","text":"<p><code>CrossValEstimator</code> provides a way to integrate cross-validation directly into model training, enabling simultaneous fitting of multiple models across data folds. By doing this, you can fit it as one transformer and get outputs for each fold during the prediction phase.</p>"},{"location":"meta/#why-crossvalestimator","title":"Why CrossValEstimator?","text":"<ul> <li> <p>Holistic Training: Cross-validation offers a more robust model training process by leveraging multiple sub-sets of your data. This way, your model's performance is less susceptible to the peculiarities of any single data split.</p> </li> <li> <p>Inherent Ensemble: By training on multiple folds, you're essentially building an ensemble of models. Ensembles often outperform individual models since they average out biases, reduce variance, and are less likely to overfit.</p> </li> <li> <p>Custom Evaluation: With the <code>evaluation_func</code> parameter, you can input your custom evaluation logic, allowing for flexible and tailored performance assessment for each fold.</p> </li> <li> <p>Flexibility with Predictions: Choose between different prediction functions like 'predict', 'predict_proba', and 'predict_log_proba' using the <code>predict_func</code> parameter.</p> </li> <li> <p>Verbose Logging: Gain insights into the training process with detailed logs during the fitting phase, aiding in debugging and understanding model performance across folds.</p> </li> </ul>"},{"location":"meta/#example","title":"Example","text":"<pre><code>from sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor\n\nfrom numerblox.meta import CrossValEstimator\n\n# Define the cross-validation strategy\ncv = KFold(n_splits=5)\n\n# Initialize the estimator\nestimator = XGBRegressor(n_estimators=100, max_depth=3)\n\n# (optional) Define a custom evaluation function\ndef custom_eval(y_true, y_pred):\n    return {\"mse\": ((y_true - y_pred) ** 2).mean()}\n\n# Initialize the CrossValEstimator\ncross_val_estimator = CrossValEstimator(cv=cv, \n                                        estimator=estimator,\n                                        evaluation_func=custom_eval)\n\n# Fit the CrossValEstimator\ncross_val_estimator.fit(X_train, y_train)\npredictions = cross_val_estimator.predict(X_test)\n</code></pre>"},{"location":"meta/#metapipeline","title":"MetaPipeline","text":"<p>The <code>MetaPipeline</code> extends the functionality of scikit-learn's <code>Pipeline</code> by seamlessly integrating models and post-model transformations. It empowers you to employ sophisticated data transformation techniques not just before, but also after your model's predictions. This is particularly useful when post-processing predictions, such as neutralizing feature exposures in financial models.</p>"},{"location":"meta/#why-metapipeline","title":"Why MetaPipeline?","text":"<ul> <li> <p>Post-Model Transformations: It can be crucial to apply transformations, like feature neutralization, after obtaining predictions. <code>MetaPipeline</code> facilitates such operations, leading to improved model generalization and stability.</p> </li> <li> <p>Streamlined Workflow: Instead of managing separate sequences for transformations and predictions, you can orchestrate them under a single umbrella, simplifying both development and production workflows.</p> </li> <li> <p>Flexible Integration: <code>MetaPipeline</code> gracefully handles a variety of objects, including <code>Pipeline</code>, <code>FeatureUnion</code>, and <code>ColumnTransformer</code>. This makes it a versatile tool adaptable to diverse tasks and data structures.</p> </li> </ul>"},{"location":"meta/#example_1","title":"Example","text":"<p>Consider a scenario where you have an <code>XGBRegressor</code> model and want to apply a <code>FeatureNeutralizer</code> after obtaining the model's predictions:</p> <pre><code>from xgboost import XGBRegressor\nfrom numerblox.meta import MetaPipeline \nfrom numerblox.neutralizers import FeatureNeutralizer\n\n# Define MetaPipeline steps\nsteps = [\n    ('xgb_regressor', XGBRegressor(n_estimators=100, max_depth=3)),\n    ('feature_neutralizer', FeatureNeutralizer(proportion=0.5))\n]\n\n# Create MetaPipeline\nmeta_pipeline = MetaPipeline(steps)\n\n# Train and predict using MetaPipeline\nmeta_pipeline.fit(X_train, y_train)\npredictions = meta_pipeline.predict(X_test)\n</code></pre> <p>For a more succinct creation of a <code>MetaPipeline</code>, you can use the <code>make_meta_pipeline</code> function:</p> <pre><code>from numerblox.meta import make_meta_pipeline\n\npipeline = make_meta_pipeline(XGBRegressor(n_estimators=100, max_depth=3),\n                              FeatureNeutralizer(proportion=0.5))\n</code></pre>"},{"location":"models/","title":"Models","text":""},{"location":"models/#eraboostedxgbregressor","title":"EraBoostedXGBRegressor","text":"<p><code>EraBoostedXGBRegressor</code> is a custom regressor extending the functionality of XGBoost, aimed at improving accuracy on specific eras in a dataset. It upweights the eras that are toughest to fit. It is designed to integrate seamlessly with scikit-learn.</p>"},{"location":"models/#why","title":"Why?","text":"<ul> <li>Era-Specific Focus: Targets the worst-performing eras in your data for performance enhancement, ensuring that the model improves where it is most needed.</li> <li>Scikit-learn integration: <code>EraBoostedXGBRegressor</code> is designed to integrate seamlessly with scikit-learn.</li> <li>Customization Options: Offers flexibility to adjust the proportion of eras to focus on, the number of trees added per iteration, and the total number of iterations for era boosting.</li> </ul>"},{"location":"models/#quickstart","title":"Quickstart","text":"<p>Make sure to include the era column as a <code>pd.Series</code> in the <code>fit</code> method.</p> <pre><code>from numerblox.models import EraBoostedXGBRegressor\n\nmodel = EraBoostedXGBRegressor(proportion=0.5, trees_per_step=10, num_iters=20)\nmodel.fit(X=X_train, y=y_train, eras=eras_train)\n\npredictions = model.predict(X_live)\n</code></pre>"},{"location":"numerframe/","title":"NumerFrame","text":"<p><code>NumerFrame</code> is an extension of <code>pd.DataFrame</code> tailored specifically for the data format and workflow commonly used by Numerai participants. It builds upon the base functionalities of a Pandas DataFrame by offering utilities that simplify working with Numerai datasets.</p>"},{"location":"numerframe/#why","title":"Why?","text":"<ul> <li> <p>Intuitive Data Handling: With built-in features like <code>get_feature_data</code>, <code>get_target_data</code>, and more, it simplifies extracting data subsets specific to Numerai competitions.</p> </li> <li> <p>Automated Column Grouping: Automatically parses columns into recognizable groups such as features, targets, predictions, making data retrieval more intuitive and less error-prone.</p> </li> <li> <p>Support for Multiple Formats: Through <code>create_numerframe</code>, it supports initializing from various data formats such as CSV, Parquet, Excel, and Pickle, providing a flexible interface for users.</p> </li> <li> <p>Optimized for Numerai: Whether you're trying to fetch specific eras, feature groups or patterns like all 20-day targets, <code>NumerFrame</code> is designed to simplify those tasks for Numerai participants.</p> </li> <li> <p>Chainable Operations: Since most operations return another <code>NumerFrame</code>, they can be conveniently chained for more complex workflows.</p> </li> <li> <p>Tailored for Machine Learning: With methods like <code>get_feature_target_pair</code>, it aids in easily splitting the data for machine learning tasks specific to the Numerai competition.</p> </li> </ul> <p>By using <code>NumerFrame</code>, participants can focus more on model development and less on data wrangling, leading to a smoother and more efficient workflow in the Numerai competition.</p>"},{"location":"numerframe/#initialization","title":"Initialization","text":"<p>A NumerFrame can be initialized either from an existing <code>pd.DataFrame</code> or with <code>create_numerframe</code>. The <code>create_numerframe</code> function takes a path to a file and returns a <code>NumerFrame</code> object. This function automatically parses the file and supports CSV, Parquet, Excel and Pickle formats.</p> <p><code>NumerFrame</code> automatically parses columns into groups so you can easily retrieve what you need. It automatically is aware of the <code>era</code> column for its operations. </p> <p><code>NumerFrame</code> follows a convention for feature groups.</p> <ul> <li> <p>Features are all columns that start with <code>feature</code>.</p> </li> <li> <p>Targets are all columns that start with <code>target</code>.</p> </li> <li> <p>Predictions are all columns that start with <code>prediction</code>.</p> </li> <li> <p>Aux columns are all that fall in none of these buckets, like <code>era</code>, <code>data_type</code> and <code>id</code>. </p> </li> <li> <p>Era column is either <code>era</code>, <code>date</code> or <code>friday_date</code>.</p> </li> </ul> <pre><code>import pandas as pd\nfrom numerblox.numerframe import NumerFrame, create_numerframe\n# From DataFrame\ndata = pd.read_parquet('train.parquet')\ndf = NumerFrame(data)\n\n# With create_numerframe\ndf = create_numerframe('train.parquet')\n</code></pre>"},{"location":"numerframe/#examples","title":"Examples","text":"<p>Basic functionality: </p> <pre><code># Get data for features, targets, predictions, and aux\nfeatures = df.get_feature_data\ntargets = df.get_target_data\npredictions = df.get_prediction_data\naux_data = df.get_aux_data\n</code></pre> <p>Additionally it is possible to get groups specific to Numerai Classic like FNCv3 and internal feature groups. The examples below show some advanced functionality in <code>NumerFrame</code>.</p> <pre><code># Get data for features, targets and predictions\nfeatures = df.get_feature_data\ntargets = df.get_target_data\npredictions = df.get_prediction_data\n\n# Get specific data groups\nfncv3_features = df.get_fncv3_feature_data\ngroup_features = df.get_group_features(group='rain')\nsmall_features = df.get_small_feature_data\nmedium_features = df.get_medium_feature_data\n\n# Fetch columns by pattern. For example all 20 day targets.\npattern_data = df.get_pattern_data(pattern='_20')\n# Or for example Jerome targets.\njerome_targets = df.get_pattern_data(pattern='_jerome_')\n\n# Split into feature and target pairs. Will get single target by default.\nX, y = df.get_feature_target_pair()\n# Optionally get all targets\nX, y = df.get_feature_target_pair(multi_target=True)\n\n# Fetch data for specified eras\nX, y = df.get_era_batch(eras=['0001', '0002'])\n# Optionally get Tensorflow tensors for NN training\nX, y = nf.get_era_batch(eras=['0001', '0002'], convert_to_tf=True)\n\n# Since every operation returns a NumerFrame they can be chained.\n# An example chained operation is getting features and targets for the last 2 eras.\nX, y = df.get_last_eras(2).get_feature_target_pair()\n</code></pre>"},{"location":"postprocessing/","title":"Postprocessing","text":""},{"location":"postprocessing/#feature-neutralization","title":"Feature Neutralization","text":"<p><code>FeatureNeutralizer</code> provides classic feature neutralization by subtracting linear model influence, ensuring that predictions are not overly influenced by a specific set of features.</p>"},{"location":"postprocessing/#why","title":"Why?","text":"<ul> <li>Reduce Overfitting: By neutralizing predictions, you can potentially reduce the risk of overfitting to specific feature characteristics.</li> <li>Control Feature Influence: Allows you to have a granular control on how much influence a set of features can exert on the final predictions. </li> <li>Enhance Model Robustness: By limiting the influence of potentially noisy or unstable features, you might improve the robustness of your model's predictions across different data periods.</li> </ul>"},{"location":"postprocessing/#quickstart","title":"Quickstart","text":"<p>Make sure to pass both the features to use for penalization as a <code>pd.DataFrame</code> and the accompanying era column as a <code>pd.Series</code> to the <code>predict</code> method.</p> <p>Additionally, <code>pred_name</code> and <code>proportion</code> can be lists. In this case, the neutralization will be performed for each prediction name and proportion. For example, if <code>pred_name=[\"prediction1\", \"prediction2\"]</code> and <code>proportion=[0.5, 0.7]</code>, then the result will be an array with 4 neutralized prediction columns. All neutralizations will be performed in parallel.</p> <p>Single column neutralization:</p> <pre><code>import pandas as pd\nfrom numerblox.neutralizers import FeatureNeutralizer\n\npredictions = pd.Series([0.24, 0.87, 0.6])\nfeature_data = pd.DataFrame([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\nera_data = pd.Series([1, 1, 2])\n\nneutralizer = FeatureNeutralizer(pred_name=\"prediction\", proportion=0.5)\nneutralizer.fit()\nneutralized_predictions = neutralizer.predict(X=predictions, features=feature_data, eras=era_data)\n</code></pre> <p>Multiple column neutralization:</p> <pre><code>import pandas as pd\nfrom numerblox.neutralizers import FeatureNeutralizer\n\npredictions = pd.DataFrame({\"prediction1\": [0.24, 0.87, 0.6], \"prediction2\": [0.24, 0.87, 0.6]})\nfeature_data = pd.DataFrame([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\nera_data = pd.Series([1, 1, 2])\n\nneutralizer = FeatureNeutralizer(pred_name=[\"prediction1\", \"prediction2\"], proportion=[0.5, 0.7])\nneutralizer.fit()\nneutralized_predictions = neutralizer.predict(X=predictions, features=feature_data, eras=era_data)\n</code></pre>"},{"location":"postprocessing/#featurepenalizer","title":"FeaturePenalizer","text":"<p><code>FeaturePenalizer</code> neutralizes predictions using TensorFlow based on provided feature exposures. It's designed to integrate seamlessly with scikit-learn.</p>"},{"location":"postprocessing/#why_1","title":"Why?","text":"<ul> <li>Limit Feature Exposure: Ensures that predictions are not excessively influenced by any individual feature, which can help in achieving more stable predictions.</li> <li>Enhanced Prediction Stability: By penalizing high feature exposures, it might lead to more stable and consistent predictions across different eras or data splits.</li> <li>Mitigate Model Biases: If a model is relying too heavily on a particular feature, penalizing can help in balancing out the biases and making the model more generalizable.</li> </ul>"},{"location":"postprocessing/#quickstart_1","title":"Quickstart","text":"<p>Make sure to pass both the features to use for penalization as a <code>pd.DataFrame</code> and the accompanying era column as a <code>pd.Series</code> to the <code>predict</code> method.</p> <pre><code>from numerblox.penalizers import FeaturePenalizer\n\npredictions = pd.Series([0.24, 0.87, 0.6])\nfeature_data = pd.DataFrame([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\nera_data = pd.Series([1, 1, 2])\n\npenalizer = FeaturePenalizer(max_exposure=0.1, pred_name=\"prediction\")\npenalizer.fit(X=predictions)\npenalized_predictions = penalizer.predict(X=predictions, features=feature_data, eras=era_data)\n</code></pre>"},{"location":"prediction_loaders/","title":"Prediction Loaders","text":"<p>Prediction loaders are designed to seamlessly fetch and transform prediction data, especially from Numerai's API. These classes can be integrated into pipelines to automate the prediction generation process for the Numerai competition.</p>"},{"location":"prediction_loaders/#why","title":"Why?","text":"<p>Numerai provides example predictions to help participants understand the expected structure and format of predictions. With the ExamplePredictions class, you can easily fetch these example predictions for different data versions, allowing you to quickly evaluate or test your models against the Numerai's standard prediction dataset.</p>"},{"location":"prediction_loaders/#examplepredictions","title":"ExamplePredictions","text":""},{"location":"prediction_loaders/#usage","title":"Usage:","text":"<p>The <code>ExamplePredictions</code> class fetches the example predictions for the specified version of the Numerai dataset. This can be useful for testing or understanding the prediction structure and data distribution.</p> <p>Downloaded files are automatically cleaned up after data is loaded with the <code>transform</code> method. To keep the files make sure to set <code>keep_files=True</code> when instantiating the class.</p> <pre><code>from numerblox.prediction_loaders import ExamplePredictions\n# Instantiate and load example predictions for v4.3\nexample_loader = ExamplePredictions(file_name=\"v4.3/live_example_preds.parquet\", keep_files=False)\nexample_preds_df = example_loader.transform()\n</code></pre> <p>Besides the v4.3 data you can also retrieve example preds from earlier datasets. Check Numerai's data page to see which datasets are supported.</p> <pre><code>from numerblox.prediction_loaders import ExamplePredictions\nexample_loader_v42 = ExamplePredictions(file_name=\"v4.2/live_example_preds.parquet\", keep_files=True)\nexample_preds_v42_df = example_loader_v41.transform()\n</code></pre>"},{"location":"preprocessing/","title":"Preprocessors","text":"<p>NumerBlox offers a suite of preprocessors to easily do Numerai specific data transformations. All preprocessors are compatible with <code>scikit-learn</code> pipelines and feature a similar API. Note that some preprocessors may require an additional <code>eras</code> or <code>tickers</code> argument in the <code>transform</code> step.</p>"},{"location":"preprocessing/#numerai-classic","title":"Numerai Classic","text":""},{"location":"preprocessing/#groupstatspreprocessor","title":"GroupStatsPreProcessor","text":"<p>The v4.2 (rain) dataset for Numerai Classic reintroduced feature groups. The <code>GroupStatsPreProcessor</code> calculates group statistics for all data groups. It uses predefined feature group mappings to generate statistical measures (mean, standard deviation, skew) for each of the feature groups. </p>"},{"location":"preprocessing/#example","title":"Example","text":"<p>Here's how you can use the <code>GroupStatsPreProcessor</code>:</p> <pre><code>from numerblox.preprocessing import GroupStatsPreProcessor\ngroup_processor = GroupStatsPreProcessor(groups=['intelligence'])\n\n# Return features with group statistics for the 'intelligence' group\nfeatures = group_processor.transform(X)\n</code></pre>"},{"location":"preprocessing/#numerai-signals","title":"Numerai Signals","text":""},{"location":"preprocessing/#reducememoryprocessor","title":"ReduceMemoryProcessor","text":"<p>The <code>ReduceMemoryProcessor</code> reduces the memory usage of the data as much as possible. It's particularly useful for Numerai Signals dataset which can be quite large.</p> <p>Note that modern Numerai Classic Data (v4.2+) already is an int8 format so this processor will be not be useful for Numerai Classic.</p> <pre><code>from numerblox.preprocessing import ReduceMemoryProcessor\n\nprocessor = ReduceMemoryProcessor(deep_mem_inspect=True, verbose=True)\nreduced_data = processor.fit_transform(dataf)\n</code></pre>"},{"location":"preprocessing/#katsufeaturegenerator","title":"KatsuFeatureGenerator","text":"<p><code>KatsuFeatureGenerator</code> performs feature engineering based on Katsu's starter notebook. This is useful for those participating in the Numerai Signals contest.</p> <p>You can specify custom windows that indicates how many days to look back when generating features.</p> <pre><code>from numerblox.preprocessing import KatsuFeatureGenerator\n\nfeature_gen = KatsuFeatureGenerator(windows=[7, 14, 21])\nenhanced_data = feature_gen.fit_transform(dataf)\n</code></pre>"},{"location":"preprocessing/#eraquantileprocessor","title":"EraQuantileProcessor","text":"<p><code>EraQuantileProcessor</code> transforms features into quantiles by era. This can help normalize data and make patterns more distinguishable.</p> <p>Using <code>.transform</code> requires passing the era column as a <code>pd.Series</code>. This is because the quantiles are calculated per era so it needs that information along with the raw input features.</p> <pre><code>from numerblox.preprocessing import EraQuantileProcessor\n\neq_processor = EraQuantileProcessor(num_quantiles=50, random_state=42)\ntransformed_data = eq_processor.fit_transform(X, eras=eras_series)\n</code></pre>"},{"location":"preprocessing/#tickermapper","title":"TickerMapper","text":"<p><code>TickerMapper</code> maps tickers from one format to another. Useful when working with data from multiple sources that have different ticker formats.</p> <p>For the default ticker mapper the following formats are supported: <code>['ticker', 'bloomberg_ticker', 'yahoo']</code>. You can also specify a custom mapping by passing a dictionary to the <code>mapper_path</code> argument at the instantiation.</p> <pre><code>from numerblox.preprocessing import TickerMapper\n\n# Transform from ticker to Bloomberg format.\nticker_mapper = TickerMapper(ticker_col=\"ticker\", target_ticker_format=\"bloomberg_ticker\")\nmapped_data = ticker_mapper.transform(dataf[\"ticker\"])\n</code></pre>"},{"location":"preprocessing/#lagpreprocessor","title":"LagPreProcessor","text":"<p><code>LagPreProcessor</code> generates lag features based on specified windows. Lag features can capture temporal patterns in time-series data.</p> <p>Note that <code>LagPreProcessor</code> needs a series of <code>tickers</code> in the <code>.transform</code> step.</p> <pre><code>from numerblox.preprocessing import LagPreProcessor\n\nlag_processor = LagPreProcessor(windows=[5, 10, 20])\nlag_processor.fit(X)\nlagged_data = lag_processor.transform(X, tickers=tickers_series)\n\n</code></pre>"},{"location":"preprocessing/#differencepreprocessor","title":"DifferencePreProcessor","text":"<p><code>DifferencePreProcessor</code> computes the difference between features and their lags. It's used after <code>LagPreProcessor</code>.</p> <p>WARNING: <code>DifferencePreProcessor</code> works only on <code>pd.DataFrame</code> and with columns that are generated in <code>LagPreProcessor</code>. If you are using these in a Pipeline make sure <code>LagPreProcessor</code> is defined before <code>DifferencePreProcessor</code> and that output API is set to Pandas (<code>pipeline.set_output(transform=\"pandas\")</code>).</p> <p>Note that <code>LagPreProcessor</code> needs a series of <code>tickers</code> in the <code>.transform</code> step so a pipeline with both preprocessors will need a <code>tickers</code> argument in <code>.transform</code>.</p> <pre><code>from sklearn.pipeline import make_pipeline\nfrom numerblox.preprocessing import DifferencePreProcessor\n\nlag = LagPreProcessor(windows=[5, 10])\ndiff = DifferencePreProcessor(windows=[5, 10], pct_diff=True)\npipe = make_pipeline(lag, diff)\npipe.set_output(transform=\"pandas\")\npipe.fit(X)\ndiff_data = pipe.transform(X, tickers=tickers_series)\n</code></pre>"},{"location":"preprocessing/#pandastafeaturegenerator","title":"PandasTaFeatureGenerator","text":"<p><code>PandasTaFeatureGenerator</code> uses the <code>pandas-ta</code> library to generate technical analysis features. It's a powerful tool for those interested in financial time-series data.</p> <p>Make sure you have <code>pandas-ta</code> installed before using this feature generator:</p> <pre><code>!pip install pandas-ta\n</code></pre> <p>Currently <code>PandasTaFeatureGenerator</code> only works on <code>pd.DataFrame</code> input. Its input is a DataFrame with columns <code>[ticker, date, open, high, low, close, volume]</code>.</p> <pre><code>from numerblox.preprocessing import PandasTaFeatureGenerator\n\nta_gen = PandasTaFeatureGenerator()\nta_features = ta_gen.transform(dataf)\n</code></pre>"},{"location":"preprocessing/#minimumdatafilter","title":"MinimumDataFilter","text":"<p><code>MinimumDataFilter</code> filters out dates and tickers that don't have enough data. For example, it makes sense to filter out dates for which you have less than 100 days of data. Also, dates that have less than 100 unique tickers can be filtered out.</p> <p>Additionally, you can specify a list of tickers to blacklist and exclude from your data.</p> <p>NOTE: This step only works with DataFrame input.</p> <pre><code>from numerblox.preprocessing import MinimumDataFilter\n\nmin_data_filter = MinimumDataFilter(min_samples_date=200, min_samples_ticker=1200, blacklist_tickers=[\"SOMETICKER.BLA\"])\nfiltered_data = min_data_filter.fit_transform(dataf)\n</code></pre>"},{"location":"preprocessing/#rolling-your-own-preprocessor","title":"Rolling your own preprocessor","text":"<p>We invite the community to contribute their own preprocessors to NumerBlox. If you have a preprocessor that you think would be useful to others, please open a PR with your code and tests. The new preprocessor should adhere to scikit-learn conventions. Here are some the most important things to keep in mind and a template.</p> <ul> <li>Make sure that your preprocessor inherits from <code>numerblox.preprocessing.base.BasePreProcessor</code>. This will automatically implement a blank fit method. It will also inherit from <code>sklearn.base.TransformerMixin</code> and <code>sklearn.base.BaseEstimator</code>.</li> <li>Make sure your preprocessor implements a <code>transform</code> method that can take a <code>np.array</code> or <code>pd.DataFrame</code> as input and outputs an <code>np.array</code>. If your preprocessor can only work with <code>pd.DataFrame</code> input, mention this explicitly in the docstring.</li> <li>Implement a <code>get_feature_names_out</code> method so it can support <code>pd.DataFrame</code> output with valid column names.</li> </ul> <pre><code>import numpy as np\nimport pandas as pd\nfrom typing import Union\nfrom sklearn.validation import check_is_fitted, check_X_y\nfrom numerblox.preprocessing.base import BasePreProcessor\n\nclass MyAwesomePreProcessor(BasePreProcessor):\n    def __init__(self, random_state: int = 0):\n        super().__init__()\n        # If you introduce additional arguments be sure to add them as attributes.\n        self.random_state = random_state\n\n    def fit(self, X: Union[np.array, pd.DataFrame], y=None):\n        # Arguments can be set for later use.\n        self.n_cols_ = X.shape[1]\n        return self\n\n    def transform(self, X: Union[np.array, pd.DataFrame]) -&gt; np.array:\n        # Do your preprocessing here.\n        # Can involve additional checks.\n        check_is_fitted(self)\n        X = check_X_y(X)\n        return X\n\n    def get_feature_names_out(self, input_features=None) -&gt; list:\n        # Return a list of feature names.\n        # If you are not using pandas output, you can skip this method.\n        check_is_fitted(self)\n        return [\"awesome_output_feature_{i}\" for i in range(self.n_cols_)]\n</code></pre>"},{"location":"submission/","title":"Submitters","text":"<p>NumerBlox provides submitters for both Numerai Classic and Signals.  Also check out <code>example/submitting.ipynb</code> for more information on Numerai submission.</p>"},{"location":"submission/#why","title":"Why?","text":"<ul> <li> <p>Simplified Workflow: Instead of managing multiple manual steps for submissions, <code>Submitters</code> allow you to simplify the submission process down to a few lines of code.</p> </li> <li> <p>Integrated Validation Checks: Before submitting, <code>Submitters</code> performs a series of checks to ensure the submission format is correct and prevent common mistakes that could lead to invalid submissions.</p> </li> <li> <p>Security: By providing a way to load credentials from a <code>.json</code> file, <code>Submitters</code> ensures that you're not hard-coding your secret credentials in the main code, reducing the risk of accidental exposure.</p> </li> <li> <p>Automatic Cleanup: For users who run automated jobs, the ability to automatically clean up the environment post-submission ensures that your workspace remains clutter-free.</p> </li> </ul> <p>With <code>Submitters</code>, you can focus more on developing and refining your model and spend less time on the manual aspects of the submission process.</p>"},{"location":"submission/#instantiation","title":"Instantiation","text":"<p>In order to use a Submitter you should first create a <code>Key</code> object which handles credentials. There are two ways to create a <code>Key</code>:</p> <p>1. Initialize <code>Key</code> with <code>pub_id</code> and <code>secret_key</code> from memory.</p> <pre><code>from numerblox.misc import Key\nkey = Key(pub_id=\"Hello\", secret_key=\"World\")\n</code></pre> <p>2. Load credentials from <code>.json</code> file with <code>load_key_from_json</code>.</p> <p>JSON file should have the following format:</p> <pre><code>{\"pub_id\": \"PUBLIC_ID\", \"secret_key\": \"SECRET_KEY\"}\n</code></pre> <p>We recommend loading from <code>.json</code>. With this method you only have to save your credentials in one (safe) place and avoid leaving reference to a secret key in Python code.</p> <pre><code>from numerblox.misc import load_key_from_json\nkey = load_key_from_json(\"my_credentials.json\")\n</code></pre>"},{"location":"submission/#numerai-classic","title":"Numerai Classic","text":"<p>Submissions can be done in 2 lines of code. To initialize the submitter object, pass a directory path for saving submissions and a <code>Key</code> object.</p> <p><code>NumeraiClassicSubmitter.full_submission</code> will perform:  1. Checks to prevent surprise behavior (including value range and column validity)  2. Saving to CSV  3. Uploading with <code>numerapi</code>.</p> <p>The <code>dataf</code> argument can be either a <code>pd.DataFrame</code> or <code>NumerFrame</code>.</p> <pre><code>from numerblox.submission import NumeraiClassicSubmitter\nsubmitter = NumeraiClassicSubmitter(directory_path=\"sub_current_round\", key=key)\n# Your prediction file with 'id' as index and defined 'cols' below.\ndataf = pd.DataFrame(columns=[\"prediction\"])\n# Only works with valid key credentials and model_name\nsubmitter.full_submission(dataf=dataf,\n                          cols=\"prediction\",\n                          file_name=\"submission.csv\",\n                          model_name=\"my_model\")\n</code></pre>"},{"location":"submission/#numerai-signals","title":"Numerai Signals","text":"<p><code>NumeraiSignalsSubmitter</code> is very similar to <code>NumeraiClassicSubmitter</code>, but has a few additional checks specific to Signals. Mainly, it checks if the data contains a valid ticker column (<code>\"cusip\"</code>, <code>\"sedol\"</code>, <code>\"ticker\"</code>, <code>\"numerai_ticker\"</code> or <code>\"bloomberg_ticker\"</code>) and a <code>'signal'</code> column.</p> <p><code>NumeraiSignalsSubmitter.full_submission</code> handles checks, saving of CSV and uploading with <code>numerapi</code>.</p> <pre><code>from numerblox.submission import NumeraiSignalsSubmitter\nsubmitter = NumeraiSignalsSubmitter(directory_path=\"sub_current_round\", key=key)\n# Your prediction file with 'id' as index, a valid ticker column and signal column below.\ndataf = pd.DataFrame(columns=['bloomberg_ticker', 'signal'])\n# Only works with valid key credentials and model_name\nsubmitter.full_submission(dataf=dataf,\n                          cols=[\"bloomberg_ticker\", \"signal\"],\n                          file_name=\"submission.csv\",\n                          model_name=\"my_model\")\n</code></pre>"},{"location":"submission/#numerbay","title":"NumerBay","text":"<p>NumerBlox also offers functionality to submit predictions from NumerBay. This is a marketplace where Numerai predictions are bought and sold. Uploading from Numerbay is similar, but also requires authentication with your NumerBay account.</p> <p>Also make sure the <code>numerbay</code> library is installed.</p> <pre><code>pip install numerbay\n</code></pre> <pre><code>from numerblox.submission import NumeraiClassicSubmitter, NumerBaySubmitter\n# Your prediction DataFrame\ndataf = pd.DataFrame(columns=[\"prediction\"])\n\n# Full submission to both Numerai and NumerBay\nnumerbay_submitter = NumerBaySubmitter(\n    tournament_submitter = NumeraiClassicSubmitter(directory_path=\"sub_current_round\", key=key),\n    numerbay_username=\"yourusername\",\n    numerbay_password=\"yourpassword\"\n)\nnumerbay_submitter.full_submission(\n    dataf=dataf,\n    model_name=\"my_model\",\n    numerbay_product_full_name=\"numerai-predictions-yourproductname\",\n    file_name=\"submission.csv\"\n)\n</code></pre>"},{"location":"submission/#note","title":"Note","text":"<p>When you are done with submissions and don't need the submission file you can remove the submission directory with 1 line. Convenient if you have automated jobs and want to avoid clutter due to saving submission files for every round.</p> <pre><code># Clean up environment\nsubmitter.remove_base_directory()\n</code></pre>"},{"location":"targets/","title":"Target Engineering","text":"<p>Target engineering object allows you to easily create synthetic targets to train on or to convert raw price data into Numerai-style targets.</p>"},{"location":"targets/#why","title":"Why?","text":"<ul> <li> <p>Enhanced Experimentation: The availability of synthetic targets through the <code>BayesianGMMTargetProcessor</code> allows modelers to test new algorithms, techniques, or strategies.</p> </li> <li> <p>Align with Numerai's Methodology: <code>SignalsTargetProcessor</code> ensures that the targets you use are consistent with Numerai's approach. This alignment boosts the relevance of your models, potentially leading to better performance in the competition.</p> </li> <li> <p>Versatility: With different windows and target types, <code>SignalsTargetProcessor</code> offers a rich set of features, allowing for a more nuanced approach to model training. By exploring different timeframes and target representations, you can gain a deeper understanding of the data's dynamics.</p> </li> <li> <p>Efficiency: Manually engineering features or creating synthetic targets can be time-consuming and error-prone. These processors automate intricate steps, saving time and ensuring accuracy.</p> </li> </ul> <p>By integrating these processors into your workflow, you can enhance your modeling capabilities, streamline experimentation, and align closer to Numerai's expectations.</p>"},{"location":"targets/#bayesiangmmtargetprocessor","title":"BayesianGMMTargetProcessor","text":"<p>The `BayesianGMMTargetProcessor`` generates synthetic targets based on a Bayesian Gaussian Mixture model. It's primarily used for creating fake targets, which are useful for experimenting and validating model structures without exposing true labels.</p>"},{"location":"targets/#example","title":"Example:","text":"<pre><code>from numerblox.targets import BayesianGMMTargetProcessor\nprocessor = BayesianGMMTargetProcessor(n_components=3)\nprocessor.fit(X=train_features, y=train_targets, eras=train_eras)\nfake_target = processor.transform(X=train_features, eras=train_eras)\n</code></pre> <p>For more detailed examples and use-cases, check out <code>examples/synthetic_data_generation.ipynb.</code></p>"},{"location":"targets/#signalstargetprocessor","title":"SignalsTargetProcessor","text":"<p>The <code>SignalsTargetProcessor</code> is specifically designed to engineer targets for Numerai Signals. This involves converting raw price data into Numerai-style targets.</p>"},{"location":"targets/#example_1","title":"Example:","text":"<pre><code>from numerblox.target_processing import SignalsTargetProcessor\nprocessor = SignalsTargetProcessor(price_col=\"close\")\nsignals_target_data = processor.transform(dataf=data, eras=eras_column)\n</code></pre>"}]}