{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 1;\n                var nbb_unformatted_code = \"%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\\n%load_ext lab_black\";\n                var nbb_formatted_code = \"%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\\n%load_ext lab_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 2;\n                var nbb_unformatted_code = \"# default_exp model\";\n                var nbb_formatted_code = \"# default_exp model\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default_exp model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "This section implements functionality concerned with generating predictions for Numerai on preprocessed data.\n",
    "\n",
    "Currently supported frameworks and formats:\n",
    "1. `.joblib` (Common format to save Python objects. These models should have a `.predict` method. Especially convenient for [sklearn models](https://scikit-learn.org/stable/supervised_learning.html).)\n",
    "2. `.pickle`/`.pkl` (Arbitrary Python objects. All pickled models should have a `.predict` method.)\n",
    "3. `.cbm` (Easy format to load [CatBoost](https://catboost.ai/en/docs/) models.)\n",
    "4. `.lgb` (Format to load [LightGBM](https://lightgbm.readthedocs.io/en/latest/) models.)\n",
    "5. `.h5` ([tf.keras](https://keras.io/) models)\n",
    "6. Baseline models for which loading from files is not relevant (i.e. `ConstantModel` and `RandomModel`.)\n",
    "\n",
    "\n",
    "It is recommended to use models within `ModelPipeline`s (section 6), but they can also be used on its own.\n",
    "\n",
    "The last section of this notebook explains two different ways you can implement your own models for `numerai-blocks`:\n",
    "1. From `BaseModel` (custom prediction logic).\n",
    "2. From `DirectoryModel` (make predictions for all models in directory with given file suffix.\n",
    "Prediction logic will already be implemented. Only write model loading logic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 3;\n                var nbb_unformatted_code = \"# hide\\nfrom nbdev.showdoc import *\";\n                var nbb_formatted_code = \"# hide\\nfrom nbdev.showdoc import *\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 4;\n                var nbb_unformatted_code = \"#export\\nimport os\\nimport gc\\nimport uuid\\nimport wandb\\nimport joblib\\nimport pickle\\nimport numpy as np\\nimport pandas as pd\\nimport lightgbm as lgb\\nimport tensorflow as tf\\nfrom pathlib import Path\\nfrom typing import Union\\nfrom tqdm.auto import tqdm\\nfrom functools import partial\\nfrom catboost import CatBoost\\nfrom typeguard import typechecked\\nfrom abc import ABC, abstractmethod\\nfrom rich import print as rich_print\\nfrom sklearn.dummy import DummyRegressor\\n\\nfrom numerai_blocks.download import NumeraiClassicDownloader\\nfrom numerai_blocks.numerframe import NumerFrame, create_numerframe\\nfrom numerai_blocks.preprocessing import display_processor_info\";\n                var nbb_formatted_code = \"# export\\nimport os\\nimport gc\\nimport uuid\\nimport wandb\\nimport joblib\\nimport pickle\\nimport numpy as np\\nimport pandas as pd\\nimport lightgbm as lgb\\nimport tensorflow as tf\\nfrom pathlib import Path\\nfrom typing import Union\\nfrom tqdm.auto import tqdm\\nfrom functools import partial\\nfrom catboost import CatBoost\\nfrom typeguard import typechecked\\nfrom abc import ABC, abstractmethod\\nfrom rich import print as rich_print\\nfrom sklearn.dummy import DummyRegressor\\n\\nfrom numerai_blocks.download import NumeraiClassicDownloader\\nfrom numerai_blocks.numerframe import NumerFrame, create_numerframe\\nfrom numerai_blocks.preprocessing import display_processor_info\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "import os\n",
    "import gc\n",
    "import uuid\n",
    "import wandb\n",
    "import joblib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "from catboost import CatBoost\n",
    "from typeguard import typechecked\n",
    "from abc import ABC, abstractmethod\n",
    "from rich import print as rich_print\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from numerai_blocks.download import NumeraiClassicDownloader\n",
    "from numerai_blocks.numerframe import NumerFrame, create_numerframe\n",
    "from numerai_blocks.preprocessing import display_processor_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BaseModel` is an abstract base class that handles some directory logic and naming conventions. All models should inherit from `BaseModel` and be sure to implement the `.predict` method.\n",
    "\n",
    "In general, models are loaded in from disk. However, if no model files are involved in your model you should pass an empty string (`\"\"`) as the `model_directory` argument.\n",
    "\n",
    "Note that a new prediction column will have the column name `prediction_{MODEL_NAME}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 5;\n                var nbb_unformatted_code = \"#export\\nclass BaseModel(ABC):\\n    \\\"\\\"\\\"\\n    Setup for model prediction on a Dataset.\\n\\n    :param model_directory: Main directory from which to read in models.\\n    :param model_name: Name that will be used to create column names and for display purposes.\\n    \\\"\\\"\\\"\\n    def __init__(self, model_directory: str,\\n                 model_name: str = None,\\n                 ):\\n        self.model_directory = Path(model_directory)\\n        self.model_name = model_name if model_name else uuid.uuid4().hex\\n        self.prediction_col_name = f\\\"prediction_{self.model_name}\\\"\\n        self.description = f\\\"{self.__class__.__name__}: '{self.model_name}' prediction\\\"\\n\\n    @abstractmethod\\n    def predict(self, dataf: Union[pd.DataFrame, NumerFrame]) -> NumerFrame:\\n        \\\"\\\"\\\" Return NumerFrame with column added for prediction. \\\"\\\"\\\"\\n        ...\\n        return NumerFrame(dataf)\\n\\n    def get_prediction_col_names(self, pred_shape: tuple) -> list:\\n        \\\"\\\"\\\" Create multiple columns if predictions are multi-target. \\\"\\\"\\\"\\n        prediction_cols = self.prediction_col_name\\n        if len(pred_shape) > 1:\\n            if pred_shape[1] > 1:\\n                prediction_cols = [f\\\"{self.prediction_col_name}_{i}\\\" for i in range(pred_shape[1])]\\n        return prediction_cols\\n\\n    def __call__(self, dataf: Union[pd.DataFrame, NumerFrame]) -> NumerFrame:\\n        return self.predict(dataf=dataf)\";\n                var nbb_formatted_code = \"# export\\nclass BaseModel(ABC):\\n    \\\"\\\"\\\"\\n    Setup for model prediction on a Dataset.\\n\\n    :param model_directory: Main directory from which to read in models.\\n    :param model_name: Name that will be used to create column names and for display purposes.\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        model_directory: str,\\n        model_name: str = None,\\n    ):\\n        self.model_directory = Path(model_directory)\\n        self.model_name = model_name if model_name else uuid.uuid4().hex\\n        self.prediction_col_name = f\\\"prediction_{self.model_name}\\\"\\n        self.description = f\\\"{self.__class__.__name__}: '{self.model_name}' prediction\\\"\\n\\n    @abstractmethod\\n    def predict(self, dataf: Union[pd.DataFrame, NumerFrame]) -> NumerFrame:\\n        \\\"\\\"\\\"Return NumerFrame with column added for prediction.\\\"\\\"\\\"\\n        ...\\n        return NumerFrame(dataf)\\n\\n    def get_prediction_col_names(self, pred_shape: tuple) -> list:\\n        \\\"\\\"\\\"Create multiple columns if predictions are multi-target.\\\"\\\"\\\"\\n        prediction_cols = self.prediction_col_name\\n        if len(pred_shape) > 1:\\n            if pred_shape[1] > 1:\\n                prediction_cols = [\\n                    f\\\"{self.prediction_col_name}_{i}\\\" for i in range(pred_shape[1])\\n                ]\\n        return prediction_cols\\n\\n    def __call__(self, dataf: Union[pd.DataFrame, NumerFrame]) -> NumerFrame:\\n        return self.predict(dataf=dataf)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "class BaseModel(ABC):\n",
    "    \"\"\"\n",
    "    Setup for model prediction on a Dataset.\n",
    "\n",
    "    :param model_directory: Main directory from which to read in models.\n",
    "    :param model_name: Name that will be used to create column names and for display purposes.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_directory: str,\n",
    "                 model_name: str = None,\n",
    "                 ):\n",
    "        self.model_directory = Path(model_directory)\n",
    "        self.model_name = model_name if model_name else uuid.uuid4().hex\n",
    "        self.prediction_col_name = f\"prediction_{self.model_name}\"\n",
    "        self.description = f\"{self.__class__.__name__}: '{self.model_name}' prediction\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, dataf: Union[pd.DataFrame, NumerFrame]) -> NumerFrame:\n",
    "        \"\"\" Return NumerFrame with column added for prediction. \"\"\"\n",
    "        ...\n",
    "        return NumerFrame(dataf)\n",
    "\n",
    "    def get_prediction_col_names(self, pred_shape: tuple) -> list:\n",
    "        \"\"\" Create multiple columns if predictions are multi-target. \"\"\"\n",
    "        prediction_cols = self.prediction_col_name\n",
    "        if len(pred_shape) > 1:\n",
    "            if pred_shape[1] > 1:\n",
    "                prediction_cols = [f\"{self.prediction_col_name}_{i}\" for i in range(pred_shape[1])]\n",
    "        return prediction_cols\n",
    "\n",
    "    def __call__(self, dataf: Union[pd.DataFrame, NumerFrame]) -> NumerFrame:\n",
    "        return self.predict(dataf=dataf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. DirectoryModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `DirectoryModel` assumes that you have a directory of models and you want to load + predict for all models with a certain `file_suffix` (for example, `.joblib`, `.cbm` or `.lgb`). This base class handles prediction logic for this situation.\n",
    "\n",
    "If you are thinking of implementing your own model and this is your use case, then you should inherit from `DirectoryModel` and be sure to implement `load_models` method. Your then don't have to implement any prediction logic in the `.predict` method.\n",
    "\n",
    "When inheriting from `DirectoryModel` the only mandatory method implementation is for `load_models`. It should instantiate all models and return them as a `list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 6;\n                var nbb_unformatted_code = \"#export\\nclass DirectoryModel(BaseModel):\\n    \\\"\\\"\\\"\\n    Base class implementation where predictions are average out from a directory of models.\\n    Examples, JoblibModel, CatBoostModel, LGBMModel, etc.\\n    Walks through every file with given file_suffix in a directory.\\n    :param model_directory: Main directory from which to read in models.\\n    :param file_suffix: File format to load (For example, .joblib, .pkl, .cbm or .lgb)\\n    :param model_name: Name that will be used to create column names and for display purposes.\\n    :param feature_cols: optional list of features to use for prediction.\\n    Selects all feature columns (i.e. column names with prefix 'feature') by default.\\n    :param combine_preds: Whether to average predictions along column axis.\\n    Only relevant for multi target models.\\n    Convenient when you want to predict the main target by averaging a multi-target model.\\n    \\\"\\\"\\\"\\n    def __init__(self, model_directory: str, file_suffix: str,\\n                 model_name: str = None,\\n                 feature_cols: list = None,\\n                 combine_preds = True,\\n                 ):\\n        super().__init__(model_directory=model_directory,\\n                         model_name=model_name,\\n                         )\\n        self.file_suffix = file_suffix\\n        self.model_paths = list(self.model_directory.glob(f'*.{self.file_suffix}'))\\n        if self.file_suffix:\\n            assert self.model_paths, f\\\"No {self.file_suffix} files found in {self.model_directory}.\\\"\\n        self.total_models = len(self.model_paths)\\n        self.feature_cols = feature_cols\\n        self.combine_preds = combine_preds\\n\\n    @display_processor_info\\n    def predict(self, dataf: NumerFrame, *args, **kwargs) -> NumerFrame:\\n        \\\"\\\"\\\"\\n        Use all recognized models to make predictions and average them out.\\n        :param dataf: A Preprocessed DataFrame where all its features can be passed to the model predict method.\\n        *args, **kwargs will be parsed into the model.predict method.\\n        :return: A new dataset with prediction column added.\\n        \\\"\\\"\\\"\\n        dataf.loc[:, self.prediction_col_name] = np.zeros(len(dataf))\\n        models = self.load_models()\\n        feature_cols = self.feature_cols if self.feature_cols else dataf.feature_cols\\n        for model in tqdm(models, desc=self.description, position=1):\\n            predictions = model.predict(dataf[feature_cols], *args, **kwargs)\\n            # Check for if model output is a Pandas DataFrame\\n            predictions = predictions.values if isinstance(predictions, pd.DataFrame) else predictions\\n            predictions = predictions.mean(axis=1) if self.combine_preds and len(predictions.shape) > 1 else predictions\\n            prediction_cols = self.get_prediction_col_names(predictions.shape)\\n            dataf.loc[:, prediction_cols] = dataf.loc[:, prediction_cols] + (predictions / self.total_models)\\n        del models; gc.collect()\\n        return NumerFrame(dataf)\\n\\n    @abstractmethod\\n    def load_models(self) -> list:\\n        \\\"\\\"\\\" Instantiate all models detected in self.model_paths. \\\"\\\"\\\"\\n        ...\";\n                var nbb_formatted_code = \"# export\\nclass DirectoryModel(BaseModel):\\n    \\\"\\\"\\\"\\n    Base class implementation where predictions are average out from a directory of models.\\n    Examples, JoblibModel, CatBoostModel, LGBMModel, etc.\\n    Walks through every file with given file_suffix in a directory.\\n    :param model_directory: Main directory from which to read in models.\\n    :param file_suffix: File format to load (For example, .joblib, .pkl, .cbm or .lgb)\\n    :param model_name: Name that will be used to create column names and for display purposes.\\n    :param feature_cols: optional list of features to use for prediction.\\n    Selects all feature columns (i.e. column names with prefix 'feature') by default.\\n    :param combine_preds: Whether to average predictions along column axis.\\n    Only relevant for multi target models.\\n    Convenient when you want to predict the main target by averaging a multi-target model.\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        model_directory: str,\\n        file_suffix: str,\\n        model_name: str = None,\\n        feature_cols: list = None,\\n        combine_preds=True,\\n    ):\\n        super().__init__(\\n            model_directory=model_directory,\\n            model_name=model_name,\\n        )\\n        self.file_suffix = file_suffix\\n        self.model_paths = list(self.model_directory.glob(f\\\"*.{self.file_suffix}\\\"))\\n        if self.file_suffix:\\n            assert (\\n                self.model_paths\\n            ), f\\\"No {self.file_suffix} files found in {self.model_directory}.\\\"\\n        self.total_models = len(self.model_paths)\\n        self.feature_cols = feature_cols\\n        self.combine_preds = combine_preds\\n\\n    @display_processor_info\\n    def predict(self, dataf: NumerFrame, *args, **kwargs) -> NumerFrame:\\n        \\\"\\\"\\\"\\n        Use all recognized models to make predictions and average them out.\\n        :param dataf: A Preprocessed DataFrame where all its features can be passed to the model predict method.\\n        *args, **kwargs will be parsed into the model.predict method.\\n        :return: A new dataset with prediction column added.\\n        \\\"\\\"\\\"\\n        dataf.loc[:, self.prediction_col_name] = np.zeros(len(dataf))\\n        models = self.load_models()\\n        feature_cols = self.feature_cols if self.feature_cols else dataf.feature_cols\\n        for model in tqdm(models, desc=self.description, position=1):\\n            predictions = model.predict(dataf[feature_cols], *args, **kwargs)\\n            # Check for if model output is a Pandas DataFrame\\n            predictions = (\\n                predictions.values\\n                if isinstance(predictions, pd.DataFrame)\\n                else predictions\\n            )\\n            predictions = (\\n                predictions.mean(axis=1)\\n                if self.combine_preds and len(predictions.shape) > 1\\n                else predictions\\n            )\\n            prediction_cols = self.get_prediction_col_names(predictions.shape)\\n            dataf.loc[:, prediction_cols] = dataf.loc[:, prediction_cols] + (\\n                predictions / self.total_models\\n            )\\n        del models\\n        gc.collect()\\n        return NumerFrame(dataf)\\n\\n    @abstractmethod\\n    def load_models(self) -> list:\\n        \\\"\\\"\\\"Instantiate all models detected in self.model_paths.\\\"\\\"\\\"\\n        ...\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "class DirectoryModel(BaseModel):\n",
    "    \"\"\"\n",
    "    Base class implementation where predictions are average out from a directory of models.\n",
    "    Examples, JoblibModel, CatBoostModel, LGBMModel, etc.\n",
    "    Walks through every file with given file_suffix in a directory.\n",
    "    :param model_directory: Main directory from which to read in models.\n",
    "    :param file_suffix: File format to load (For example, .joblib, .pkl, .cbm or .lgb)\n",
    "    :param model_name: Name that will be used to create column names and for display purposes.\n",
    "    :param feature_cols: optional list of features to use for prediction.\n",
    "    Selects all feature columns (i.e. column names with prefix 'feature') by default.\n",
    "    :param combine_preds: Whether to average predictions along column axis.\n",
    "    Only relevant for multi target models.\n",
    "    Convenient when you want to predict the main target by averaging a multi-target model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_directory: str, file_suffix: str,\n",
    "                 model_name: str = None,\n",
    "                 feature_cols: list = None,\n",
    "                 combine_preds = True,\n",
    "                 ):\n",
    "        super().__init__(model_directory=model_directory,\n",
    "                         model_name=model_name,\n",
    "                         )\n",
    "        self.file_suffix = file_suffix\n",
    "        self.model_paths = list(self.model_directory.glob(f'*.{self.file_suffix}'))\n",
    "        if self.file_suffix:\n",
    "            assert self.model_paths, f\"No {self.file_suffix} files found in {self.model_directory}.\"\n",
    "        self.total_models = len(self.model_paths)\n",
    "        self.feature_cols = feature_cols\n",
    "        self.combine_preds = combine_preds\n",
    "\n",
    "    @display_processor_info\n",
    "    def predict(self, dataf: NumerFrame, *args, **kwargs) -> NumerFrame:\n",
    "        \"\"\"\n",
    "        Use all recognized models to make predictions and average them out.\n",
    "        :param dataf: A Preprocessed DataFrame where all its features can be passed to the model predict method.\n",
    "        *args, **kwargs will be parsed into the model.predict method.\n",
    "        :return: A new dataset with prediction column added.\n",
    "        \"\"\"\n",
    "        dataf.loc[:, self.prediction_col_name] = np.zeros(len(dataf))\n",
    "        models = self.load_models()\n",
    "        feature_cols = self.feature_cols if self.feature_cols else dataf.feature_cols\n",
    "        for model in tqdm(models, desc=self.description, position=1):\n",
    "            predictions = model.predict(dataf[feature_cols], *args, **kwargs)\n",
    "            # Check for if model output is a Pandas DataFrame\n",
    "            predictions = predictions.values if isinstance(predictions, pd.DataFrame) else predictions\n",
    "            predictions = predictions.mean(axis=1) if self.combine_preds and len(predictions.shape) > 1 else predictions\n",
    "            prediction_cols = self.get_prediction_col_names(predictions.shape)\n",
    "            dataf.loc[:, prediction_cols] = dataf.loc[:, prediction_cols] + (predictions / self.total_models)\n",
    "        del models; gc.collect()\n",
    "        return NumerFrame(dataf)\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_models(self) -> list:\n",
    "        \"\"\" Instantiate all models detected in self.model_paths. \"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single model formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementations for common Numerai model prediction situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. SingleModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases you just want to load a single model file and create predictions for that model. `SingleModel` supports this.\n",
    "\n",
    "This class supports multiple model formats for easy use. All models should have a `.predict` method.\n",
    "Currently, `.joblib`, `.cbm`, `.pkl`, `.pickle` and `.h5` (keras) format are supported.\n",
    "\n",
    "**Things to keep in mind**\n",
    "- This model will use all available features in the `NumerFrame` and use them for prediction. Make sure to define proper feature selection if the models does not use all features.\n",
    "- If you have XGBoost models we recommend saving them as `.joblib`.\n",
    "- The added prediction column will have the column name `prediction_{MODEL_NAME}` if 1 target is predicted.\n",
    "For multiple targets the new column names will be `prediction_{MODEL_NAME}_{i}` for each target number i (starting with 0).\n",
    "- We welcome the Numerai community to extend `SingleModel` for more file formats. See the Contributing section in `README.md` for more information on contributing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 7;\n                var nbb_unformatted_code = \"#export\\n@typechecked\\nclass SingleModel(BaseModel):\\n    \\\"\\\"\\\"\\n    Load single model from file and perform prediction logic.\\n    :param model_file_path: Full path to model file.\\n    :param model_name: Name that will be used to create column names and for display purposes.\\n    :param combine_preds: Whether to average predictions along column axis.\\n    Only relevant for multi target models.\\n    Convenient when you want to predict the main target by averaging a multi-target model.\\n    :param autoencoder_mlp: Whether your model is an autoencoder + MLP model.\\n    Will take the 3rd of tuple output in this case. Only relevant for NN models.\\n    More info on autoencoders:\\n    https://forum.numer.ai/t/autoencoder-and-multitask-mlp-on-new-dataset-from-kaggle-jane-street/4338\\n    :param feature_cols: optional list of features to use for prediction.\\n    Selects all feature columns (i.e. column names with prefix 'feature') by default.\\n    \\\"\\\"\\\"\\n    def __init__(self, model_file_path: str, model_name: str = None,\\n                 combine_preds = False, autoencoder_mlp = False,\\n                 feature_cols: list = None\\n                 ):\\n        self.model_file_path = Path(model_file_path)\\n        assert self.model_file_path.exists(), f\\\"File path '{self.model_file_path}' does not exist.\\\"\\n        assert self.model_file_path.is_file(), f\\\"File path must point to file. Not valid for '{self.model_file_path}'.\\\"\\n        super().__init__(model_directory=str(self.model_file_path.parent),\\n                         model_name=model_name,\\n                         )\\n        self.model_suffix = self.model_file_path.suffix\\n        self.suffix_to_model_mapping = {\\\".joblib\\\": joblib.load,\\n                                        \\\".cbm\\\": CatBoost().load_model,\\n                                        \\\".pkl\\\": pickle.load,\\n                                        \\\".pickle\\\": pickle.load,\\n                                        \\\".h5\\\": partial(tf.keras.models.load_model, compile=False)\\n                                        }\\n        self.__check_valid_suffix()\\n        self.combine_preds = combine_preds\\n        self.autoencoder_mlp = autoencoder_mlp\\n        self.feature_cols = feature_cols\\n\\n    def predict(self, dataf: NumerFrame, *args, **kwargs) -> NumerFrame:\\n        model = self._load_model(*args, **kwargs)\\n        feature_cols = self.feature_cols if self.feature_cols else dataf.feature_cols\\n        predictions = model.predict(dataf[feature_cols])\\n        # Check for if model output is a Pandas DataFrame\\n        predictions = predictions.values if isinstance(predictions, pd.DataFrame) else predictions\\n        predictions = predictions[2] if self.autoencoder_mlp else predictions\\n        predictions = predictions.mean(axis=1) if self.combine_preds else predictions\\n        prediction_cols = self.get_prediction_col_names(predictions.shape)\\n        dataf.loc[:, prediction_cols] = predictions\\n        del model; gc.collect()\\n        return NumerFrame(dataf)\\n\\n    def _load_model(self, *args, **kwargs):\\n        \\\"\\\"\\\" Load arbitrary model from path using suffix to model mapping. \\\"\\\"\\\"\\n        return self.suffix_to_model_mapping[self.model_suffix](str(self.model_file_path), *args, **kwargs)\\n\\n    def __check_valid_suffix(self):\\n        \\\"\\\"\\\" Detailed message if model is not supported in this class. \\\"\\\"\\\"\\n        try:\\n            self.suffix_to_model_mapping[self.model_suffix]\\n        except KeyError:\\n            raise NotImplementedError(\\n                f\\\"Format '{self.model_suffix}' is not available. Available versions are {list(self.suffix_to_model_mapping.keys())}\\\"\\n            )\";\n                var nbb_formatted_code = \"# export\\n@typechecked\\nclass SingleModel(BaseModel):\\n    \\\"\\\"\\\"\\n    Load single model from file and perform prediction logic.\\n    :param model_file_path: Full path to model file.\\n    :param model_name: Name that will be used to create column names and for display purposes.\\n    :param combine_preds: Whether to average predictions along column axis.\\n    Only relevant for multi target models.\\n    Convenient when you want to predict the main target by averaging a multi-target model.\\n    :param autoencoder_mlp: Whether your model is an autoencoder + MLP model.\\n    Will take the 3rd of tuple output in this case. Only relevant for NN models.\\n    More info on autoencoders:\\n    https://forum.numer.ai/t/autoencoder-and-multitask-mlp-on-new-dataset-from-kaggle-jane-street/4338\\n    :param feature_cols: optional list of features to use for prediction.\\n    Selects all feature columns (i.e. column names with prefix 'feature') by default.\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        model_file_path: str,\\n        model_name: str = None,\\n        combine_preds=False,\\n        autoencoder_mlp=False,\\n        feature_cols: list = None,\\n    ):\\n        self.model_file_path = Path(model_file_path)\\n        assert (\\n            self.model_file_path.exists()\\n        ), f\\\"File path '{self.model_file_path}' does not exist.\\\"\\n        assert (\\n            self.model_file_path.is_file()\\n        ), f\\\"File path must point to file. Not valid for '{self.model_file_path}'.\\\"\\n        super().__init__(\\n            model_directory=str(self.model_file_path.parent),\\n            model_name=model_name,\\n        )\\n        self.model_suffix = self.model_file_path.suffix\\n        self.suffix_to_model_mapping = {\\n            \\\".joblib\\\": joblib.load,\\n            \\\".cbm\\\": CatBoost().load_model,\\n            \\\".pkl\\\": pickle.load,\\n            \\\".pickle\\\": pickle.load,\\n            \\\".h5\\\": partial(tf.keras.models.load_model, compile=False),\\n        }\\n        self.__check_valid_suffix()\\n        self.combine_preds = combine_preds\\n        self.autoencoder_mlp = autoencoder_mlp\\n        self.feature_cols = feature_cols\\n\\n    def predict(self, dataf: NumerFrame, *args, **kwargs) -> NumerFrame:\\n        model = self._load_model(*args, **kwargs)\\n        feature_cols = self.feature_cols if self.feature_cols else dataf.feature_cols\\n        predictions = model.predict(dataf[feature_cols])\\n        # Check for if model output is a Pandas DataFrame\\n        predictions = (\\n            predictions.values if isinstance(predictions, pd.DataFrame) else predictions\\n        )\\n        predictions = predictions[2] if self.autoencoder_mlp else predictions\\n        predictions = predictions.mean(axis=1) if self.combine_preds else predictions\\n        prediction_cols = self.get_prediction_col_names(predictions.shape)\\n        dataf.loc[:, prediction_cols] = predictions\\n        del model\\n        gc.collect()\\n        return NumerFrame(dataf)\\n\\n    def _load_model(self, *args, **kwargs):\\n        \\\"\\\"\\\"Load arbitrary model from path using suffix to model mapping.\\\"\\\"\\\"\\n        return self.suffix_to_model_mapping[self.model_suffix](\\n            str(self.model_file_path), *args, **kwargs\\n        )\\n\\n    def __check_valid_suffix(self):\\n        \\\"\\\"\\\"Detailed message if model is not supported in this class.\\\"\\\"\\\"\\n        try:\\n            self.suffix_to_model_mapping[self.model_suffix]\\n        except KeyError:\\n            raise NotImplementedError(\\n                f\\\"Format '{self.model_suffix}' is not available. Available versions are {list(self.suffix_to_model_mapping.keys())}\\\"\\n            )\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "@typechecked\n",
    "class SingleModel(BaseModel):\n",
    "    \"\"\"\n",
    "    Load single model from file and perform prediction logic.\n",
    "    :param model_file_path: Full path to model file.\n",
    "    :param model_name: Name that will be used to create column names and for display purposes.\n",
    "    :param combine_preds: Whether to average predictions along column axis.\n",
    "    Only relevant for multi target models.\n",
    "    Convenient when you want to predict the main target by averaging a multi-target model.\n",
    "    :param autoencoder_mlp: Whether your model is an autoencoder + MLP model.\n",
    "    Will take the 3rd of tuple output in this case. Only relevant for NN models.\n",
    "    More info on autoencoders:\n",
    "    https://forum.numer.ai/t/autoencoder-and-multitask-mlp-on-new-dataset-from-kaggle-jane-street/4338\n",
    "    :param feature_cols: optional list of features to use for prediction.\n",
    "    Selects all feature columns (i.e. column names with prefix 'feature') by default.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_file_path: str, model_name: str = None,\n",
    "                 combine_preds = False, autoencoder_mlp = False,\n",
    "                 feature_cols: list = None\n",
    "                 ):\n",
    "        self.model_file_path = Path(model_file_path)\n",
    "        assert self.model_file_path.exists(), f\"File path '{self.model_file_path}' does not exist.\"\n",
    "        assert self.model_file_path.is_file(), f\"File path must point to file. Not valid for '{self.model_file_path}'.\"\n",
    "        super().__init__(model_directory=str(self.model_file_path.parent),\n",
    "                         model_name=model_name,\n",
    "                         )\n",
    "        self.model_suffix = self.model_file_path.suffix\n",
    "        self.suffix_to_model_mapping = {\".joblib\": joblib.load,\n",
    "                                        \".cbm\": CatBoost().load_model,\n",
    "                                        \".pkl\": pickle.load,\n",
    "                                        \".pickle\": pickle.load,\n",
    "                                        \".h5\": partial(tf.keras.models.load_model, compile=False)\n",
    "                                        }\n",
    "        self.__check_valid_suffix()\n",
    "        self.combine_preds = combine_preds\n",
    "        self.autoencoder_mlp = autoencoder_mlp\n",
    "        self.feature_cols = feature_cols\n",
    "\n",
    "    def predict(self, dataf: NumerFrame, *args, **kwargs) -> NumerFrame:\n",
    "        model = self._load_model(*args, **kwargs)\n",
    "        feature_cols = self.feature_cols if self.feature_cols else dataf.feature_cols\n",
    "        predictions = model.predict(dataf[feature_cols])\n",
    "        # Check for if model output is a Pandas DataFrame\n",
    "        predictions = predictions.values if isinstance(predictions, pd.DataFrame) else predictions\n",
    "        predictions = predictions[2] if self.autoencoder_mlp else predictions\n",
    "        predictions = predictions.mean(axis=1) if self.combine_preds else predictions\n",
    "        prediction_cols = self.get_prediction_col_names(predictions.shape)\n",
    "        dataf.loc[:, prediction_cols] = predictions\n",
    "        del model; gc.collect()\n",
    "        return NumerFrame(dataf)\n",
    "\n",
    "    def _load_model(self, *args, **kwargs):\n",
    "        \"\"\" Load arbitrary model from path using suffix to model mapping. \"\"\"\n",
    "        return self.suffix_to_model_mapping[self.model_suffix](str(self.model_file_path), *args, **kwargs)\n",
    "\n",
    "    def __check_valid_suffix(self):\n",
    "        \"\"\" Detailed message if model is not supported in this class. \"\"\"\n",
    "        try:\n",
    "            self.suffix_to_model_mapping[self.model_suffix]\n",
    "        except KeyError:\n",
    "            raise NotImplementedError(\n",
    "                f\"Format '{self.model_suffix}' is not available. Available versions are {list(self.suffix_to_model_mapping.keys())}\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  prediction_test\n",
      "id                               \n",
      "n559bd06a8861222         0.506948\n",
      "n9d39dea58c9e3cf         0.492578\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 8;\n                var nbb_unformatted_code = \"dataf = create_numerframe(\\\"test_assets/mini_numerai_version_2_data.parquet\\\")\\ntest_paths = [\\\"test_assets/joblib_v2_example_model.joblib\\\"]\\nfor path in test_paths:\\n    model = SingleModel(path, model_name=\\\"test\\\")\\n    print(model.predict(dataf).get_prediction_data.head(2))\";\n                var nbb_formatted_code = \"dataf = create_numerframe(\\\"test_assets/mini_numerai_version_2_data.parquet\\\")\\ntest_paths = [\\\"test_assets/joblib_v2_example_model.joblib\\\"]\\nfor path in test_paths:\\n    model = SingleModel(path, model_name=\\\"test\\\")\\n    print(model.predict(dataf).get_prediction_data.head(2))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataf = create_numerframe(\"test_assets/mini_numerai_version_2_data.parquet\")\n",
    "test_paths = [\"test_assets/joblib_v2_example_model.joblib\"]\n",
    "for path in test_paths:\n",
    "    model = SingleModel(path, model_name=\"test\")\n",
    "    print(model.predict(dataf).get_prediction_data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'.joblib': <function joblib.numpy_pickle.load(filename, mmap_mode=None)>,\n '.cbm': <bound method CatBoost.load_model of <catboost.core.CatBoost object at 0x7facb19a7990>>,\n '.pkl': <function _pickle.load(file, *, fix_imports=True, encoding='ASCII', errors='strict')>,\n '.pickle': <function _pickle.load(file, *, fix_imports=True, encoding='ASCII', errors='strict')>,\n '.h5': functools.partial(<function load_model at 0x7fac60b1b4d0>, compile=False)}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 9;\n                var nbb_unformatted_code = \"model = SingleModel(test_paths[0], model_name=\\\"test\\\")\\nmodel.suffix_to_model_mapping\";\n                var nbb_formatted_code = \"model = SingleModel(test_paths[0], model_name=\\\"test\\\")\\nmodel.suffix_to_model_mapping\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SingleModel(test_paths[0], model_name=\"test\")\n",
    "model.suffix_to_model_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. WandbKerasModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is for a specific case if you are logging Keras model using [Weights & Biases](https://wandb.ai/site) and want to download the best model for a specific run. `WandbKerasModel` wraps `SingleModel` and only implements additional logic for downloading models from Weights & Biases.\n",
    "\n",
    "To authenticate your W&B account you are given several options:\n",
    "1. Run `wandb login` in terminal and follow instructions ([docs](https://docs.wandb.ai/ref/cli/wandb-login)).\n",
    "2. Configure [global environment variable](https://docs.wandb.ai/guides/track/advanced/environment-variables) `\"WANDB_API_KEY\"`.\n",
    "3. Run `wandb.init(project=PROJECT_NAME, entity=ENTITY_NAME)` and\n",
    "pass API key from [https://wandb.ai/authorize](https://wandb.ai/authorize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 10;\n                var nbb_unformatted_code = \"#export\\n@typechecked\\nclass WandbKerasModel(SingleModel):\\n    \\\"\\\"\\\"\\n    Download best .h5 model from Weights & Biases (W&B) run in local directory and make predictions.\\n    More info on W&B: https://wandb.ai/site\\n    :param run_path: W&B path structured as entity/project/run_id.\\n    Can be copied from the Overview tab of a W&B run.\\n    For more info: https://docs.wandb.ai/ref/app/pages/run-page#overview-tab\\n    Entity, project and id can be found in Overview tab of W&B run.\\n    :param file_name: Name of .h5 file as saved in W&B run.\\n    'model-best.h5' by default.\\n    File name can be found under files tab of W&B run.\\n    :param combine_preds: Whether to average predictions along column axis.\\n    Convenient when you want to predict the main target by averaging a multi-target model.\\n    :param autoencoder_mlp: Whether your model is an autoencoder + MLP model.\\n    Will take the 3rd of tuple output in this case. Only relevant for NN models.\\n    More info on autoencoders:\\n    https://forum.numer.ai/t/autoencoder-and-multitask-mlp-on-new-dataset-from-kaggle-jane-street/4338\\n    :param replace: Replace any model files saved under the same file name with downloaden W&B run model. WARNING: Setting to True may overwrite models in your local environment.\\n    :param feature_cols: optional list of features to use for prediction.\\n    Selects all feature columns (i.e. column names with prefix 'feature') by default.\\n\\n    To authenticate your W&B account you are given several options:\\n    1. Run wandb login in terminal and follow instructions.\\n    2. Configure global environment variable \\\"WANDB_API_KEY\\\".\\n    3. Run wandb.init(project=PROJECT_NAME, entity=ENTITY_NAME) and\\n    pass API key from https://wandb.ai/authorize\\n    \\\"\\\"\\\"\\n    def __init__(self,\\n                 run_path: str,\\n                 file_name: str = \\\"model-best.h5\\\",\\n                 combine_preds = False,\\n                 autoencoder_mlp = False,\\n                 replace = False,\\n                 feature_cols: list = None\\n                 ):\\n        self.run_path = run_path\\n        self.file_name = file_name\\n        self.replace = replace\\n\\n        self._download_model()\\n        super().__init__(model_file_path=f\\\"{self.run_path.split('/')[-1]}_{self.file_name}\\\",\\n                         model_name=self.run_path,\\n                         combine_preds=combine_preds,\\n                         autoencoder_mlp=autoencoder_mlp,\\n                         feature_cols=feature_cols\\n                         )\\n\\n    def _download_model(self):\\n        \\\"\\\"\\\"\\n        Use W&B API to download .h5 model file.\\n        More info on API: https://docs.wandb.ai/guides/track/public-api-guide\\n        \\\"\\\"\\\"\\n        if Path(self.file_name).is_file() and not self.replace:\\n            rich_print(f\\\":warning: [red] Model file '{self.file_name}' already exists in local environment.\\\\\\n            Skipping download of W&B run model. If this is not the model you want to use for prediction\\\\\\n            consider moving it or set 'replace=True' at initialization to overwrite. [/red] :warning:\\\")\\n        else:\\n            rich_print(f\\\":page_facing_up: [green] Downloading '{self.file_name}' from '{self.run_path}' in W&B Cloud. [/green] :page_facing_up:\\\")\\n        run = wandb.Api().run(self.run_path)\\n        run.file(name=self.file_name).download(replace=self.replace)\\n        os.rename(self.file_name, f\\\"{self.run_path.split('/')[-1]}_{self.file_name}\\\")\";\n                var nbb_formatted_code = \"# export\\n@typechecked\\nclass WandbKerasModel(SingleModel):\\n    \\\"\\\"\\\"\\n    Download best .h5 model from Weights & Biases (W&B) run in local directory and make predictions.\\n    More info on W&B: https://wandb.ai/site\\n    :param run_path: W&B path structured as entity/project/run_id.\\n    Can be copied from the Overview tab of a W&B run.\\n    For more info: https://docs.wandb.ai/ref/app/pages/run-page#overview-tab\\n    Entity, project and id can be found in Overview tab of W&B run.\\n    :param file_name: Name of .h5 file as saved in W&B run.\\n    'model-best.h5' by default.\\n    File name can be found under files tab of W&B run.\\n    :param combine_preds: Whether to average predictions along column axis.\\n    Convenient when you want to predict the main target by averaging a multi-target model.\\n    :param autoencoder_mlp: Whether your model is an autoencoder + MLP model.\\n    Will take the 3rd of tuple output in this case. Only relevant for NN models.\\n    More info on autoencoders:\\n    https://forum.numer.ai/t/autoencoder-and-multitask-mlp-on-new-dataset-from-kaggle-jane-street/4338\\n    :param replace: Replace any model files saved under the same file name with downloaden W&B run model. WARNING: Setting to True may overwrite models in your local environment.\\n    :param feature_cols: optional list of features to use for prediction.\\n    Selects all feature columns (i.e. column names with prefix 'feature') by default.\\n\\n    To authenticate your W&B account you are given several options:\\n    1. Run wandb login in terminal and follow instructions.\\n    2. Configure global environment variable \\\"WANDB_API_KEY\\\".\\n    3. Run wandb.init(project=PROJECT_NAME, entity=ENTITY_NAME) and\\n    pass API key from https://wandb.ai/authorize\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        run_path: str,\\n        file_name: str = \\\"model-best.h5\\\",\\n        combine_preds=False,\\n        autoencoder_mlp=False,\\n        replace=False,\\n        feature_cols: list = None,\\n    ):\\n        self.run_path = run_path\\n        self.file_name = file_name\\n        self.replace = replace\\n\\n        self._download_model()\\n        super().__init__(\\n            model_file_path=f\\\"{self.run_path.split('/')[-1]}_{self.file_name}\\\",\\n            model_name=self.run_path,\\n            combine_preds=combine_preds,\\n            autoencoder_mlp=autoencoder_mlp,\\n            feature_cols=feature_cols,\\n        )\\n\\n    def _download_model(self):\\n        \\\"\\\"\\\"\\n        Use W&B API to download .h5 model file.\\n        More info on API: https://docs.wandb.ai/guides/track/public-api-guide\\n        \\\"\\\"\\\"\\n        if Path(self.file_name).is_file() and not self.replace:\\n            rich_print(\\n                f\\\":warning: [red] Model file '{self.file_name}' already exists in local environment.\\\\\\n            Skipping download of W&B run model. If this is not the model you want to use for prediction\\\\\\n            consider moving it or set 'replace=True' at initialization to overwrite. [/red] :warning:\\\"\\n            )\\n        else:\\n            rich_print(\\n                f\\\":page_facing_up: [green] Downloading '{self.file_name}' from '{self.run_path}' in W&B Cloud. [/green] :page_facing_up:\\\"\\n            )\\n        run = wandb.Api().run(self.run_path)\\n        run.file(name=self.file_name).download(replace=self.replace)\\n        os.rename(self.file_name, f\\\"{self.run_path.split('/')[-1]}_{self.file_name}\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "@typechecked\n",
    "class WandbKerasModel(SingleModel):\n",
    "    \"\"\"\n",
    "    Download best .h5 model from Weights & Biases (W&B) run in local directory and make predictions.\n",
    "    More info on W&B: https://wandb.ai/site\n",
    "    :param run_path: W&B path structured as entity/project/run_id.\n",
    "    Can be copied from the Overview tab of a W&B run.\n",
    "    For more info: https://docs.wandb.ai/ref/app/pages/run-page#overview-tab\n",
    "    Entity, project and id can be found in Overview tab of W&B run.\n",
    "    :param file_name: Name of .h5 file as saved in W&B run.\n",
    "    'model-best.h5' by default.\n",
    "    File name can be found under files tab of W&B run.\n",
    "    :param combine_preds: Whether to average predictions along column axis.\n",
    "    Convenient when you want to predict the main target by averaging a multi-target model.\n",
    "    :param autoencoder_mlp: Whether your model is an autoencoder + MLP model.\n",
    "    Will take the 3rd of tuple output in this case. Only relevant for NN models.\n",
    "    More info on autoencoders:\n",
    "    https://forum.numer.ai/t/autoencoder-and-multitask-mlp-on-new-dataset-from-kaggle-jane-street/4338\n",
    "    :param replace: Replace any model files saved under the same file name with downloaden W&B run model. WARNING: Setting to True may overwrite models in your local environment.\n",
    "    :param feature_cols: optional list of features to use for prediction.\n",
    "    Selects all feature columns (i.e. column names with prefix 'feature') by default.\n",
    "\n",
    "    To authenticate your W&B account you are given several options:\n",
    "    1. Run wandb login in terminal and follow instructions.\n",
    "    2. Configure global environment variable \"WANDB_API_KEY\".\n",
    "    3. Run wandb.init(project=PROJECT_NAME, entity=ENTITY_NAME) and\n",
    "    pass API key from https://wandb.ai/authorize\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 run_path: str,\n",
    "                 file_name: str = \"model-best.h5\",\n",
    "                 combine_preds = False,\n",
    "                 autoencoder_mlp = False,\n",
    "                 replace = False,\n",
    "                 feature_cols: list = None\n",
    "                 ):\n",
    "        self.run_path = run_path\n",
    "        self.file_name = file_name\n",
    "        self.replace = replace\n",
    "\n",
    "        self._download_model()\n",
    "        super().__init__(model_file_path=f\"{self.run_path.split('/')[-1]}_{self.file_name}\",\n",
    "                         model_name=self.run_path,\n",
    "                         combine_preds=combine_preds,\n",
    "                         autoencoder_mlp=autoencoder_mlp,\n",
    "                         feature_cols=feature_cols\n",
    "                         )\n",
    "\n",
    "    def _download_model(self):\n",
    "        \"\"\"\n",
    "        Use W&B API to download .h5 model file.\n",
    "        More info on API: https://docs.wandb.ai/guides/track/public-api-guide\n",
    "        \"\"\"\n",
    "        if Path(self.file_name).is_file() and not self.replace:\n",
    "            rich_print(f\":warning: [red] Model file '{self.file_name}' already exists in local environment.\\\n",
    "            Skipping download of W&B run model. If this is not the model you want to use for prediction\\\n",
    "            consider moving it or set 'replace=True' at initialization to overwrite. [/red] :warning:\")\n",
    "        else:\n",
    "            rich_print(f\":page_facing_up: [green] Downloading '{self.file_name}' from '{self.run_path}' in W&B Cloud. [/green] :page_facing_up:\")\n",
    "        run = wandb.Api().run(self.run_path)\n",
    "        run.file(name=self.file_name).download(replace=self.replace)\n",
    "        os.rename(self.file_name, f\"{self.run_path.split('/')[-1]}_{self.file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 11;\n                var nbb_unformatted_code = \"run_path = \\\"user123/project/abcd1234\\\"\\n# model = WandbKerasModel(run_path=run_path)\";\n                var nbb_formatted_code = \"run_path = \\\"user123/project/abcd1234\\\"\\n# model = WandbKerasModel(run_path=run_path)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_path = \"user123/project/abcd1234\"\n",
    "# model = WandbKerasModel(run_path=run_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3. CSVSub\n",
    "\n",
    "This model is a wrapper for if you want to add predictions from external CSVs in a directory to your model pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 12;\n                var nbb_unformatted_code = \"#export\\n@typechecked\\nclass ExternalCSVs(BaseModel):\\n    \\\"\\\"\\\"\\n    Load external submissions and add to NumerFrame.\\n    :param data_directory: Directory path for retrieving external submission.\\n    All csv files in this directory will be added to NumerFrame.\\n    Make sure all external predictions are prepared as ready for submission.\\n    i.e. IDs lining up and one column named 'prediction'.\\n    \\\"\\\"\\\"\\n    def __init__(self, data_directory: str = \\\"external_submissions\\\"):\\n        super().__init__(model_directory=data_directory)\\n        self.data_directory = Path(data_directory)\\n        self.paths = list(self.data_directory.glob(\\\"*.csv\\\"))\\n        if not self.paths:\\n            rich_print(f\\\":warning: WARNING: No csvs found in directory '{self.data_directory}'. :warning:\\\")\\n\\n    def predict(self, dataf: NumerFrame) -> NumerFrame:\\n        \\\"\\\"\\\" Return NumerFrame with added external predictions. \\\"\\\"\\\"\\n        for path in tqdm(self.paths, desc=\\\"External submissions\\\"):\\n            dataf.loc[:, f\\\"prediction_{path.name}\\\"] = self._get_preds(path)\\n        return NumerFrame(dataf)\\n\\n    def _get_preds(self, path: Path) -> pd.Series:\\n        pred_col = pd.read_csv(path, index_col=0, header=0)['prediction']\\n        if not pred_col.between(0, 1).all():\\n            raise ValueError(f\\\"Prediction values must be between 0 and 1. Does not hold for '{path.name}'.\\\")\\n        return pred_col\";\n                var nbb_formatted_code = \"# export\\n@typechecked\\nclass ExternalCSVs(BaseModel):\\n    \\\"\\\"\\\"\\n    Load external submissions and add to NumerFrame.\\n    :param data_directory: Directory path for retrieving external submission.\\n    All csv files in this directory will be added to NumerFrame.\\n    Make sure all external predictions are prepared as ready for submission.\\n    i.e. IDs lining up and one column named 'prediction'.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, data_directory: str = \\\"external_submissions\\\"):\\n        super().__init__(model_directory=data_directory)\\n        self.data_directory = Path(data_directory)\\n        self.paths = list(self.data_directory.glob(\\\"*.csv\\\"))\\n        if not self.paths:\\n            rich_print(\\n                f\\\":warning: WARNING: No csvs found in directory '{self.data_directory}'. :warning:\\\"\\n            )\\n\\n    def predict(self, dataf: NumerFrame) -> NumerFrame:\\n        \\\"\\\"\\\"Return NumerFrame with added external predictions.\\\"\\\"\\\"\\n        for path in tqdm(self.paths, desc=\\\"External submissions\\\"):\\n            dataf.loc[:, f\\\"prediction_{path.name}\\\"] = self._get_preds(path)\\n        return NumerFrame(dataf)\\n\\n    def _get_preds(self, path: Path) -> pd.Series:\\n        pred_col = pd.read_csv(path, index_col=0, header=0)[\\\"prediction\\\"]\\n        if not pred_col.between(0, 1).all():\\n            raise ValueError(\\n                f\\\"Prediction values must be between 0 and 1. Does not hold for '{path.name}'.\\\"\\n            )\\n        return pred_col\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "@typechecked\n",
    "class ExternalCSVs(BaseModel):\n",
    "    \"\"\"\n",
    "    Load external submissions and add to NumerFrame.\n",
    "    :param data_directory: Directory path for retrieving external submission.\n",
    "    All csv files in this directory will be added to NumerFrame.\n",
    "    Make sure all external predictions are prepared as ready for submission.\n",
    "    i.e. IDs lining up and one column named 'prediction'.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_directory: str = \"external_submissions\"):\n",
    "        super().__init__(model_directory=data_directory)\n",
    "        self.data_directory = Path(data_directory)\n",
    "        self.paths = list(self.data_directory.glob(\"*.csv\"))\n",
    "        if not self.paths:\n",
    "            rich_print(f\":warning: WARNING: No csvs found in directory '{self.data_directory}'. :warning:\")\n",
    "\n",
    "    def predict(self, dataf: NumerFrame) -> NumerFrame:\n",
    "        \"\"\" Return NumerFrame with added external predictions. \"\"\"\n",
    "        for path in tqdm(self.paths, desc=\"External submissions\"):\n",
    "            dataf.loc[:, f\"prediction_{path.name}\"] = self._get_preds(path)\n",
    "        return NumerFrame(dataf)\n",
    "\n",
    "    def _get_preds(self, path: Path) -> pd.Series:\n",
    "        pred_col = pd.read_csv(path, index_col=0, header=0)['prediction']\n",
    "        if not pred_col.between(0, 1).all():\n",
    "            raise ValueError(f\"Prediction values must be between 0 and 1. Does not hold for '{path.name}'.\")\n",
    "        return pred_col"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For testing, the `external_submissions` directory contains `test_predictions.csv` with values from feature `target_thomas_20`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "External submissions:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e3030fb9d9b4281ad781ccc35164967"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 13;\n                var nbb_unformatted_code = \"dataf = create_numerframe(\\\"test_assets/mini_numerai_version_2_data.parquet\\\")\\nexternal = ExternalCSVs(data_directory=\\\"test_assets/external_submissions\\\")\\nnew_dataf = external.predict(dataf)\";\n                var nbb_formatted_code = \"dataf = create_numerframe(\\\"test_assets/mini_numerai_version_2_data.parquet\\\")\\nexternal = ExternalCSVs(data_directory=\\\"test_assets/external_submissions\\\")\\nnew_dataf = external.predict(dataf)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataf = create_numerframe(\"test_assets/mini_numerai_version_2_data.parquet\")\n",
    "external = ExternalCSVs(data_directory=\"test_assets/external_submissions\")\n",
    "new_dataf = external.predict(dataf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "                  prediction_test_predictions.csv\nid                                               \nn559bd06a8861222                         0.333333\nn9d39dea58c9e3cf                         0.500000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prediction_test_predictions.csv</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>n559bd06a8861222</th>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>n9d39dea58c9e3cf</th>\n      <td>0.500000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 14;\n                var nbb_unformatted_code = \"assert new_dataf['prediction_test_predictions.csv'].astype(np.float32).equals(new_dataf['target_thomas_20'])\\nnew_dataf.get_prediction_data.head(2)\";\n                var nbb_formatted_code = \"assert (\\n    new_dataf[\\\"prediction_test_predictions.csv\\\"]\\n    .astype(np.float32)\\n    .equals(new_dataf[\\\"target_thomas_20\\\"])\\n)\\nnew_dataf.get_prediction_data.head(2)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert new_dataf['prediction_test_predictions.csv'].astype(np.float32).equals(new_dataf['target_thomas_20'])\n",
    "new_dataf.get_prediction_data.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If no submissions are found in the given `data_directory` you should recieve a warning."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": " WARNING: No csvs found in directory \u001B[32m'Some_nonexisting_directory_12354321'\u001B[0m. \n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> WARNING: No csvs found in directory <span style=\"color: #008000; text-decoration-color: #008000\">'Some_nonexisting_directory_12354321'</span>. \n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 15;\n                var nbb_unformatted_code = \"ExternalCSVs(data_directory=\\\"Some_nonexisting_directory_12354321\\\");\";\n                var nbb_formatted_code = \"ExternalCSVs(data_directory=\\\"Some_nonexisting_directory_12354321\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ExternalCSVs(data_directory=\"Some_nonexisting_directory_12354321\");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading all models in directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Joblib directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many models, like `scikit-learn`, can conveniently be saved as `.joblib` files. This class automatically loads all `.joblib` files in a given folder and generates (averaged out) predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 16;\n                var nbb_unformatted_code = \"#export\\n@typechecked\\nclass JoblibModel(DirectoryModel):\\n    \\\"\\\"\\\"\\n    Load and predict for arbitrary models in directory saved as .joblib.\\n    All loaded models should have a .predict method and accept the features present in the data.\\n    :param model_directory: Main directory from which to read in models.\\n    :param model_name: Name that will be used to create column names and for display purposes.\\n    :param feature_cols: optional list of features to use for prediction.\\n    Selects all feature columns (i.e. column names with prefix 'feature') by default.\\n    \\\"\\\"\\\"\\n    def __init__(self,\\n                 model_directory: str,\\n                 model_name: str = None,\\n                 feature_cols: list = None\\n                 ):\\n        file_suffix = 'joblib'\\n        super().__init__(model_directory=model_directory,\\n                         file_suffix=file_suffix,\\n                         model_name=model_name,\\n                         feature_cols=feature_cols\\n                         )\\n\\n    def load_models(self) -> list:\\n        return [joblib.load(path) for path in self.model_paths]\";\n                var nbb_formatted_code = \"# export\\n@typechecked\\nclass JoblibModel(DirectoryModel):\\n    \\\"\\\"\\\"\\n    Load and predict for arbitrary models in directory saved as .joblib.\\n    All loaded models should have a .predict method and accept the features present in the data.\\n    :param model_directory: Main directory from which to read in models.\\n    :param model_name: Name that will be used to create column names and for display purposes.\\n    :param feature_cols: optional list of features to use for prediction.\\n    Selects all feature columns (i.e. column names with prefix 'feature') by default.\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self, model_directory: str, model_name: str = None, feature_cols: list = None\\n    ):\\n        file_suffix = \\\"joblib\\\"\\n        super().__init__(\\n            model_directory=model_directory,\\n            file_suffix=file_suffix,\\n            model_name=model_name,\\n            feature_cols=feature_cols,\\n        )\\n\\n    def load_models(self) -> list:\\n        return [joblib.load(path) for path in self.model_paths]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "@typechecked\n",
    "class JoblibModel(DirectoryModel):\n",
    "    \"\"\"\n",
    "    Load and predict for arbitrary models in directory saved as .joblib.\n",
    "    All loaded models should have a .predict method and accept the features present in the data.\n",
    "    :param model_directory: Main directory from which to read in models.\n",
    "    :param model_name: Name that will be used to create column names and for display purposes.\n",
    "    :param feature_cols: optional list of features to use for prediction.\n",
    "    Selects all feature columns (i.e. column names with prefix 'feature') by default.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model_directory: str,\n",
    "                 model_name: str = None,\n",
    "                 feature_cols: list = None\n",
    "                 ):\n",
    "        file_suffix = 'joblib'\n",
    "        super().__init__(model_directory=model_directory,\n",
    "                         file_suffix=file_suffix,\n",
    "                         model_name=model_name,\n",
    "                         feature_cols=feature_cols\n",
    "                         )\n",
    "\n",
    "    def load_models(self) -> list:\n",
    "        return [joblib.load(path) for path in self.model_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "JoblibModel: 'Joblib_LGB' prediction:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df75ee78870e4a8f837b5e79273af651"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": " Finished step \u001B[1mDirectoryModel\u001B[0m. Output \u001B[33mshape\u001B[0m=\u001B[1m(\u001B[0m\u001B[1;36m10\u001B[0m, \u001B[1;36m1074\u001B[0m\u001B[1m)\u001B[0m. Time taken for step: \n\u001B[1;34m0:00:00\u001B[0m\u001B[34m.\u001B[0m\u001B[1;34m372850\u001B[0m. \n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Finished step <span style=\"font-weight: bold\">DirectoryModel</span>. Output <span style=\"color: #808000; text-decoration-color: #808000\">shape</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1074</span><span style=\"font-weight: bold\">)</span>. Time taken for step: \n<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">0:00:00</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">372850</span>. \n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                  prediction_Joblib_LGB\nid                                     \nn559bd06a8861222               0.506948\nn9d39dea58c9e3cf               0.492578\nnb64f06d3a9fc9f1               0.490879",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prediction_Joblib_LGB</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>n559bd06a8861222</th>\n      <td>0.506948</td>\n    </tr>\n    <tr>\n      <th>n9d39dea58c9e3cf</th>\n      <td>0.492578</td>\n    </tr>\n    <tr>\n      <th>nb64f06d3a9fc9f1</th>\n      <td>0.490879</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 17;\n                var nbb_unformatted_code = \"dataf = create_numerframe(\\\"test_assets/mini_numerai_version_2_data.parquet\\\", metadata={\\\"version\\\": 2})\\nmodel = JoblibModel(\\\"test_assets\\\", model_name=\\\"Joblib_LGB\\\")\\npredictions = model.predict(dataf).get_prediction_data\\nassert predictions['prediction_Joblib_LGB'].between(0, 1).all()\\npredictions.head(3)\";\n                var nbb_formatted_code = \"dataf = create_numerframe(\\n    \\\"test_assets/mini_numerai_version_2_data.parquet\\\", metadata={\\\"version\\\": 2}\\n)\\nmodel = JoblibModel(\\\"test_assets\\\", model_name=\\\"Joblib_LGB\\\")\\npredictions = model.predict(dataf).get_prediction_data\\nassert predictions[\\\"prediction_Joblib_LGB\\\"].between(0, 1).all()\\npredictions.head(3)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataf = create_numerframe(\"test_assets/mini_numerai_version_2_data.parquet\", metadata={\"version\": 2})\n",
    "model = JoblibModel(\"test_assets\", model_name=\"Joblib_LGB\")\n",
    "predictions = model.predict(dataf).get_prediction_data\n",
    "assert predictions['prediction_Joblib_LGB'].between(0, 1).all()\n",
    "predictions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Catboost directory (.cbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model setup loads in all `CatBoost` (`.cbm`) models present in a given directory and makes (averaged out) predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 18;\n                var nbb_unformatted_code = \"#export\\n@typechecked\\nclass CatBoostModel(DirectoryModel):\\n    \\\"\\\"\\\"\\n    Load and predict with all .cbm models (CatBoostRegressor) in directory.\\n    :param model_directory: Main directory from which to read in models.\\n    :param model_name: Name that will be used to define column names and for display purposes.\\n    :param feature_cols: optional list of features to use for prediction.\\n    Selects all feature columns (i.e. column names with prefix 'feature') by default.\\n    \\\"\\\"\\\"\\n    def __init__(self,\\n                 model_directory: str,\\n                 model_name: str = None,\\n                 feature_cols: list = None\\n                 ):\\n        file_suffix = 'cbm'\\n        super().__init__(model_directory=model_directory,\\n                         file_suffix=file_suffix,\\n                         model_name=model_name,\\n                         feature_cols=feature_cols\\n                         )\\n\\n    def load_models(self) -> list:\\n        return [CatBoost().load_model(path) for path in self.model_paths]\";\n                var nbb_formatted_code = \"# export\\n@typechecked\\nclass CatBoostModel(DirectoryModel):\\n    \\\"\\\"\\\"\\n    Load and predict with all .cbm models (CatBoostRegressor) in directory.\\n    :param model_directory: Main directory from which to read in models.\\n    :param model_name: Name that will be used to define column names and for display purposes.\\n    :param feature_cols: optional list of features to use for prediction.\\n    Selects all feature columns (i.e. column names with prefix 'feature') by default.\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self, model_directory: str, model_name: str = None, feature_cols: list = None\\n    ):\\n        file_suffix = \\\"cbm\\\"\\n        super().__init__(\\n            model_directory=model_directory,\\n            file_suffix=file_suffix,\\n            model_name=model_name,\\n            feature_cols=feature_cols,\\n        )\\n\\n    def load_models(self) -> list:\\n        return [CatBoost().load_model(path) for path in self.model_paths]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "@typechecked\n",
    "class CatBoostModel(DirectoryModel):\n",
    "    \"\"\"\n",
    "    Load and predict with all .cbm models (CatBoostRegressor) in directory.\n",
    "    :param model_directory: Main directory from which to read in models.\n",
    "    :param model_name: Name that will be used to define column names and for display purposes.\n",
    "    :param feature_cols: optional list of features to use for prediction.\n",
    "    Selects all feature columns (i.e. column names with prefix 'feature') by default.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model_directory: str,\n",
    "                 model_name: str = None,\n",
    "                 feature_cols: list = None\n",
    "                 ):\n",
    "        file_suffix = 'cbm'\n",
    "        super().__init__(model_directory=model_directory,\n",
    "                         file_suffix=file_suffix,\n",
    "                         model_name=model_name,\n",
    "                         feature_cols=feature_cols\n",
    "                         )\n",
    "\n",
    "    def load_models(self) -> list:\n",
    "        return [CatBoost().load_model(path) for path in self.model_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": " Finished step \u001B[1mGroupStatsPreProcessor\u001B[0m. Output \u001B[33mshape\u001B[0m=\u001B[1m(\u001B[0m\u001B[1;36m10\u001B[0m, \u001B[1;36m332\u001B[0m\u001B[1m)\u001B[0m. Time taken for step: \n\u001B[1;34m0:00:00\u001B[0m\u001B[34m.\u001B[0m\u001B[1;34m040821\u001B[0m. \n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Finished step <span style=\"font-weight: bold\">GroupStatsPreProcessor</span>. Output <span style=\"color: #808000; text-decoration-color: #808000\">shape</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">332</span><span style=\"font-weight: bold\">)</span>. Time taken for step: \n<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">0:00:00</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">040821</span>. \n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 19;\n                var nbb_unformatted_code = \"from numerai_blocks.preprocessing import GroupStatsPreProcessor\\ndataf = create_numerframe(\\\"test_assets/mini_numerai_version_1_data.csv\\\", {\\\"version\\\": 1})\\nprocessed_dataf = GroupStatsPreProcessor()(dataf)\\nmodel = CatBoostModel(\\\"test_assets\\\", model_name=\\\"CB\\\")\";\n                var nbb_formatted_code = \"from numerai_blocks.preprocessing import GroupStatsPreProcessor\\n\\ndataf = create_numerframe(\\\"test_assets/mini_numerai_version_1_data.csv\\\", {\\\"version\\\": 1})\\nprocessed_dataf = GroupStatsPreProcessor()(dataf)\\nmodel = CatBoostModel(\\\"test_assets\\\", model_name=\\\"CB\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numerai_blocks.preprocessing import GroupStatsPreProcessor\n",
    "dataf = create_numerframe(\"test_assets/mini_numerai_version_1_data.csv\", {\"version\": 1})\n",
    "processed_dataf = GroupStatsPreProcessor()(dataf)\n",
    "model = CatBoostModel(\"test_assets\", model_name=\"CB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "CatBoostModel: 'CB' prediction:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "125da504466243d2b8368f13e41b42d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": " Finished step \u001B[1mDirectoryModel\u001B[0m. Output \u001B[33mshape\u001B[0m=\u001B[1m(\u001B[0m\u001B[1;36m10\u001B[0m, \u001B[1;36m333\u001B[0m\u001B[1m)\u001B[0m. Time taken for step: \u001B[1;34m0:00:00\u001B[0m\u001B[34m.\u001B[0m\u001B[1;34m313855\u001B[0m.\n\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Finished step <span style=\"font-weight: bold\">DirectoryModel</span>. Output <span style=\"color: #808000; text-decoration-color: #808000\">shape</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">333</span><span style=\"font-weight: bold\">)</span>. Time taken for step: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">0:00:00</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">313855</span>.\n\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "   prediction_CB\n0       0.492046\n1       0.499881\n2       0.485325",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prediction_CB</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.492046</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.499881</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.485325</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 20;\n                var nbb_unformatted_code = \"predictions = model.predict(processed_dataf).get_prediction_data\\nassert predictions['prediction_CB'].between(0, 1).all()\\npredictions.head(3)\";\n                var nbb_formatted_code = \"predictions = model.predict(processed_dataf).get_prediction_data\\nassert predictions[\\\"prediction_CB\\\"].between(0, 1).all()\\npredictions.head(3)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict(processed_dataf).get_prediction_data\n",
    "assert predictions['prediction_CB'].between(0, 1).all()\n",
    "predictions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. LightGBM directory (.lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model setup loads in all `LightGBM` (`.lgb`) models present in a given directory and makes (averaged out) predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 21;\n                var nbb_unformatted_code = \"#export\\n@typechecked\\nclass LGBMModel(DirectoryModel):\\n    \\\"\\\"\\\"\\n    Load and predict with all .lgb models (LightGBM) in directory.\\n    :param model_directory: Main directory from which to read in models.\\n    :param model_name: Name that will be used to define column names and for display purposes.\\n    :param feature_cols: optional list of features to use for prediction.\\n    Selects all feature columns (i.e. column names with prefix 'feature') by default.\\n    \\\"\\\"\\\"\\n    def __init__(self,\\n                 model_directory: str,\\n                 model_name: str = None,\\n                 feature_cols: list = None\\n                 ):\\n        file_suffix = 'lgb'\\n        super().__init__(model_directory=model_directory,\\n                         file_suffix=file_suffix,\\n                         model_name=model_name,\\n                         feature_cols=feature_cols\\n                         )\\n\\n    def load_models(self) -> list:\\n        return [lgb.Booster(model_file=str(path)) for path in self.model_paths]\";\n                var nbb_formatted_code = \"# export\\n@typechecked\\nclass LGBMModel(DirectoryModel):\\n    \\\"\\\"\\\"\\n    Load and predict with all .lgb models (LightGBM) in directory.\\n    :param model_directory: Main directory from which to read in models.\\n    :param model_name: Name that will be used to define column names and for display purposes.\\n    :param feature_cols: optional list of features to use for prediction.\\n    Selects all feature columns (i.e. column names with prefix 'feature') by default.\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self, model_directory: str, model_name: str = None, feature_cols: list = None\\n    ):\\n        file_suffix = \\\"lgb\\\"\\n        super().__init__(\\n            model_directory=model_directory,\\n            file_suffix=file_suffix,\\n            model_name=model_name,\\n            feature_cols=feature_cols,\\n        )\\n\\n    def load_models(self) -> list:\\n        return [lgb.Booster(model_file=str(path)) for path in self.model_paths]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "@typechecked\n",
    "class LGBMModel(DirectoryModel):\n",
    "    \"\"\"\n",
    "    Load and predict with all .lgb models (LightGBM) in directory.\n",
    "    :param model_directory: Main directory from which to read in models.\n",
    "    :param model_name: Name that will be used to define column names and for display purposes.\n",
    "    :param feature_cols: optional list of features to use for prediction.\n",
    "    Selects all feature columns (i.e. column names with prefix 'feature') by default.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model_directory: str,\n",
    "                 model_name: str = None,\n",
    "                 feature_cols: list = None\n",
    "                 ):\n",
    "        file_suffix = 'lgb'\n",
    "        super().__init__(model_directory=model_directory,\n",
    "                         file_suffix=file_suffix,\n",
    "                         model_name=model_name,\n",
    "                         feature_cols=feature_cols\n",
    "                         )\n",
    "\n",
    "    def load_models(self) -> list:\n",
    "        return [lgb.Booster(model_file=str(path)) for path in self.model_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "LGBMModel: 'LGB' prediction:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2123a82a5a2141c2907430ed84f374ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": " Finished step \u001B[1mDirectoryModel\u001B[0m. Output \u001B[33mshape\u001B[0m=\u001B[1m(\u001B[0m\u001B[1;36m10\u001B[0m, \u001B[1;36m1074\u001B[0m\u001B[1m)\u001B[0m. Time taken for step: \n\u001B[1;34m0:00:00\u001B[0m\u001B[34m.\u001B[0m\u001B[1;34m398737\u001B[0m. \n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Finished step <span style=\"font-weight: bold\">DirectoryModel</span>. Output <span style=\"color: #808000; text-decoration-color: #808000\">shape</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1074</span><span style=\"font-weight: bold\">)</span>. Time taken for step: \n<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">0:00:00</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">398737</span>. \n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                  prediction_LGB\nid                              \nn559bd06a8861222        0.506948\nn9d39dea58c9e3cf        0.492578\nnb64f06d3a9fc9f1        0.490879",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prediction_LGB</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>n559bd06a8861222</th>\n      <td>0.506948</td>\n    </tr>\n    <tr>\n      <th>n9d39dea58c9e3cf</th>\n      <td>0.492578</td>\n    </tr>\n    <tr>\n      <th>nb64f06d3a9fc9f1</th>\n      <td>0.490879</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 22;\n                var nbb_unformatted_code = \"dataf = create_numerframe(\\\"test_assets/mini_numerai_version_2_data.parquet\\\")\\nmodel = LGBMModel(\\\"test_assets\\\", model_name=\\\"LGB\\\")\\npredictions = model.predict(dataf).get_prediction_data\\nassert predictions['prediction_LGB'].between(0, 1).all()\\npredictions.head(3)\";\n                var nbb_formatted_code = \"dataf = create_numerframe(\\\"test_assets/mini_numerai_version_2_data.parquet\\\")\\nmodel = LGBMModel(\\\"test_assets\\\", model_name=\\\"LGB\\\")\\npredictions = model.predict(dataf).get_prediction_data\\nassert predictions[\\\"prediction_LGB\\\"].between(0, 1).all()\\npredictions.head(3)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataf = create_numerframe(\"test_assets/mini_numerai_version_2_data.parquet\")\n",
    "model = LGBMModel(\"test_assets\", model_name=\"LGB\")\n",
    "predictions = model.predict(dataf).get_prediction_data\n",
    "assert predictions['prediction_LGB'].between(0, 1).all()\n",
    "predictions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a baseline is always an important step for data science problems. This section introduces models that should only be used a baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. ConstantModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model simply outputs a constant of your choice. Convenient for setting classification baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 23;\n                var nbb_unformatted_code = \"#export\\nclass ConstantModel(BaseModel):\\n    \\\"\\\"\\\"\\n    WARNING: Only use this Model for testing purposes.\\n    Create constant prediction.\\n    :param constant: Value for constant prediction.\\n    :param model_name: Name that will be used to create column names and for display purposes.\\n    \\\"\\\"\\\"\\n    def __init__(self, constant: float = 0.5, model_name: str = None):\\n        self.constant = constant\\n        model_name = model_name if model_name else f\\\"constant_{self.constant}\\\"\\n        super().__init__(model_directory=\\\"\\\",\\n                         model_name=model_name\\n                         )\\n        self.clf = DummyRegressor(strategy='constant', constant=constant).fit([0.], [0.])\\n\\n    def predict(self, dataf: NumerFrame) -> NumerFrame:\\n        dataf.loc[:, self.prediction_col_name] = self.clf.predict(dataf.get_feature_data)\\n        return NumerFrame(dataf)\";\n                var nbb_formatted_code = \"# export\\nclass ConstantModel(BaseModel):\\n    \\\"\\\"\\\"\\n    WARNING: Only use this Model for testing purposes.\\n    Create constant prediction.\\n    :param constant: Value for constant prediction.\\n    :param model_name: Name that will be used to create column names and for display purposes.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, constant: float = 0.5, model_name: str = None):\\n        self.constant = constant\\n        model_name = model_name if model_name else f\\\"constant_{self.constant}\\\"\\n        super().__init__(model_directory=\\\"\\\", model_name=model_name)\\n        self.clf = DummyRegressor(strategy=\\\"constant\\\", constant=constant).fit(\\n            [0.0], [0.0]\\n        )\\n\\n    def predict(self, dataf: NumerFrame) -> NumerFrame:\\n        dataf.loc[:, self.prediction_col_name] = self.clf.predict(\\n            dataf.get_feature_data\\n        )\\n        return NumerFrame(dataf)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "class ConstantModel(BaseModel):\n",
    "    \"\"\"\n",
    "    WARNING: Only use this Model for testing purposes.\n",
    "    Create constant prediction.\n",
    "    :param constant: Value for constant prediction.\n",
    "    :param model_name: Name that will be used to create column names and for display purposes.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant: float = 0.5, model_name: str = None):\n",
    "        self.constant = constant\n",
    "        model_name = model_name if model_name else f\"constant_{self.constant}\"\n",
    "        super().__init__(model_directory=\"\",\n",
    "                         model_name=model_name\n",
    "                         )\n",
    "        self.clf = DummyRegressor(strategy='constant', constant=constant).fit([0.], [0.])\n",
    "\n",
    "    def predict(self, dataf: NumerFrame) -> NumerFrame:\n",
    "        dataf.loc[:, self.prediction_col_name] = self.clf.predict(dataf.get_feature_data)\n",
    "        return NumerFrame(dataf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   prediction_constant_0.85\n0                      0.85\n1                      0.85\n2                      0.85",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prediction_constant_0.85</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.85</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.85</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.85</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 24;\n                var nbb_unformatted_code = \"constant = 0.85\\ndataf = create_numerframe(\\\"test_assets/mini_numerai_version_1_data.csv\\\")\\nconstant_model = ConstantModel(constant=constant)\\npredictions = constant_model.predict(dataf).get_prediction_data\\nassert (predictions.to_numpy() == constant).all()\\npredictions.head(3)\";\n                var nbb_formatted_code = \"constant = 0.85\\ndataf = create_numerframe(\\\"test_assets/mini_numerai_version_1_data.csv\\\")\\nconstant_model = ConstantModel(constant=constant)\\npredictions = constant_model.predict(dataf).get_prediction_data\\nassert (predictions.to_numpy() == constant).all()\\npredictions.head(3)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "constant = 0.85\n",
    "dataf = create_numerframe(\"test_assets/mini_numerai_version_1_data.csv\")\n",
    "constant_model = ConstantModel(constant=constant)\n",
    "predictions = constant_model.predict(dataf).get_prediction_data\n",
    "assert (predictions.to_numpy() == constant).all()\n",
    "predictions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. RandomModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model returns uniformly distributed predictions (range $[0...1)$). Good baseline for regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 25;\n                var nbb_unformatted_code = \"#export\\nclass RandomModel(BaseModel):\\n    \\\"\\\"\\\"\\n    WARNING: Only use this Model for testing purposes.\\n    Create uniformly distributed predictions.\\n    :param model_name: Name that will be used to create column names and for display purposes.\\n    \\\"\\\"\\\"\\n    def __init__(self, model_name: str = None):\\n        model_name = model_name if model_name else \\\"random\\\"\\n        super().__init__(model_directory=\\\"\\\",\\n                         model_name=model_name\\n                         )\\n\\n    def predict(self, dataf: Union[pd.DataFrame, NumerFrame]) -> NumerFrame:\\n        dataf.loc[:, self.prediction_col_name] = np.random.uniform(size=len(dataf))\\n        return NumerFrame(dataf)\";\n                var nbb_formatted_code = \"# export\\nclass RandomModel(BaseModel):\\n    \\\"\\\"\\\"\\n    WARNING: Only use this Model for testing purposes.\\n    Create uniformly distributed predictions.\\n    :param model_name: Name that will be used to create column names and for display purposes.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, model_name: str = None):\\n        model_name = model_name if model_name else \\\"random\\\"\\n        super().__init__(model_directory=\\\"\\\", model_name=model_name)\\n\\n    def predict(self, dataf: Union[pd.DataFrame, NumerFrame]) -> NumerFrame:\\n        dataf.loc[:, self.prediction_col_name] = np.random.uniform(size=len(dataf))\\n        return NumerFrame(dataf)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "class RandomModel(BaseModel):\n",
    "    \"\"\"\n",
    "    WARNING: Only use this Model for testing purposes.\n",
    "    Create uniformly distributed predictions.\n",
    "    :param model_name: Name that will be used to create column names and for display purposes.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = None):\n",
    "        model_name = model_name if model_name else \"random\"\n",
    "        super().__init__(model_directory=\"\",\n",
    "                         model_name=model_name\n",
    "                         )\n",
    "\n",
    "    def predict(self, dataf: Union[pd.DataFrame, NumerFrame]) -> NumerFrame:\n",
    "        dataf.loc[:, self.prediction_col_name] = np.random.uniform(size=len(dataf))\n",
    "        return NumerFrame(dataf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   prediction_random\n0           0.793875\n1           0.536771\n2           0.180774",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prediction_random</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.793875</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.536771</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.180774</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 26;\n                var nbb_unformatted_code = \"dataf = create_numerframe(\\\"test_assets/mini_numerai_version_1_data.csv\\\")\\nrandom_model = RandomModel()\\npredictions = random_model.predict(dataf).get_prediction_data\\nassert predictions['prediction_random'].between(0, 1).all()\\npredictions.head(3)\";\n                var nbb_formatted_code = \"dataf = create_numerframe(\\\"test_assets/mini_numerai_version_1_data.csv\\\")\\nrandom_model = RandomModel()\\npredictions = random_model.predict(dataf).get_prediction_data\\nassert predictions[\\\"prediction_random\\\"].between(0, 1).all()\\npredictions.head(3)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataf = create_numerframe(\"test_assets/mini_numerai_version_1_data.csv\")\n",
    "random_model = RandomModel()\n",
    "predictions = random_model.predict(dataf).get_prediction_data\n",
    "assert predictions['prediction_random'].between(0, 1).all()\n",
    "predictions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Example (validation) predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Model performs downloading and adding of example predictions for Numerai Classic. Convenient when you are constructing a `ModelPipeline` (see section 6) and want to include example predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 27;\n                var nbb_unformatted_code = \"#export\\n@typechecked\\nclass ExamplePredictionsModel(BaseModel):\\n    \\\"\\\"\\\"\\n    Load example predictions and add to NumerFrame.\\n    :param file_name: File to download from NumerAPI.\\n    'example_validation_predictions.parquet' by default.\\n    :param data_directory: Directory path to download example predictions to\\n    or directory where example data already exists.\\n    :param round_num: Optional round number. Downloads most recent round by default.\\n    \\\"\\\"\\\"\\n    def __init__(self, file_name: str = \\\"example_validation_predictions.parquet\\\",\\n                 data_directory: str = \\\"example_predictions_model\\\",\\n                 round_num: int = None):\\n        super().__init__(model_directory=\\\"\\\",\\n                         model_name=\\\"example\\\",\\n                         )\\n        self.file_name = file_name\\n        self.data_directory = data_directory\\n        self.round_num = round_num\\n\\n    @display_processor_info\\n    def predict(self, dataf: NumerFrame) -> NumerFrame:\\n        \\\"\\\"\\\" Return NumerFrame with added example predictions. \\\"\\\"\\\"\\n        self._download_example_preds()\\n        example_preds = self._load_example_preds()\\n        dataf.loc[:, self.prediction_col_name] = dataf.merge(example_preds, on='id', how='left')['prediction']\\n        self.downloader.remove_base_directory()\\n        return NumerFrame(dataf)\\n\\n    def _download_example_preds(self):\\n        self.downloader = NumeraiClassicDownloader(directory_path=self.data_directory)\\n        self.dest_path = f\\\"{str(self.downloader.dir)}/{self.file_name}\\\"\\n        self.downloader.download_single_dataset(filename=self.file_name,\\n                                                dest_path=self.dest_path,\\n                                                round_num=self.round_num)\\n\\n    def _load_example_preds(self, *args, **kwargs):\\n        return pd.read_parquet(self.dest_path, *args, **kwargs)\";\n                var nbb_formatted_code = \"# export\\n@typechecked\\nclass ExamplePredictionsModel(BaseModel):\\n    \\\"\\\"\\\"\\n    Load example predictions and add to NumerFrame.\\n    :param file_name: File to download from NumerAPI.\\n    'example_validation_predictions.parquet' by default.\\n    :param data_directory: Directory path to download example predictions to\\n    or directory where example data already exists.\\n    :param round_num: Optional round number. Downloads most recent round by default.\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        file_name: str = \\\"example_validation_predictions.parquet\\\",\\n        data_directory: str = \\\"example_predictions_model\\\",\\n        round_num: int = None,\\n    ):\\n        super().__init__(\\n            model_directory=\\\"\\\",\\n            model_name=\\\"example\\\",\\n        )\\n        self.file_name = file_name\\n        self.data_directory = data_directory\\n        self.round_num = round_num\\n\\n    @display_processor_info\\n    def predict(self, dataf: NumerFrame) -> NumerFrame:\\n        \\\"\\\"\\\"Return NumerFrame with added example predictions.\\\"\\\"\\\"\\n        self._download_example_preds()\\n        example_preds = self._load_example_preds()\\n        dataf.loc[:, self.prediction_col_name] = dataf.merge(\\n            example_preds, on=\\\"id\\\", how=\\\"left\\\"\\n        )[\\\"prediction\\\"]\\n        self.downloader.remove_base_directory()\\n        return NumerFrame(dataf)\\n\\n    def _download_example_preds(self):\\n        self.downloader = NumeraiClassicDownloader(directory_path=self.data_directory)\\n        self.dest_path = f\\\"{str(self.downloader.dir)}/{self.file_name}\\\"\\n        self.downloader.download_single_dataset(\\n            filename=self.file_name, dest_path=self.dest_path, round_num=self.round_num\\n        )\\n\\n    def _load_example_preds(self, *args, **kwargs):\\n        return pd.read_parquet(self.dest_path, *args, **kwargs)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "@typechecked\n",
    "class ExamplePredictionsModel(BaseModel):\n",
    "    \"\"\"\n",
    "    Load example predictions and add to NumerFrame.\n",
    "    :param file_name: File to download from NumerAPI.\n",
    "    'example_validation_predictions.parquet' by default.\n",
    "    :param data_directory: Directory path to download example predictions to\n",
    "    or directory where example data already exists.\n",
    "    :param round_num: Optional round number. Downloads most recent round by default.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_name: str = \"example_validation_predictions.parquet\",\n",
    "                 data_directory: str = \"example_predictions_model\",\n",
    "                 round_num: int = None):\n",
    "        super().__init__(model_directory=\"\",\n",
    "                         model_name=\"example\",\n",
    "                         )\n",
    "        self.file_name = file_name\n",
    "        self.data_directory = data_directory\n",
    "        self.round_num = round_num\n",
    "\n",
    "    @display_processor_info\n",
    "    def predict(self, dataf: NumerFrame) -> NumerFrame:\n",
    "        \"\"\" Return NumerFrame with added example predictions. \"\"\"\n",
    "        self._download_example_preds()\n",
    "        example_preds = self._load_example_preds()\n",
    "        dataf.loc[:, self.prediction_col_name] = dataf.merge(example_preds, on='id', how='left')['prediction']\n",
    "        self.downloader.remove_base_directory()\n",
    "        return NumerFrame(dataf)\n",
    "\n",
    "    def _download_example_preds(self):\n",
    "        self.downloader = NumeraiClassicDownloader(directory_path=self.data_directory)\n",
    "        self.dest_path = f\"{str(self.downloader.dir)}/{self.file_name}\"\n",
    "        self.downloader.download_single_dataset(filename=self.file_name,\n",
    "                                                dest_path=self.dest_path,\n",
    "                                                round_num=self.round_num)\n",
    "\n",
    "    def _load_example_preds(self, *args, **kwargs):\n",
    "        return pd.read_parquet(self.dest_path, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "No existing directory found at \u001B[32m'\u001B[0m\u001B[34mexample_predictions_model\u001B[0m\u001B[32m'\u001B[0m. Creating directory\u001B[33m...\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">No existing directory found at <span style=\"color: #008000; text-decoration-color: #008000\">'</span><span style=\"color: #000080; text-decoration-color: #000080\">example_predictions_model</span><span style=\"color: #008000; text-decoration-color: #008000\">'</span>. Creating directory<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": " \u001B[32mDownloading\u001B[0m \u001B[32m'numerai_validation_data.parquet'\u001B[0m \n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> <span style=\"color: #008000; text-decoration-color: #008000\">Downloading</span> <span style=\"color: #008000; text-decoration-color: #008000\">'numerai_validation_data.parquet'</span> \n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-28 18:56:16,421 INFO numerapi.utils: starting download\n",
      "example_predictions_model/numerai_validation_data.parquet: 228MB [00:47, 4.74MB/s]                              \n"
     ]
    },
    {
     "data": {
      "text/plain": " \u001B[32mDownloading\u001B[0m \u001B[32m'example_validation_predictions.parquet'\u001B[0m \n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> <span style=\"color: #008000; text-decoration-color: #008000\">Downloading</span> <span style=\"color: #008000; text-decoration-color: #008000\">'example_validation_predictions.parquet'</span> \n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-28 18:57:08,822 INFO numerapi.utils: starting download\n",
      "example_predictions_model/example_validation_predictions.parquet: 13.0MB [00:02, 5.80MB/s]                            \n"
     ]
    },
    {
     "data": {
      "text/plain": " \u001B[31mDeleting directory for \u001B[0m\u001B[31m'NumeraiClassicDownloader\u001B[0m\u001B[32m'\u001B[0m \nPath: \u001B[32m'/Users/clepelaars/Desktop/crowdcent/repositories/numerai-blocks/nbs/example_prediction\u001B[0m\n\u001B[32ms_model'\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> <span style=\"color: #800000; text-decoration-color: #800000\">Deleting directory for </span><span style=\"color: #800000; text-decoration-color: #800000\">'NumeraiClassicDownloader</span><span style=\"color: #008000; text-decoration-color: #008000\">'</span> \nPath: <span style=\"color: #008000; text-decoration-color: #008000\">'/Users/clepelaars/Desktop/crowdcent/repositories/numerai-blocks/nbs/example_prediction</span>\n<span style=\"color: #008000; text-decoration-color: #008000\">s_model'</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": " Finished step \u001B[1mExamplePredictionsModel\u001B[0m. Output \u001B[33mshape\u001B[0m=\u001B[1m(\u001B[0m\u001B[1;36m539658\u001B[0m, \u001B[1;36m1074\u001B[0m\u001B[1m)\u001B[0m. Time taken for step: \n\u001B[1;34m0:00:06\u001B[0m\u001B[34m.\u001B[0m\u001B[1;34m423734\u001B[0m. \n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Finished step <span style=\"font-weight: bold\">ExamplePredictionsModel</span>. Output <span style=\"color: #808000; text-decoration-color: #808000\">shape</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">539658</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1074</span><span style=\"font-weight: bold\">)</span>. Time taken for step: \n<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">0:00:06</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">423734</span>. \n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                  prediction_example\nid                                  \nn000777698096000            0.228263\nn0009793a3b91c27            0.731198\nn00099ccd6698ab0            0.846885",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prediction_example</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>n000777698096000</th>\n      <td>0.228263</td>\n    </tr>\n    <tr>\n      <th>n0009793a3b91c27</th>\n      <td>0.731198</td>\n    </tr>\n    <tr>\n      <th>n00099ccd6698ab0</th>\n      <td>0.846885</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 28;\n                var nbb_unformatted_code = \"# slow\\n# Download validation data\\ndownloader = NumeraiClassicDownloader(\\\"example_predictions_model\\\")\\nval_file = \\\"numerai_validation_data.parquet\\\"\\nval_save_path = f\\\"{str(downloader.dir)}/{val_file}\\\"\\ndownloader.download_single_dataset(filename=val_file,\\n                                   dest_path=val_save_path)\\n\\n# Load validation data and add example predictions\\ndataf = create_numerframe(val_save_path)\\nexample_model = ExamplePredictionsModel()\\npredictions = example_model.predict(dataf).get_prediction_data\\nassert predictions['prediction_example'].between(0, 1).all()\\npredictions.head(3)\";\n                var nbb_formatted_code = \"# slow\\n# Download validation data\\ndownloader = NumeraiClassicDownloader(\\\"example_predictions_model\\\")\\nval_file = \\\"numerai_validation_data.parquet\\\"\\nval_save_path = f\\\"{str(downloader.dir)}/{val_file}\\\"\\ndownloader.download_single_dataset(filename=val_file, dest_path=val_save_path)\\n\\n# Load validation data and add example predictions\\ndataf = create_numerframe(val_save_path)\\nexample_model = ExamplePredictionsModel()\\npredictions = example_model.predict(dataf).get_prediction_data\\nassert predictions[\\\"prediction_example\\\"].between(0, 1).all()\\npredictions.head(3)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "# Download validation data\n",
    "downloader = NumeraiClassicDownloader(\"example_predictions_model\")\n",
    "val_file = \"numerai_validation_data.parquet\"\n",
    "val_save_path = f\"{str(downloader.dir)}/{val_file}\"\n",
    "downloader.download_single_dataset(filename=val_file,\n",
    "                                   dest_path=val_save_path)\n",
    "\n",
    "# Load validation data and add example predictions\n",
    "dataf = create_numerframe(val_save_path)\n",
    "example_model = ExamplePredictionsModel()\n",
    "predictions = example_model.predict(dataf).get_prediction_data\n",
    "assert predictions['prediction_example'].between(0, 1).all()\n",
    "predictions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two different ways to implement new models. Both have their own conveniences and use cases.\n",
    "**4.1.** Inherit from `BaseModel` (custom prediction logic).\n",
    "**4.2.** Inherit from `DirectoryModel` (make predictions for all models in directory with given file suffix.\n",
    "Prediction logic will already be implemented. Only implement model loading logic).\n",
    "\n",
    "**4.1. (From BaseModel)** works well when you have no or only a single file that you use for generating predictions.\n",
    "Examples:\n",
    "1. Loading a model is not relevant or your model is already loaded in memory.\n",
    "2. You would like predictions for one model loaded from disk.\n",
    "3. The object you are loading already aggregates multiple models and transformation steps (such as [scikit-learn FeatureUnion](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html)).\n",
    "\n",
    "**4.2. (From DirectoryModel)** is convenient when you have a lot of similar models in a directory and want to generate predictions for all of them.\n",
    "Examples:\n",
    "1. You have multiple similar models saved through a cross validation process.\n",
    "2. You have a bagging strategy where you have a lot of models trained on slightly different data or with different initializations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. From BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arbitrary models can be instantiated and use for prediction generation by inheriting from `BaseModel`. Arbitrary logic (model loading, prediction, etc.) can be defined in `.predict` as long as the method takes a `NumerFrame` as input and outputs a `NumerFrame`. The Model should be able to typecheck by adding the `@typeguard.typechecked` decorator at the top of the class.\n",
    "\n",
    "For clear console output we recommend adding the `@display_processor_info` decorator to the `.predict` method.\n",
    "\n",
    "If your model does not involve reading files from disk specify `model_directory=\"\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 29;\n                var nbb_unformatted_code = \"#export\\n@typechecked\\nclass AwesomeModel(BaseModel):\\n    \\\"\\\"\\\"\\n    - TEMPLATE -\\n    Predict with arbitrary prediction logic and model formats.\\n\\n    :param model_directory: Main directory from which to read in models.\\n    :param model_name: Name that will be used to define column names and for display purposes.\\n    :param feature_cols: optional list of features to use for prediction.\\n    Selects all feature columns (i.e. column names with prefix 'feature') by default.\\n    \\\"\\\"\\\"\\n    def __init__(self, model_directory: str, model_name: str = None,\\n                 feature_cols: list = None):\\n        super().__init__(model_directory=model_directory,\\n                         model_name=model_name,\\n                         )\\n        self.feature_cols = feature_cols\\n\\n    @display_processor_info\\n    def predict(self, dataf: NumerFrame) -> NumerFrame:\\n        \\\"\\\"\\\" Return NumerFrame with column(s) added for prediction(s). \\\"\\\"\\\"\\n        # Get all features\\n        feature_cols = self.feature_cols if self.feature_cols else dataf.feature_cols\\n        feature_df = dataf[feature_cols]\\n        # Predict and add to new column\\n        ...\\n        # Parse all contents of NumerFrame to the next pipeline step\\n        return NumerFrame(dataf)\";\n                var nbb_formatted_code = \"# export\\n@typechecked\\nclass AwesomeModel(BaseModel):\\n    \\\"\\\"\\\"\\n    - TEMPLATE -\\n    Predict with arbitrary prediction logic and model formats.\\n\\n    :param model_directory: Main directory from which to read in models.\\n    :param model_name: Name that will be used to define column names and for display purposes.\\n    :param feature_cols: optional list of features to use for prediction.\\n    Selects all feature columns (i.e. column names with prefix 'feature') by default.\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self, model_directory: str, model_name: str = None, feature_cols: list = None\\n    ):\\n        super().__init__(\\n            model_directory=model_directory,\\n            model_name=model_name,\\n        )\\n        self.feature_cols = feature_cols\\n\\n    @display_processor_info\\n    def predict(self, dataf: NumerFrame) -> NumerFrame:\\n        \\\"\\\"\\\"Return NumerFrame with column(s) added for prediction(s).\\\"\\\"\\\"\\n        # Get all features\\n        feature_cols = self.feature_cols if self.feature_cols else dataf.feature_cols\\n        feature_df = dataf[feature_cols]\\n        # Predict and add to new column\\n        ...\\n        # Parse all contents of NumerFrame to the next pipeline step\\n        return NumerFrame(dataf)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "@typechecked\n",
    "class AwesomeModel(BaseModel):\n",
    "    \"\"\"\n",
    "    - TEMPLATE -\n",
    "    Predict with arbitrary prediction logic and model formats.\n",
    "\n",
    "    :param model_directory: Main directory from which to read in models.\n",
    "    :param model_name: Name that will be used to define column names and for display purposes.\n",
    "    :param feature_cols: optional list of features to use for prediction.\n",
    "    Selects all feature columns (i.e. column names with prefix 'feature') by default.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_directory: str, model_name: str = None,\n",
    "                 feature_cols: list = None):\n",
    "        super().__init__(model_directory=model_directory,\n",
    "                         model_name=model_name,\n",
    "                         )\n",
    "        self.feature_cols = feature_cols\n",
    "\n",
    "    @display_processor_info\n",
    "    def predict(self, dataf: NumerFrame) -> NumerFrame:\n",
    "        \"\"\" Return NumerFrame with column(s) added for prediction(s). \"\"\"\n",
    "        # Get all features\n",
    "        feature_cols = self.feature_cols if self.feature_cols else dataf.feature_cols\n",
    "        feature_df = dataf[feature_cols]\n",
    "        # Predict and add to new column\n",
    "        ...\n",
    "        # Parse all contents of NumerFrame to the next pipeline step\n",
    "        return NumerFrame(dataf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. From DirectoryModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to implement a setup similar to `JoblibModel` and `CatBoostModel`. Namely, load in all models of a certain type from a directory, predict for all and take the average. If this is your use case, inherit from `DirectoryModel` and be sure to implement the `load_models` method.\n",
    "\n",
    "For a `DirectoryModel` you should specify a `file_suffix` (like `.joblib` or `.cbm`) which will be used to store all available models in `self.model_paths`.\n",
    "\n",
    "The `.predict` method will in this case already be implemented, but can be overridden if the prediction logic is more complex. For example, if you want to apply weighted averaging or a geometric mean for models within a given directory.\n",
    "\n",
    "\n",
    "Like with inheriting from `BaseModel`, This Model should also be able to typecheck by adding the `@typeguard.typechecked` decorator at the top of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 30;\n                var nbb_unformatted_code = \"#export\\n@typechecked\\nclass AwesomeDirectoryModel(DirectoryModel):\\n    \\\"\\\"\\\"\\n    - TEMPLATE -\\n    Load in all models of arbitrary file format and predict for all.\\n\\n    :param model_directory: Main directory from which to read in models.\\n    :param model_name: Name that will be used to define column names and for display purposes.\\n    :param feature_cols: optional list of features to use for prediction.\\n    Selects all feature columns (i.e. column names with prefix 'feature') by default.\\n    \\\"\\\"\\\"\\n    def __init__(self,\\n                 model_directory: str,\\n                 model_name: str = None,\\n                 feature_cols: list = None\\n                 ):\\n        file_suffix = '.anything'\\n        super().__init__(model_directory=model_directory,\\n                         file_suffix=file_suffix,\\n                         model_name=model_name,\\n                         feature_cols=feature_cols\\n                         )\\n\\n    def load_models(self) -> list:\\n        \\\"\\\"\\\" Instantiate all models and return as a list. (abstract method) \\\"\\\"\\\"\\n        ...\";\n                var nbb_formatted_code = \"# export\\n@typechecked\\nclass AwesomeDirectoryModel(DirectoryModel):\\n    \\\"\\\"\\\"\\n    - TEMPLATE -\\n    Load in all models of arbitrary file format and predict for all.\\n\\n    :param model_directory: Main directory from which to read in models.\\n    :param model_name: Name that will be used to define column names and for display purposes.\\n    :param feature_cols: optional list of features to use for prediction.\\n    Selects all feature columns (i.e. column names with prefix 'feature') by default.\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self, model_directory: str, model_name: str = None, feature_cols: list = None\\n    ):\\n        file_suffix = \\\".anything\\\"\\n        super().__init__(\\n            model_directory=model_directory,\\n            file_suffix=file_suffix,\\n            model_name=model_name,\\n            feature_cols=feature_cols,\\n        )\\n\\n    def load_models(self) -> list:\\n        \\\"\\\"\\\"Instantiate all models and return as a list. (abstract method)\\\"\\\"\\\"\\n        ...\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "@typechecked\n",
    "class AwesomeDirectoryModel(DirectoryModel):\n",
    "    \"\"\"\n",
    "    - TEMPLATE -\n",
    "    Load in all models of arbitrary file format and predict for all.\n",
    "\n",
    "    :param model_directory: Main directory from which to read in models.\n",
    "    :param model_name: Name that will be used to define column names and for display purposes.\n",
    "    :param feature_cols: optional list of features to use for prediction.\n",
    "    Selects all feature columns (i.e. column names with prefix 'feature') by default.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model_directory: str,\n",
    "                 model_name: str = None,\n",
    "                 feature_cols: list = None\n",
    "                 ):\n",
    "        file_suffix = '.anything'\n",
    "        super().__init__(model_directory=model_directory,\n",
    "                         file_suffix=file_suffix,\n",
    "                         model_name=model_name,\n",
    "                         feature_cols=feature_cols\n",
    "                         )\n",
    "\n",
    "    def load_models(self) -> list:\n",
    "        \"\"\" Instantiate all models and return as a list. (abstract method) \"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_misc.ipynb.\n",
      "Converted 01_download.ipynb.\n",
      "Converted 02_numerframe.ipynb.\n",
      "Converted 03_preprocessing.ipynb.\n",
      "Converted 04_model.ipynb.\n",
      "Converted 05_postprocessing.ipynb.\n",
      "Converted 06_modelpipeline.ipynb.\n",
      "Converted 07_evaluation.ipynb.\n",
      "Converted 08_key.ipynb.\n",
      "Converted 09_submission.ipynb.\n",
      "Converted 10_staking.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 31;\n                var nbb_unformatted_code = \"# hide\\n# Run this cell to sync all changes with library\\nfrom nbdev.export import notebook2script\\n\\nnotebook2script()\";\n                var nbb_formatted_code = \"# hide\\n# Run this cell to sync all changes with library\\nfrom nbdev.export import notebook2script\\n\\nnotebook2script()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "# Run this cell to sync all changes with library\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}