{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 1;\n                var nbb_unformatted_code = \"%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\\n%load_ext lab_black\";\n                var nbb_formatted_code = \"%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\\n%load_ext lab_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 2;\n                var nbb_unformatted_code = \"# default_exp evaluation\";\n                var nbb_formatted_code = \"# default_exp evaluation\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default_exp evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section provides convenient evaluation schemes for both Numerai Classic and Signals.\n",
    "The `Evaluator` takes a `NumerFrame` is input and return a Pandas DataFrame containing metrics for each prediction column that is given."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 3;\n                var nbb_unformatted_code = \"# export\\nimport numpy as np\\nimport pandas as pd\\nfrom tqdm.auto import tqdm\\nfrom typing import Tuple, Union\\n\\nfrom numerai_blocks.numerframe import NumerFrame, create_numerframe\\nfrom numerai_blocks.postprocessing import FeatureNeutralizer\";\n                var nbb_formatted_code = \"# export\\nimport numpy as np\\nimport pandas as pd\\nfrom tqdm.auto import tqdm\\nfrom typing import Tuple, Union\\n\\nfrom numerai_blocks.numerframe import NumerFrame, create_numerframe\\nfrom numerai_blocks.postprocessing import FeatureNeutralizer\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Union\n",
    "\n",
    "from numerai_blocks.numerframe import NumerFrame, create_numerframe\n",
    "from numerai_blocks.postprocessing import FeatureNeutralizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Base"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`BaseEvaluator` implements all the evaluation logic that is common for Numerai Classic and Signals. This includes:\n",
    "- Mean, Standard Deviation and Sharpe for era returns.\n",
    "- Max drawdown\n",
    "- Annual Percentage Yield (APY)\n",
    "- Mean, Standard deviation and Sharpe for [MMC (Meta Model Contribution)](https://docs.numer.ai/tournament/metamodel-contribution) returns.\n",
    "- Correlation with example predictions\n",
    "- Max [feature exposure](https://forum.numer.ai/t/model-diagnostics-feature-exposure/899)\n",
    "- [Feature neutral](https://docs.numer.ai/tournament/feature-neutral-correlation) Mean, Standard deviation and Sharpe\n",
    "- Mean, Standard Deviation and Sharpe for TB200 (Buy top 200 stocks and sell bottom 200 stocks).\n",
    "- Mean, Standard Deviation and Sharpe for TB500 (Buy top 500 stocks and sell bottom 500 stocks).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 4;\n                var nbb_unformatted_code = \"# export\\nclass BaseEvaluator:\\n    \\\"\\\"\\\"\\n    Evaluation functionality that is relevant for both\\n    Numerai Classic and Numerai Signals.\\n    :param era_col: Column name pointing to eras.\\n    Most commonly \\\"era\\\" for Numerai Classic and \\\"friday_date\\\" for Numerai Signals.\\n    :param fast_mode: Will skip compute intensive metrics if set to True,\\n    namely max_exposure, feature neutral mean, TB200 and TB500.\\n    \\\"\\\"\\\"\\n    def __init__(self, era_col: str = \\\"era\\\", fast_mode = False):\\n        self.era_col = era_col\\n        self.fast_mode = fast_mode\\n\\n    def full_evaluation(self,\\n                        dataf: NumerFrame,\\n                        example_col: str,\\n                        pred_cols: list = None,\\n                        target_col: str = \\\"target\\\"\\n                        ) -> pd.DataFrame:\\n        \\\"\\\"\\\"\\n        Perform evaluation for each prediction column in the NumerFrame\\n        against give target and example prediction column.\\n        \\\"\\\"\\\"\\n        val_stats = pd.DataFrame()\\n        dataf = dataf.fillna(0.5)\\n        pred_cols = dataf.prediction_cols if not pred_cols else pred_cols\\n        for col in tqdm(pred_cols, desc=\\\"Evaluation: \\\"):\\n            col_stats = self.evaluation_one_col(dataf=dataf, pred_col=col,\\n                                                target_col=target_col,\\n                                                example_col=example_col)\\n            val_stats = pd.concat([val_stats, col_stats], axis=0)\\n        return val_stats\\n\\n    def evaluation_one_col(self, dataf: Union[pd.DataFrame, NumerFrame], pred_col: str, target_col: str, example_col: str):\\n        \\\"\\\"\\\"\\n        Perform evaluation for one prediction column\\n        against given target and example prediction column.\\n        \\\"\\\"\\\"\\n        col_stats = pd.DataFrame()\\n        # Compute stats\\n        val_corrs = self.per_era_corrs(dataf=dataf,\\n                                        pred_col=pred_col,\\n                                        target_col=target_col\\n                                       )\\n        mean, std, sharpe = self.mean_std_sharpe(era_corrs=val_corrs)\\n        max_drawdown = self.max_drawdown(era_corrs=val_corrs)\\n        apy = self.apy(era_corrs=val_corrs)\\n        example_corr = self.example_correlation(dataf=dataf,\\n                                                pred_col=pred_col,\\n                                                example_col=example_col\\n                                                )\\n        mmc_mean, mmc_std, mmc_sharpe = self.mmc(dataf=dataf,\\n                                                 pred_col=pred_col,\\n                                                 target_col=target_col,\\n                                                 example_col=example_col\\n                                                 )\\n\\n        col_stats.loc[pred_col, \\\"target\\\"] = target_col\\n        col_stats.loc[pred_col, \\\"mean\\\"] = mean\\n        col_stats.loc[pred_col, \\\"std\\\"] = std\\n        col_stats.loc[pred_col, \\\"sharpe\\\"] = sharpe\\n        col_stats.loc[pred_col, \\\"max_drawdown\\\"] = max_drawdown\\n        col_stats.loc[pred_col, \\\"apy\\\"] = apy\\n        col_stats.loc[pred_col, \\\"mmc_mean\\\"] = mmc_mean\\n        col_stats.loc[pred_col, \\\"mmc_std\\\"] = mmc_std\\n        col_stats.loc[pred_col, \\\"mmc_sharpe\\\"] = mmc_sharpe\\n        col_stats.loc[pred_col, \\\"corr_with_example_preds\\\"] = example_corr\\n\\n        # Compute intensive stats\\n        if not self.fast_mode:\\n            max_feature_exposure = self.max_feature_exposure(dataf=dataf, pred_col=pred_col)\\n            fn_mean, fn_std, fn_sharpe = self.feature_neutral_mean_std_sharpe(dataf=dataf,\\n                                                                              pred_col=pred_col,\\n                                                                              target_col=target_col\\n                                                                              )\\n            tb200_mean, tb200_std, tb200_sharpe = self.tbx_mean_std_sharpe(dataf=dataf,\\n                                                                           pred_col=pred_col,\\n                                                                           target_col=target_col,\\n                                                                           tb=200\\n                                                                           )\\n            tb500_mean, tb500_std, tb500_sharpe = self.tbx_mean_std_sharpe(dataf=dataf,\\n                                                                           pred_col=pred_col,\\n                                                                           target_col=target_col,\\n                                                                           tb=500\\n                                                                           )\\n            col_stats.loc[pred_col, \\\"max_feature_exposure\\\"] = max_feature_exposure\\n            col_stats.loc[pred_col, \\\"feature_neutral_mean\\\"] = fn_mean\\n            col_stats.loc[pred_col, \\\"feature_neutral_std\\\"] = fn_std\\n            col_stats.loc[pred_col, \\\"feature_neutral_sharpe\\\"] = fn_sharpe\\n            col_stats.loc[pred_col, \\\"tb200_mean\\\"] = tb200_mean\\n            col_stats.loc[pred_col, \\\"tb200_std\\\"] = tb200_std\\n            col_stats.loc[pred_col, \\\"tb200_sharpe\\\"] = tb200_sharpe\\n            col_stats.loc[pred_col, \\\"tb500_mean\\\"] = tb500_mean\\n            col_stats.loc[pred_col, \\\"tb500_std\\\"] = tb500_std\\n            col_stats.loc[pred_col, \\\"tb500_sharpe\\\"] = tb500_sharpe\\n        return col_stats\\n\\n    def per_era_corrs(self, dataf: pd.DataFrame, pred_col: str,\\n                      target_col: str) -> pd.Series:\\n        \\\"\\\"\\\" Correlation between prediction and target for each era. \\\"\\\"\\\"\\n        return dataf.groupby(dataf[self.era_col])\\\\\\n            .apply(lambda d: self._normalize_uniform(d[pred_col].fillna(0.5))\\n                   .corr(d[target_col]))\\n\\n    def mean_std_sharpe(self, era_corrs: pd.Series) -> Tuple[np.float64, np.float64, np.float64]:\\n        \\\"\\\"\\\"\\n        Average, standard deviation and Sharpe ratio for\\n        correlations per era.\\n        \\\"\\\"\\\"\\n        mean = pd.Series(era_corrs.mean()).item()\\n        std = pd.Series(era_corrs.std(ddof=0)).item()\\n        sharpe = mean / std\\n        return mean, std, sharpe\\n\\n    @staticmethod\\n    def max_drawdown(era_corrs: pd.Series) -> np.float64:\\n        \\\"\\\"\\\" Maximum drawdown per era. \\\"\\\"\\\"\\n        # Arbitrarily large window\\n        rolling_max = (era_corrs + 1).cumprod().rolling(window=9000,\\n                                                        min_periods=1).max()\\n        daily_value = (era_corrs + 1).cumprod()\\n        max_drawdown = -((rolling_max - daily_value) / rolling_max).max()\\n        return max_drawdown\\n\\n    @staticmethod\\n    def apy(era_corrs: pd.Series, stake_compounding_lag: int = 4) -> np.float64:\\n        \\\"\\\"\\\"\\n        Annual percentage yield.\\n        :param era_corrs: Correlation scores by era\\n        :param stake_compounding_lag: Compounding lag for Numerai rounds (4 for Numerai Classic)\\n        \\\"\\\"\\\"\\n        payout_scores = era_corrs.clip(-0.25, 0.25)\\n        payout_daily_value = (payout_scores + 1).cumprod()\\n        apy = (\\n                      (\\n                              (payout_daily_value.dropna().iloc[-1])\\n                              ** (1 / len(payout_scores))\\n                      )\\n                      ** (52 - stake_compounding_lag)  # 52 weeks of compounding minus n for stake compounding lag\\n                      - 1\\n              ) * 100\\n        return apy\\n\\n    def example_correlation(self, dataf: Union[pd.DataFrame, NumerFrame],\\n                            pred_col: str, example_col: str):\\n        \\\"\\\"\\\" Correlations with example predictions. \\\"\\\"\\\"\\n        return self.per_era_corrs(dataf=dataf,\\n                                  pred_col=pred_col,\\n                                  target_col=example_col,\\n                                  ).mean()\\n\\n    def max_feature_exposure(self, dataf: Union[pd.DataFrame, NumerFrame], pred_col: str) -> np.float64:\\n        \\\"\\\"\\\" Maximum exposure over all features. \\\"\\\"\\\"\\n        max_per_era = dataf.groupby(self.era_col).apply(\\n            lambda d: d[dataf.feature_cols].corrwith(d[pred_col]).abs().max())\\n        max_feature_exposure = max_per_era.mean(skipna=True)\\n        return max_feature_exposure\\n\\n    def feature_neutral_mean_std_sharpe(self, dataf: Union[pd.DataFrame, NumerFrame],\\n                             pred_col: str, target_col: str) -> Tuple[np.float64, np.float64, np.float64]:\\n        \\\"\\\"\\\"\\n        Feature neutralized mean performance.\\n        More info: https://docs.numer.ai/tournament/feature-neutral-correlation\\n        \\\"\\\"\\\"\\n        fn = FeatureNeutralizer(pred_name=pred_col,\\n                                proportion=1.0)\\n        neutralized_dataf = fn(dataf=dataf)\\n        neutral_corrs = self.per_era_corrs(dataf=neutralized_dataf,\\n                                           pred_col=f\\\"{pred_col}_neutralized_1.0\\\",\\n                                           target_col=target_col)\\n        mean, std, sharpe = self.mean_std_sharpe(era_corrs=neutral_corrs)\\n        return mean, std, sharpe\\n\\n    def tbx_mean_std_sharpe(self,\\n                            dataf: pd.DataFrame,\\n                            pred_col: str,\\n                            target_col: str,\\n                            tb: int = 200\\n                            ) -> Tuple[np.float64, np.float64, np.float64]:\\n        \\\"\\\"\\\"\\n        Calculate Mean, Standard deviation and Sharpe ratio\\n        when we focus on the x top and x bottom predictions.\\n        :param tb: How many of top and bottom predictions to focus on.\\n        TB200 and TB500 are the most common situations.\\n        \\\"\\\"\\\"\\n        tb_val_corrs = self._score_by_date(dataf=dataf,\\n                                           columns=[pred_col],\\n                                           target=target_col,\\n                                           tb=tb)\\n        return self.mean_std_sharpe(era_corrs=tb_val_corrs)\\n\\n    def mmc(self, dataf: pd.DataFrame,\\n            pred_col: str,\\n            target_col: str,\\n            example_col: str\\n            ) -> Tuple[np.float64, np.float64, np.float64]:\\n        \\\"\\\"\\\"\\n        MMC Mean, standard deviation and Sharpe ratio.\\n        More info: https://docs.numer.ai/tournament/metamodel-contribution\\n        \\\"\\\"\\\"\\n        mmc_scores = []\\n        corr_scores = []\\n        for _, x in dataf.groupby(self.era_col):\\n            series = self._neutralize_series(self._normalize_uniform(x[pred_col]), (x[example_col]))\\n            mmc_scores.append(np.cov(series, x[target_col])[0, 1] / (0.29 ** 2))\\n            corr_scores.append(self._normalize_uniform(x[pred_col]).corr(x[target_col]))\\n\\n        val_mmc_mean = np.mean(mmc_scores)\\n        val_mmc_std = np.std(mmc_scores)\\n        corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\\n        corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\\n        return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe\\n\\n    @staticmethod\\n    def _neutralize_series(series, by, proportion=1.0):\\n        scores = series.values.reshape(-1, 1)\\n        exposures = by.values.reshape(-1, 1)\\n\\n        # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\\n        exposures = np.hstack(\\n            (exposures,\\n             np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\\n\\n        correction = proportion * (exposures.dot(\\n            np.linalg.lstsq(exposures, scores, rcond=None)[0]))\\n        corrected_scores = scores - correction\\n        neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\\n        return neutralized\\n\\n    def _score_by_date(self, dataf: pd.DataFrame, columns: list, target: str, tb: int = None):\\n        \\\"\\\"\\\"\\n        Get era correlation based on given TB (x top and bottom predictions).\\n        :param tb: How many of top and bottom predictions to focus on.\\n        TB200 is the most common situation.\\n        \\\"\\\"\\\"\\n        unique_eras = dataf[self.era_col].unique()\\n        computed = []\\n        for u in unique_eras:\\n            df_era = dataf[dataf[self.era_col] == u]\\n            era_pred = np.float64(df_era[columns].values.T)\\n            era_target = np.float64(df_era[target].values.T)\\n\\n            if tb is None:\\n                ccs = np.corrcoef(era_target, era_pred)[0, 1:]\\n            else:\\n                tbidx = np.argsort(era_pred, axis=1)\\n                tbidx = np.concatenate([tbidx[:, :tb], tbidx[:, -tb:]], axis=1)\\n                ccs = [np.corrcoef(era_target[idx], pred[idx])[0, 1] for idx, pred in zip(tbidx, era_pred)]\\n                ccs = np.array(ccs)\\n            computed.append(ccs)\\n        return pd.DataFrame(np.array(computed), columns=columns, index=dataf[self.era_col].unique())\\n\\n    @staticmethod\\n    def _normalize_uniform(df: pd.DataFrame) -> pd.Series:\\n        \\\"\\\"\\\" Normalize predictions uniformly using ranks. \\\"\\\"\\\"\\n        x = (df.rank(method=\\\"first\\\") - 0.5) / len(df)\\n        return pd.Series(x, index=df.index)\";\n                var nbb_formatted_code = \"# export\\nclass BaseEvaluator:\\n    \\\"\\\"\\\"\\n    Evaluation functionality that is relevant for both\\n    Numerai Classic and Numerai Signals.\\n    :param era_col: Column name pointing to eras.\\n    Most commonly \\\"era\\\" for Numerai Classic and \\\"friday_date\\\" for Numerai Signals.\\n    :param fast_mode: Will skip compute intensive metrics if set to True,\\n    namely max_exposure, feature neutral mean, TB200 and TB500.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, era_col: str = \\\"era\\\", fast_mode=False):\\n        self.era_col = era_col\\n        self.fast_mode = fast_mode\\n\\n    def full_evaluation(\\n        self,\\n        dataf: NumerFrame,\\n        example_col: str,\\n        pred_cols: list = None,\\n        target_col: str = \\\"target\\\",\\n    ) -> pd.DataFrame:\\n        \\\"\\\"\\\"\\n        Perform evaluation for each prediction column in the NumerFrame\\n        against give target and example prediction column.\\n        \\\"\\\"\\\"\\n        val_stats = pd.DataFrame()\\n        dataf = dataf.fillna(0.5)\\n        pred_cols = dataf.prediction_cols if not pred_cols else pred_cols\\n        for col in tqdm(pred_cols, desc=\\\"Evaluation: \\\"):\\n            col_stats = self.evaluation_one_col(\\n                dataf=dataf,\\n                pred_col=col,\\n                target_col=target_col,\\n                example_col=example_col,\\n            )\\n            val_stats = pd.concat([val_stats, col_stats], axis=0)\\n        return val_stats\\n\\n    def evaluation_one_col(\\n        self,\\n        dataf: Union[pd.DataFrame, NumerFrame],\\n        pred_col: str,\\n        target_col: str,\\n        example_col: str,\\n    ):\\n        \\\"\\\"\\\"\\n        Perform evaluation for one prediction column\\n        against given target and example prediction column.\\n        \\\"\\\"\\\"\\n        col_stats = pd.DataFrame()\\n        # Compute stats\\n        val_corrs = self.per_era_corrs(\\n            dataf=dataf, pred_col=pred_col, target_col=target_col\\n        )\\n        mean, std, sharpe = self.mean_std_sharpe(era_corrs=val_corrs)\\n        max_drawdown = self.max_drawdown(era_corrs=val_corrs)\\n        apy = self.apy(era_corrs=val_corrs)\\n        example_corr = self.example_correlation(\\n            dataf=dataf, pred_col=pred_col, example_col=example_col\\n        )\\n        mmc_mean, mmc_std, mmc_sharpe = self.mmc(\\n            dataf=dataf,\\n            pred_col=pred_col,\\n            target_col=target_col,\\n            example_col=example_col,\\n        )\\n\\n        col_stats.loc[pred_col, \\\"target\\\"] = target_col\\n        col_stats.loc[pred_col, \\\"mean\\\"] = mean\\n        col_stats.loc[pred_col, \\\"std\\\"] = std\\n        col_stats.loc[pred_col, \\\"sharpe\\\"] = sharpe\\n        col_stats.loc[pred_col, \\\"max_drawdown\\\"] = max_drawdown\\n        col_stats.loc[pred_col, \\\"apy\\\"] = apy\\n        col_stats.loc[pred_col, \\\"mmc_mean\\\"] = mmc_mean\\n        col_stats.loc[pred_col, \\\"mmc_std\\\"] = mmc_std\\n        col_stats.loc[pred_col, \\\"mmc_sharpe\\\"] = mmc_sharpe\\n        col_stats.loc[pred_col, \\\"corr_with_example_preds\\\"] = example_corr\\n\\n        # Compute intensive stats\\n        if not self.fast_mode:\\n            max_feature_exposure = self.max_feature_exposure(\\n                dataf=dataf, pred_col=pred_col\\n            )\\n            fn_mean, fn_std, fn_sharpe = self.feature_neutral_mean_std_sharpe(\\n                dataf=dataf, pred_col=pred_col, target_col=target_col\\n            )\\n            tb200_mean, tb200_std, tb200_sharpe = self.tbx_mean_std_sharpe(\\n                dataf=dataf, pred_col=pred_col, target_col=target_col, tb=200\\n            )\\n            tb500_mean, tb500_std, tb500_sharpe = self.tbx_mean_std_sharpe(\\n                dataf=dataf, pred_col=pred_col, target_col=target_col, tb=500\\n            )\\n            col_stats.loc[pred_col, \\\"max_feature_exposure\\\"] = max_feature_exposure\\n            col_stats.loc[pred_col, \\\"feature_neutral_mean\\\"] = fn_mean\\n            col_stats.loc[pred_col, \\\"feature_neutral_std\\\"] = fn_std\\n            col_stats.loc[pred_col, \\\"feature_neutral_sharpe\\\"] = fn_sharpe\\n            col_stats.loc[pred_col, \\\"tb200_mean\\\"] = tb200_mean\\n            col_stats.loc[pred_col, \\\"tb200_std\\\"] = tb200_std\\n            col_stats.loc[pred_col, \\\"tb200_sharpe\\\"] = tb200_sharpe\\n            col_stats.loc[pred_col, \\\"tb500_mean\\\"] = tb500_mean\\n            col_stats.loc[pred_col, \\\"tb500_std\\\"] = tb500_std\\n            col_stats.loc[pred_col, \\\"tb500_sharpe\\\"] = tb500_sharpe\\n        return col_stats\\n\\n    def per_era_corrs(\\n        self, dataf: pd.DataFrame, pred_col: str, target_col: str\\n    ) -> pd.Series:\\n        \\\"\\\"\\\"Correlation between prediction and target for each era.\\\"\\\"\\\"\\n        return dataf.groupby(dataf[self.era_col]).apply(\\n            lambda d: self._normalize_uniform(d[pred_col].fillna(0.5)).corr(\\n                d[target_col]\\n            )\\n        )\\n\\n    def mean_std_sharpe(\\n        self, era_corrs: pd.Series\\n    ) -> Tuple[np.float64, np.float64, np.float64]:\\n        \\\"\\\"\\\"\\n        Average, standard deviation and Sharpe ratio for\\n        correlations per era.\\n        \\\"\\\"\\\"\\n        mean = pd.Series(era_corrs.mean()).item()\\n        std = pd.Series(era_corrs.std(ddof=0)).item()\\n        sharpe = mean / std\\n        return mean, std, sharpe\\n\\n    @staticmethod\\n    def max_drawdown(era_corrs: pd.Series) -> np.float64:\\n        \\\"\\\"\\\"Maximum drawdown per era.\\\"\\\"\\\"\\n        # Arbitrarily large window\\n        rolling_max = (\\n            (era_corrs + 1).cumprod().rolling(window=9000, min_periods=1).max()\\n        )\\n        daily_value = (era_corrs + 1).cumprod()\\n        max_drawdown = -((rolling_max - daily_value) / rolling_max).max()\\n        return max_drawdown\\n\\n    @staticmethod\\n    def apy(era_corrs: pd.Series, stake_compounding_lag: int = 4) -> np.float64:\\n        \\\"\\\"\\\"\\n        Annual percentage yield.\\n        :param era_corrs: Correlation scores by era\\n        :param stake_compounding_lag: Compounding lag for Numerai rounds (4 for Numerai Classic)\\n        \\\"\\\"\\\"\\n        payout_scores = era_corrs.clip(-0.25, 0.25)\\n        payout_daily_value = (payout_scores + 1).cumprod()\\n        apy = (\\n            ((payout_daily_value.dropna().iloc[-1]) ** (1 / len(payout_scores)))\\n            ** (\\n                52 - stake_compounding_lag\\n            )  # 52 weeks of compounding minus n for stake compounding lag\\n            - 1\\n        ) * 100\\n        return apy\\n\\n    def example_correlation(\\n        self, dataf: Union[pd.DataFrame, NumerFrame], pred_col: str, example_col: str\\n    ):\\n        \\\"\\\"\\\"Correlations with example predictions.\\\"\\\"\\\"\\n        return self.per_era_corrs(\\n            dataf=dataf,\\n            pred_col=pred_col,\\n            target_col=example_col,\\n        ).mean()\\n\\n    def max_feature_exposure(\\n        self, dataf: Union[pd.DataFrame, NumerFrame], pred_col: str\\n    ) -> np.float64:\\n        \\\"\\\"\\\"Maximum exposure over all features.\\\"\\\"\\\"\\n        max_per_era = dataf.groupby(self.era_col).apply(\\n            lambda d: d[dataf.feature_cols].corrwith(d[pred_col]).abs().max()\\n        )\\n        max_feature_exposure = max_per_era.mean(skipna=True)\\n        return max_feature_exposure\\n\\n    def feature_neutral_mean_std_sharpe(\\n        self, dataf: Union[pd.DataFrame, NumerFrame], pred_col: str, target_col: str\\n    ) -> Tuple[np.float64, np.float64, np.float64]:\\n        \\\"\\\"\\\"\\n        Feature neutralized mean performance.\\n        More info: https://docs.numer.ai/tournament/feature-neutral-correlation\\n        \\\"\\\"\\\"\\n        fn = FeatureNeutralizer(pred_name=pred_col, proportion=1.0)\\n        neutralized_dataf = fn(dataf=dataf)\\n        neutral_corrs = self.per_era_corrs(\\n            dataf=neutralized_dataf,\\n            pred_col=f\\\"{pred_col}_neutralized_1.0\\\",\\n            target_col=target_col,\\n        )\\n        mean, std, sharpe = self.mean_std_sharpe(era_corrs=neutral_corrs)\\n        return mean, std, sharpe\\n\\n    def tbx_mean_std_sharpe(\\n        self, dataf: pd.DataFrame, pred_col: str, target_col: str, tb: int = 200\\n    ) -> Tuple[np.float64, np.float64, np.float64]:\\n        \\\"\\\"\\\"\\n        Calculate Mean, Standard deviation and Sharpe ratio\\n        when we focus on the x top and x bottom predictions.\\n        :param tb: How many of top and bottom predictions to focus on.\\n        TB200 and TB500 are the most common situations.\\n        \\\"\\\"\\\"\\n        tb_val_corrs = self._score_by_date(\\n            dataf=dataf, columns=[pred_col], target=target_col, tb=tb\\n        )\\n        return self.mean_std_sharpe(era_corrs=tb_val_corrs)\\n\\n    def mmc(\\n        self, dataf: pd.DataFrame, pred_col: str, target_col: str, example_col: str\\n    ) -> Tuple[np.float64, np.float64, np.float64]:\\n        \\\"\\\"\\\"\\n        MMC Mean, standard deviation and Sharpe ratio.\\n        More info: https://docs.numer.ai/tournament/metamodel-contribution\\n        \\\"\\\"\\\"\\n        mmc_scores = []\\n        corr_scores = []\\n        for _, x in dataf.groupby(self.era_col):\\n            series = self._neutralize_series(\\n                self._normalize_uniform(x[pred_col]), (x[example_col])\\n            )\\n            mmc_scores.append(np.cov(series, x[target_col])[0, 1] / (0.29 ** 2))\\n            corr_scores.append(self._normalize_uniform(x[pred_col]).corr(x[target_col]))\\n\\n        val_mmc_mean = np.mean(mmc_scores)\\n        val_mmc_std = np.std(mmc_scores)\\n        corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\\n        corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\\n        return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe\\n\\n    @staticmethod\\n    def _neutralize_series(series, by, proportion=1.0):\\n        scores = series.values.reshape(-1, 1)\\n        exposures = by.values.reshape(-1, 1)\\n\\n        # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\\n        exposures = np.hstack(\\n            (exposures, np.array([np.mean(series)] * len(exposures)).reshape(-1, 1))\\n        )\\n\\n        correction = proportion * (\\n            exposures.dot(np.linalg.lstsq(exposures, scores, rcond=None)[0])\\n        )\\n        corrected_scores = scores - correction\\n        neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\\n        return neutralized\\n\\n    def _score_by_date(\\n        self, dataf: pd.DataFrame, columns: list, target: str, tb: int = None\\n    ):\\n        \\\"\\\"\\\"\\n        Get era correlation based on given TB (x top and bottom predictions).\\n        :param tb: How many of top and bottom predictions to focus on.\\n        TB200 is the most common situation.\\n        \\\"\\\"\\\"\\n        unique_eras = dataf[self.era_col].unique()\\n        computed = []\\n        for u in unique_eras:\\n            df_era = dataf[dataf[self.era_col] == u]\\n            era_pred = np.float64(df_era[columns].values.T)\\n            era_target = np.float64(df_era[target].values.T)\\n\\n            if tb is None:\\n                ccs = np.corrcoef(era_target, era_pred)[0, 1:]\\n            else:\\n                tbidx = np.argsort(era_pred, axis=1)\\n                tbidx = np.concatenate([tbidx[:, :tb], tbidx[:, -tb:]], axis=1)\\n                ccs = [\\n                    np.corrcoef(era_target[idx], pred[idx])[0, 1]\\n                    for idx, pred in zip(tbidx, era_pred)\\n                ]\\n                ccs = np.array(ccs)\\n            computed.append(ccs)\\n        return pd.DataFrame(\\n            np.array(computed), columns=columns, index=dataf[self.era_col].unique()\\n        )\\n\\n    @staticmethod\\n    def _normalize_uniform(df: pd.DataFrame) -> pd.Series:\\n        \\\"\\\"\\\"Normalize predictions uniformly using ranks.\\\"\\\"\\\"\\n        x = (df.rank(method=\\\"first\\\") - 0.5) / len(df)\\n        return pd.Series(x, index=df.index)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class BaseEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluation functionality that is relevant for both\n",
    "    Numerai Classic and Numerai Signals.\n",
    "    :param era_col: Column name pointing to eras.\n",
    "    Most commonly \"era\" for Numerai Classic and \"friday_date\" for Numerai Signals.\n",
    "    :param fast_mode: Will skip compute intensive metrics if set to True,\n",
    "    namely max_exposure, feature neutral mean, TB200 and TB500.\n",
    "    \"\"\"\n",
    "    def __init__(self, era_col: str = \"era\", fast_mode = False):\n",
    "        self.era_col = era_col\n",
    "        self.fast_mode = fast_mode\n",
    "\n",
    "    def full_evaluation(self,\n",
    "                        dataf: NumerFrame,\n",
    "                        example_col: str,\n",
    "                        pred_cols: list = None,\n",
    "                        target_col: str = \"target\"\n",
    "                        ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform evaluation for each prediction column in the NumerFrame\n",
    "        against give target and example prediction column.\n",
    "        \"\"\"\n",
    "        val_stats = pd.DataFrame()\n",
    "        dataf = dataf.fillna(0.5)\n",
    "        pred_cols = dataf.prediction_cols if not pred_cols else pred_cols\n",
    "        for col in tqdm(pred_cols, desc=\"Evaluation: \"):\n",
    "            col_stats = self.evaluation_one_col(dataf=dataf, pred_col=col,\n",
    "                                                target_col=target_col,\n",
    "                                                example_col=example_col)\n",
    "            val_stats = pd.concat([val_stats, col_stats], axis=0)\n",
    "        return val_stats\n",
    "\n",
    "    def evaluation_one_col(self, dataf: Union[pd.DataFrame, NumerFrame], pred_col: str, target_col: str, example_col: str):\n",
    "        \"\"\"\n",
    "        Perform evaluation for one prediction column\n",
    "        against given target and example prediction column.\n",
    "        \"\"\"\n",
    "        col_stats = pd.DataFrame()\n",
    "        # Compute stats\n",
    "        val_corrs = self.per_era_corrs(dataf=dataf,\n",
    "                                        pred_col=pred_col,\n",
    "                                        target_col=target_col\n",
    "                                       )\n",
    "        mean, std, sharpe = self.mean_std_sharpe(era_corrs=val_corrs)\n",
    "        max_drawdown = self.max_drawdown(era_corrs=val_corrs)\n",
    "        apy = self.apy(era_corrs=val_corrs)\n",
    "        example_corr = self.example_correlation(dataf=dataf,\n",
    "                                                pred_col=pred_col,\n",
    "                                                example_col=example_col\n",
    "                                                )\n",
    "        mmc_mean, mmc_std, mmc_sharpe = self.mmc(dataf=dataf,\n",
    "                                                 pred_col=pred_col,\n",
    "                                                 target_col=target_col,\n",
    "                                                 example_col=example_col\n",
    "                                                 )\n",
    "\n",
    "        col_stats.loc[pred_col, \"target\"] = target_col\n",
    "        col_stats.loc[pred_col, \"mean\"] = mean\n",
    "        col_stats.loc[pred_col, \"std\"] = std\n",
    "        col_stats.loc[pred_col, \"sharpe\"] = sharpe\n",
    "        col_stats.loc[pred_col, \"max_drawdown\"] = max_drawdown\n",
    "        col_stats.loc[pred_col, \"apy\"] = apy\n",
    "        col_stats.loc[pred_col, \"mmc_mean\"] = mmc_mean\n",
    "        col_stats.loc[pred_col, \"mmc_std\"] = mmc_std\n",
    "        col_stats.loc[pred_col, \"mmc_sharpe\"] = mmc_sharpe\n",
    "        col_stats.loc[pred_col, \"corr_with_example_preds\"] = example_corr\n",
    "\n",
    "        # Compute intensive stats\n",
    "        if not self.fast_mode:\n",
    "            max_feature_exposure = self.max_feature_exposure(dataf=dataf, pred_col=pred_col)\n",
    "            fn_mean, fn_std, fn_sharpe = self.feature_neutral_mean_std_sharpe(dataf=dataf,\n",
    "                                                                              pred_col=pred_col,\n",
    "                                                                              target_col=target_col\n",
    "                                                                              )\n",
    "            tb200_mean, tb200_std, tb200_sharpe = self.tbx_mean_std_sharpe(dataf=dataf,\n",
    "                                                                           pred_col=pred_col,\n",
    "                                                                           target_col=target_col,\n",
    "                                                                           tb=200\n",
    "                                                                           )\n",
    "            tb500_mean, tb500_std, tb500_sharpe = self.tbx_mean_std_sharpe(dataf=dataf,\n",
    "                                                                           pred_col=pred_col,\n",
    "                                                                           target_col=target_col,\n",
    "                                                                           tb=500\n",
    "                                                                           )\n",
    "            col_stats.loc[pred_col, \"max_feature_exposure\"] = max_feature_exposure\n",
    "            col_stats.loc[pred_col, \"feature_neutral_mean\"] = fn_mean\n",
    "            col_stats.loc[pred_col, \"feature_neutral_std\"] = fn_std\n",
    "            col_stats.loc[pred_col, \"feature_neutral_sharpe\"] = fn_sharpe\n",
    "            col_stats.loc[pred_col, \"tb200_mean\"] = tb200_mean\n",
    "            col_stats.loc[pred_col, \"tb200_std\"] = tb200_std\n",
    "            col_stats.loc[pred_col, \"tb200_sharpe\"] = tb200_sharpe\n",
    "            col_stats.loc[pred_col, \"tb500_mean\"] = tb500_mean\n",
    "            col_stats.loc[pred_col, \"tb500_std\"] = tb500_std\n",
    "            col_stats.loc[pred_col, \"tb500_sharpe\"] = tb500_sharpe\n",
    "        return col_stats\n",
    "\n",
    "    def per_era_corrs(self, dataf: pd.DataFrame, pred_col: str,\n",
    "                      target_col: str) -> pd.Series:\n",
    "        \"\"\" Correlation between prediction and target for each era. \"\"\"\n",
    "        return dataf.groupby(dataf[self.era_col])\\\n",
    "            .apply(lambda d: self._normalize_uniform(d[pred_col].fillna(0.5))\n",
    "                   .corr(d[target_col]))\n",
    "\n",
    "    def mean_std_sharpe(self, era_corrs: pd.Series) -> Tuple[np.float64, np.float64, np.float64]:\n",
    "        \"\"\"\n",
    "        Average, standard deviation and Sharpe ratio for\n",
    "        correlations per era.\n",
    "        \"\"\"\n",
    "        mean = pd.Series(era_corrs.mean()).item()\n",
    "        std = pd.Series(era_corrs.std(ddof=0)).item()\n",
    "        sharpe = mean / std\n",
    "        return mean, std, sharpe\n",
    "\n",
    "    @staticmethod\n",
    "    def max_drawdown(era_corrs: pd.Series) -> np.float64:\n",
    "        \"\"\" Maximum drawdown per era. \"\"\"\n",
    "        # Arbitrarily large window\n",
    "        rolling_max = (era_corrs + 1).cumprod().rolling(window=9000,\n",
    "                                                        min_periods=1).max()\n",
    "        daily_value = (era_corrs + 1).cumprod()\n",
    "        max_drawdown = -((rolling_max - daily_value) / rolling_max).max()\n",
    "        return max_drawdown\n",
    "\n",
    "    @staticmethod\n",
    "    def apy(era_corrs: pd.Series, stake_compounding_lag: int = 4) -> np.float64:\n",
    "        \"\"\"\n",
    "        Annual percentage yield.\n",
    "        :param era_corrs: Correlation scores by era\n",
    "        :param stake_compounding_lag: Compounding lag for Numerai rounds (4 for Numerai Classic)\n",
    "        \"\"\"\n",
    "        payout_scores = era_corrs.clip(-0.25, 0.25)\n",
    "        payout_daily_value = (payout_scores + 1).cumprod()\n",
    "        apy = (\n",
    "                      (\n",
    "                              (payout_daily_value.dropna().iloc[-1])\n",
    "                              ** (1 / len(payout_scores))\n",
    "                      )\n",
    "                      ** (52 - stake_compounding_lag)  # 52 weeks of compounding minus n for stake compounding lag\n",
    "                      - 1\n",
    "              ) * 100\n",
    "        return apy\n",
    "\n",
    "    def example_correlation(self, dataf: Union[pd.DataFrame, NumerFrame],\n",
    "                            pred_col: str, example_col: str):\n",
    "        \"\"\" Correlations with example predictions. \"\"\"\n",
    "        return self.per_era_corrs(dataf=dataf,\n",
    "                                  pred_col=pred_col,\n",
    "                                  target_col=example_col,\n",
    "                                  ).mean()\n",
    "\n",
    "    def max_feature_exposure(self, dataf: Union[pd.DataFrame, NumerFrame], pred_col: str) -> np.float64:\n",
    "        \"\"\" Maximum exposure over all features. \"\"\"\n",
    "        max_per_era = dataf.groupby(self.era_col).apply(\n",
    "            lambda d: d[dataf.feature_cols].corrwith(d[pred_col]).abs().max())\n",
    "        max_feature_exposure = max_per_era.mean(skipna=True)\n",
    "        return max_feature_exposure\n",
    "\n",
    "    def feature_neutral_mean_std_sharpe(self, dataf: Union[pd.DataFrame, NumerFrame],\n",
    "                             pred_col: str, target_col: str) -> Tuple[np.float64, np.float64, np.float64]:\n",
    "        \"\"\"\n",
    "        Feature neutralized mean performance.\n",
    "        More info: https://docs.numer.ai/tournament/feature-neutral-correlation\n",
    "        \"\"\"\n",
    "        fn = FeatureNeutralizer(pred_name=pred_col,\n",
    "                                proportion=1.0)\n",
    "        neutralized_dataf = fn(dataf=dataf)\n",
    "        neutral_corrs = self.per_era_corrs(dataf=neutralized_dataf,\n",
    "                                           pred_col=f\"{pred_col}_neutralized_1.0\",\n",
    "                                           target_col=target_col)\n",
    "        mean, std, sharpe = self.mean_std_sharpe(era_corrs=neutral_corrs)\n",
    "        return mean, std, sharpe\n",
    "\n",
    "    def tbx_mean_std_sharpe(self,\n",
    "                            dataf: pd.DataFrame,\n",
    "                            pred_col: str,\n",
    "                            target_col: str,\n",
    "                            tb: int = 200\n",
    "                            ) -> Tuple[np.float64, np.float64, np.float64]:\n",
    "        \"\"\"\n",
    "        Calculate Mean, Standard deviation and Sharpe ratio\n",
    "        when we focus on the x top and x bottom predictions.\n",
    "        :param tb: How many of top and bottom predictions to focus on.\n",
    "        TB200 and TB500 are the most common situations.\n",
    "        \"\"\"\n",
    "        tb_val_corrs = self._score_by_date(dataf=dataf,\n",
    "                                           columns=[pred_col],\n",
    "                                           target=target_col,\n",
    "                                           tb=tb)\n",
    "        return self.mean_std_sharpe(era_corrs=tb_val_corrs)\n",
    "\n",
    "    def mmc(self, dataf: pd.DataFrame,\n",
    "            pred_col: str,\n",
    "            target_col: str,\n",
    "            example_col: str\n",
    "            ) -> Tuple[np.float64, np.float64, np.float64]:\n",
    "        \"\"\"\n",
    "        MMC Mean, standard deviation and Sharpe ratio.\n",
    "        More info: https://docs.numer.ai/tournament/metamodel-contribution\n",
    "        \"\"\"\n",
    "        mmc_scores = []\n",
    "        corr_scores = []\n",
    "        for _, x in dataf.groupby(self.era_col):\n",
    "            series = self._neutralize_series(self._normalize_uniform(x[pred_col]), (x[example_col]))\n",
    "            mmc_scores.append(np.cov(series, x[target_col])[0, 1] / (0.29 ** 2))\n",
    "            corr_scores.append(self._normalize_uniform(x[pred_col]).corr(x[target_col]))\n",
    "\n",
    "        val_mmc_mean = np.mean(mmc_scores)\n",
    "        val_mmc_std = np.std(mmc_scores)\n",
    "        corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
    "        corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
    "        return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe\n",
    "\n",
    "    @staticmethod\n",
    "    def _neutralize_series(series, by, proportion=1.0):\n",
    "        scores = series.values.reshape(-1, 1)\n",
    "        exposures = by.values.reshape(-1, 1)\n",
    "\n",
    "        # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
    "        exposures = np.hstack(\n",
    "            (exposures,\n",
    "             np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n",
    "\n",
    "        correction = proportion * (exposures.dot(\n",
    "            np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n",
    "        corrected_scores = scores - correction\n",
    "        neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
    "        return neutralized\n",
    "\n",
    "    def _score_by_date(self, dataf: pd.DataFrame, columns: list, target: str, tb: int = None):\n",
    "        \"\"\"\n",
    "        Get era correlation based on given TB (x top and bottom predictions).\n",
    "        :param tb: How many of top and bottom predictions to focus on.\n",
    "        TB200 is the most common situation.\n",
    "        \"\"\"\n",
    "        unique_eras = dataf[self.era_col].unique()\n",
    "        computed = []\n",
    "        for u in unique_eras:\n",
    "            df_era = dataf[dataf[self.era_col] == u]\n",
    "            era_pred = np.float64(df_era[columns].values.T)\n",
    "            era_target = np.float64(df_era[target].values.T)\n",
    "\n",
    "            if tb is None:\n",
    "                ccs = np.corrcoef(era_target, era_pred)[0, 1:]\n",
    "            else:\n",
    "                tbidx = np.argsort(era_pred, axis=1)\n",
    "                tbidx = np.concatenate([tbidx[:, :tb], tbidx[:, -tb:]], axis=1)\n",
    "                ccs = [np.corrcoef(era_target[idx], pred[idx])[0, 1] for idx, pred in zip(tbidx, era_pred)]\n",
    "                ccs = np.array(ccs)\n",
    "            computed.append(ccs)\n",
    "        return pd.DataFrame(np.array(computed), columns=columns, index=dataf[self.era_col].unique())\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_uniform(df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\" Normalize predictions uniformly using ranks. \"\"\"\n",
    "        x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
    "        return pd.Series(x, index=df.index)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Numerai Classic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`NumeraiClassicEvaluator` extends the base evaluation scheme with metrics specific to Numerai Classic."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 5;\n                var nbb_unformatted_code = \"#export\\nclass NumeraiClassicEvaluator(BaseEvaluator):\\n    \\\"\\\"\\\" Evaluator for all metrics that are relevant in Numerai Classic. \\\"\\\"\\\"\\n    def __init__(self, era_col: str = \\\"era\\\", fast_mode = False):\\n        super().__init__(era_col=era_col, fast_mode=fast_mode)\";\n                var nbb_formatted_code = \"# export\\nclass NumeraiClassicEvaluator(BaseEvaluator):\\n    \\\"\\\"\\\"Evaluator for all metrics that are relevant in Numerai Classic.\\\"\\\"\\\"\\n\\n    def __init__(self, era_col: str = \\\"era\\\", fast_mode=False):\\n        super().__init__(era_col=era_col, fast_mode=fast_mode)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "class NumeraiClassicEvaluator(BaseEvaluator):\n",
    "    \"\"\" Evaluator for all metrics that are relevant in Numerai Classic. \"\"\"\n",
    "    def __init__(self, era_col: str = \"era\", fast_mode = False):\n",
    "        super().__init__(era_col=era_col, fast_mode=fast_mode)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Numerai Signals"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`NumeraiSignalsEvaluator` extends the base evaluation scheme with metrics specific to Numerai Signals.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 6;\n                var nbb_unformatted_code = \"# export\\nclass NumeraiSignalsEvaluator(BaseEvaluator):\\n    \\\"\\\"\\\" Evaluator for all metrics that are relevant in Numerai Signals. \\\"\\\"\\\"\\n    def __init__(self, era_col: str = \\\"friday_date\\\", fast_mode = False):\\n        super().__init__(era_col=era_col, fast_mode=fast_mode)\";\n                var nbb_formatted_code = \"# export\\nclass NumeraiSignalsEvaluator(BaseEvaluator):\\n    \\\"\\\"\\\"Evaluator for all metrics that are relevant in Numerai Signals.\\\"\\\"\\\"\\n\\n    def __init__(self, era_col: str = \\\"friday_date\\\", fast_mode=False):\\n        super().__init__(era_col=era_col, fast_mode=fast_mode)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class NumeraiSignalsEvaluator(BaseEvaluator):\n",
    "    \"\"\" Evaluator for all metrics that are relevant in Numerai Signals. \"\"\"\n",
    "    def __init__(self, era_col: str = \"friday_date\", fast_mode = False):\n",
    "        super().__init__(era_col=era_col, fast_mode=fast_mode)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example usage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will test this evaluation scheme on version 2 evaluation data with example predictions. The baseline reference (`example_col`) will be uniform predictions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "No existing directory found at \u001B[32m'\u001B[0m\u001B[34meval_test_1234321\u001B[0m\u001B[32m'\u001B[0m. Creating directory\u001B[33m...\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">No existing directory found at <span style=\"color: #008000; text-decoration-color: #008000\">'</span><span style=\"color: #000080; text-decoration-color: #000080\">eval_test_1234321</span><span style=\"color: #008000; text-decoration-color: #008000\">'</span>. Creating directory<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": " \u001B[32mDownloading\u001B[0m \u001B[32m'example_predictions.parquet'\u001B[0m \n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> <span style=\"color: #008000; text-decoration-color: #008000\">Downloading</span> <span style=\"color: #008000; text-decoration-color: #008000\">'example_predictions.parquet'</span> \n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 14:44:08,875 INFO numerapi.utils: starting download\n",
      "eval_test_1234321/example_predictions.parquet: 33.5MB [00:07, 4.27MB/s]                            \n"
     ]
    },
    {
     "data": {
      "text/plain": " \u001B[32mDownloading\u001B[0m \u001B[32m'example_validation_predictions.parquet'\u001B[0m \n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> <span style=\"color: #008000; text-decoration-color: #008000\">Downloading</span> <span style=\"color: #008000; text-decoration-color: #008000\">'example_validation_predictions.parquet'</span> \n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 14:44:18,445 INFO numerapi.utils: starting download\n",
      "eval_test_1234321/example_validation_predictions.parquet: 13.0MB [00:02, 5.44MB/s]                            \n"
     ]
    },
    {
     "data": {
      "text/plain": " \u001B[32mDownloading\u001B[0m \u001B[32m'numerai_validation_data.parquet'\u001B[0m \n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> <span style=\"color: #008000; text-decoration-color: #008000\">Downloading</span> <span style=\"color: #008000; text-decoration-color: #008000\">'numerai_validation_data.parquet'</span> \n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 14:44:23,331 INFO numerapi.utils: starting download\n",
      "eval_test_1234321/numerai_validation_data.parquet: 228MB [00:43, 5.20MB/s]                            \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 7;\n                var nbb_unformatted_code = \"from numerai_blocks.download import NumeraiClassicDownloader\\ndirectory = \\\"eval_test_1234321/\\\"\\nval_filename = \\\"numerai_validation_data.parquet\\\"\\nfull_val_path = directory + val_filename\\ndownloader = NumeraiClassicDownloader(directory_path=directory)\\ndownloader.download_example_data()\\ndownloader.download_single_dataset(filename=val_filename,\\n                                   dest_path=full_val_path)\";\n                var nbb_formatted_code = \"from numerai_blocks.download import NumeraiClassicDownloader\\n\\ndirectory = \\\"eval_test_1234321/\\\"\\nval_filename = \\\"numerai_validation_data.parquet\\\"\\nfull_val_path = directory + val_filename\\ndownloader = NumeraiClassicDownloader(directory_path=directory)\\ndownloader.download_example_data()\\ndownloader.download_single_dataset(filename=val_filename, dest_path=full_val_path)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numerai_blocks.download import NumeraiClassicDownloader\n",
    "directory = \"eval_test_1234321/\"\n",
    "val_filename = \"numerai_validation_data.parquet\"\n",
    "full_val_path = directory + val_filename\n",
    "downloader = NumeraiClassicDownloader(directory_path=directory)\n",
    "downloader.download_example_data()\n",
    "downloader.download_single_dataset(filename=val_filename,\n",
    "                                   dest_path=full_val_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 8;\n                var nbb_unformatted_code = \"np.random.seed(1234)\\ntest_dataf = create_numerframe(full_val_path)\\nexample_preds = pd.read_parquet(directory + \\\"/example_validation_predictions.parquet\\\")\\ntest_dataf = test_dataf.merge(example_preds, on='id', how='left')\\ntest_dataf = NumerFrame(test_dataf[test_dataf.feature_cols[:5] + test_dataf.target_cols \\\\\\n                        + test_dataf.prediction_cols + test_dataf.aux_cols])\\ntest_dataf.loc[:, 'prediction_random'] = np.random.uniform(size=len(test_dataf))\";\n                var nbb_formatted_code = \"np.random.seed(1234)\\ntest_dataf = create_numerframe(full_val_path)\\nexample_preds = pd.read_parquet(directory + \\\"/example_validation_predictions.parquet\\\")\\ntest_dataf = test_dataf.merge(example_preds, on=\\\"id\\\", how=\\\"left\\\")\\ntest_dataf = NumerFrame(\\n    test_dataf[\\n        test_dataf.feature_cols[:5]\\n        + test_dataf.target_cols\\n        + test_dataf.prediction_cols\\n        + test_dataf.aux_cols\\n    ]\\n)\\ntest_dataf.loc[:, \\\"prediction_random\\\"] = np.random.uniform(size=len(test_dataf))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "test_dataf = create_numerframe(full_val_path)\n",
    "example_preds = pd.read_parquet(directory + \"/example_validation_predictions.parquet\")\n",
    "test_dataf = test_dataf.merge(example_preds, on='id', how='left')\n",
    "# Take only first 5 feature to speed up example (feature neutralization)\n",
    "test_dataf = NumerFrame(test_dataf[test_dataf.feature_cols[:5] + test_dataf.target_cols \\\n",
    "                        + test_dataf.prediction_cols + test_dataf.aux_cols])\n",
    "test_dataf.loc[:, 'prediction_random'] = np.random.uniform(size=len(test_dataf))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                  feature_dichasial_hammier_spawner  \\\nid                                                    \nn000777698096000                               0.50   \nn0009793a3b91c27                               0.75   \n\n                  feature_rheumy_epistemic_prancer  \\\nid                                                   \nn000777698096000                              0.50   \nn0009793a3b91c27                              0.25   \n\n                  feature_pert_performative_hormuz  \\\nid                                                   \nn000777698096000                              0.25   \nn0009793a3b91c27                              0.50   \n\n                  feature_hillier_unpitied_theobromine  \\\nid                                                       \nn000777698096000                                  0.25   \nn0009793a3b91c27                                  0.75   \n\n                  feature_perigean_bewitching_thruster  target  \\\nid                                                               \nn000777698096000                                   0.0    0.25   \nn0009793a3b91c27                                   1.0    0.50   \n\n                  target_nomi_20  target_nomi_60  target_jerome_20  \\\nid                                                                   \nn000777698096000            0.25            0.50              0.25   \nn0009793a3b91c27            0.50            0.75              0.50   \n\n                  target_jerome_60  ...  target_william_20  target_william_60  \\\nid                                  ...                                         \nn000777698096000              0.50  ...           0.166667           0.500000   \nn0009793a3b91c27              0.75  ...           0.666667           0.833333   \n\n                  target_arthur_20  target_arthur_60  target_thomas_20  \\\nid                                                                       \nn000777698096000          0.166667          0.500000          0.166667   \nn0009793a3b91c27          0.666667          0.833333          0.500000   \n\n                  target_thomas_60  prediction   era   data_type  \\\nid                                                                 \nn000777698096000               0.5    0.228263  0857  validation   \nn0009793a3b91c27               0.5    0.731198  0857  validation   \n\n                  prediction_random  \nid                                   \nn000777698096000           0.191519  \nn0009793a3b91c27           0.622109  \n\n[2 rows x 30 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_dichasial_hammier_spawner</th>\n      <th>feature_rheumy_epistemic_prancer</th>\n      <th>feature_pert_performative_hormuz</th>\n      <th>feature_hillier_unpitied_theobromine</th>\n      <th>feature_perigean_bewitching_thruster</th>\n      <th>target</th>\n      <th>target_nomi_20</th>\n      <th>target_nomi_60</th>\n      <th>target_jerome_20</th>\n      <th>target_jerome_60</th>\n      <th>...</th>\n      <th>target_william_20</th>\n      <th>target_william_60</th>\n      <th>target_arthur_20</th>\n      <th>target_arthur_60</th>\n      <th>target_thomas_20</th>\n      <th>target_thomas_60</th>\n      <th>prediction</th>\n      <th>era</th>\n      <th>data_type</th>\n      <th>prediction_random</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>n000777698096000</th>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>...</td>\n      <td>0.166667</td>\n      <td>0.500000</td>\n      <td>0.166667</td>\n      <td>0.500000</td>\n      <td>0.166667</td>\n      <td>0.5</td>\n      <td>0.228263</td>\n      <td>0857</td>\n      <td>validation</td>\n      <td>0.191519</td>\n    </tr>\n    <tr>\n      <th>n0009793a3b91c27</th>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>1.0</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>...</td>\n      <td>0.666667</td>\n      <td>0.833333</td>\n      <td>0.666667</td>\n      <td>0.833333</td>\n      <td>0.500000</td>\n      <td>0.5</td>\n      <td>0.731198</td>\n      <td>0857</td>\n      <td>validation</td>\n      <td>0.622109</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows  30 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 9;\n                var nbb_unformatted_code = \"test_dataf.head(2)\";\n                var nbb_formatted_code = \"test_dataf.head(2)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataf.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Full evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Evaluation:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90aa0d6f75ab4b53860ac0cd661cc455"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 14:45:14,042 INFO numexpr.utils: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-02-14 14:45:14,043 INFO numexpr.utils: NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/plain": " Neutralized \u001B[1;34m'prediction'\u001B[0m\u001B[1;34m with proportion \u001B[0m\u001B[1;34m'1.0'\u001B[0m\u001B[1;34m \u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Neutralized <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">'prediction'</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> with proportion </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">'1.0'</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> </span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "New neutralized column = \u001B[1;32m'prediction_neutralized_1.0'\u001B[0m.\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New neutralized column = <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'prediction_neutralized_1.0'</span>.\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": " Finished step \u001B[1mFeatureNeutralizer\u001B[0m. Output \u001B[33mshape\u001B[0m=\u001B[1m(\u001B[0m\u001B[1;36m539658\u001B[0m, \u001B[1;36m31\u001B[0m\u001B[1m)\u001B[0m. Time taken for step: \n\u001B[1;34m0:00:01\u001B[0m\u001B[34m.\u001B[0m\u001B[1;34m853230\u001B[0m. \n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Finished step <span style=\"font-weight: bold\">FeatureNeutralizer</span>. Output <span style=\"color: #808000; text-decoration-color: #808000\">shape</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">539658</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span><span style=\"font-weight: bold\">)</span>. Time taken for step: \n<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">0:00:01</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">853230</span>. \n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": " Neutralized \u001B[1;34m'prediction_random'\u001B[0m\u001B[1;34m with proportion \u001B[0m\u001B[1;34m'1.0'\u001B[0m\u001B[1;34m \u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Neutralized <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">'prediction_random'</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> with proportion </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">'1.0'</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> </span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "New neutralized column = \u001B[1;32m'prediction_random_neutralized_1.0'\u001B[0m.\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New neutralized column = <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'prediction_random_neutralized_1.0'</span>.\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": " Finished step \u001B[1mFeatureNeutralizer\u001B[0m. Output \u001B[33mshape\u001B[0m=\u001B[1m(\u001B[0m\u001B[1;36m539658\u001B[0m, \u001B[1;36m32\u001B[0m\u001B[1m)\u001B[0m. Time taken for step: \n\u001B[1;34m0:00:01\u001B[0m\u001B[34m.\u001B[0m\u001B[1;34m307832\u001B[0m. \n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Finished step <span style=\"font-weight: bold\">FeatureNeutralizer</span>. Output <span style=\"color: #808000; text-decoration-color: #808000\">shape</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">539658</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"font-weight: bold\">)</span>. Time taken for step: \n<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">0:00:01</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">307832</span>. \n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 10;\n                var nbb_unformatted_code = \"# slow\\nevaluator = NumeraiClassicEvaluator()\\nval_stats = evaluator.full_evaluation(dataf=test_dataf,\\n                                      target_col=\\\"target\\\",\\n                                      pred_cols=[\\\"prediction\\\", \\\"prediction_random\\\"],\\n                                      example_col=\\\"prediction_random\\\"\\n                                      )\";\n                var nbb_formatted_code = \"# slow\\nevaluator = NumeraiClassicEvaluator()\\nval_stats = evaluator.full_evaluation(\\n    dataf=test_dataf,\\n    target_col=\\\"target\\\",\\n    pred_cols=[\\\"prediction\\\", \\\"prediction_random\\\"],\\n    example_col=\\\"prediction_random\\\",\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# skip\n",
    "# slow\n",
    "evaluator = NumeraiClassicEvaluator()\n",
    "val_stats = evaluator.full_evaluation(dataf=test_dataf,\n",
    "                                      target_col=\"target\",\n",
    "                                      pred_cols=[\"prediction\", \"prediction_random\"],\n",
    "                                      example_col=\"prediction_random\"\n",
    "                                      )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "                   target      mean       std    sharpe  max_drawdown  \\\nprediction         target  0.025453  0.026586  0.957381     -0.082849   \nprediction_random  target  0.001232  0.013302  0.092622     -0.107464   \n\n                          apy  mmc_mean   mmc_std  mmc_sharpe  \\\nprediction         228.846183  0.019524  0.020418    0.956887   \nprediction_random    5.639523  0.000019  0.000115    0.094133   \n\n                   corr_with_example_preds  max_feature_exposure  \\\nprediction                       -0.000807              0.099629   \nprediction_random                 0.999930              0.021222   \n\n                   feature_neutral_mean  feature_neutral_std  \\\nprediction                     0.025627             0.026719   \nprediction_random              0.001239             0.013326   \n\n                   feature_neutral_sharpe  tb200_mean  tb200_std  \\\nprediction                       0.959144    0.045748   0.058146   \nprediction_random                0.092986   -0.007608   0.053580   \n\n                   tb200_sharpe  tb500_mean  tb500_std  tb500_sharpe  \nprediction             0.786766    0.041661   0.042485      0.980604  \nprediction_random     -0.141986   -0.004470   0.028103     -0.159062  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>sharpe</th>\n      <th>max_drawdown</th>\n      <th>apy</th>\n      <th>mmc_mean</th>\n      <th>mmc_std</th>\n      <th>mmc_sharpe</th>\n      <th>corr_with_example_preds</th>\n      <th>max_feature_exposure</th>\n      <th>feature_neutral_mean</th>\n      <th>feature_neutral_std</th>\n      <th>feature_neutral_sharpe</th>\n      <th>tb200_mean</th>\n      <th>tb200_std</th>\n      <th>tb200_sharpe</th>\n      <th>tb500_mean</th>\n      <th>tb500_std</th>\n      <th>tb500_sharpe</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>prediction</th>\n      <td>target</td>\n      <td>0.025453</td>\n      <td>0.026586</td>\n      <td>0.957381</td>\n      <td>-0.082849</td>\n      <td>228.846183</td>\n      <td>0.019524</td>\n      <td>0.020418</td>\n      <td>0.956887</td>\n      <td>-0.000807</td>\n      <td>0.099629</td>\n      <td>0.025627</td>\n      <td>0.026719</td>\n      <td>0.959144</td>\n      <td>0.045748</td>\n      <td>0.058146</td>\n      <td>0.786766</td>\n      <td>0.041661</td>\n      <td>0.042485</td>\n      <td>0.980604</td>\n    </tr>\n    <tr>\n      <th>prediction_random</th>\n      <td>target</td>\n      <td>0.001232</td>\n      <td>0.013302</td>\n      <td>0.092622</td>\n      <td>-0.107464</td>\n      <td>5.639523</td>\n      <td>0.000019</td>\n      <td>0.000115</td>\n      <td>0.094133</td>\n      <td>0.999930</td>\n      <td>0.021222</td>\n      <td>0.001239</td>\n      <td>0.013326</td>\n      <td>0.092986</td>\n      <td>-0.007608</td>\n      <td>0.053580</td>\n      <td>-0.141986</td>\n      <td>-0.004470</td>\n      <td>0.028103</td>\n      <td>-0.159062</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 11;\n                var nbb_unformatted_code = \"val_stats\";\n                var nbb_formatted_code = \"val_stats\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `Evaluator` returns a Pandas DataFrame containing metrics for each prediction column defined.\n",
    "Note that any column can be used as example prediction. For practical use cases we recommend using proper example predictions (provided by Numerai) instead of random predictions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fast evaluation\n",
    "\n",
    "`fast_mode` skips max. feature exposure, feature neutral mean, TB200 and TB500 calculations, which can take a while to compute on full Numerai datasets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "Evaluation:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3af9d670e064bd797a69c3b7ac77151"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 12;\n                var nbb_unformatted_code = \"evaluator = NumeraiClassicEvaluator(fast_mode=True)\\nval_stats_fast = evaluator.full_evaluation(dataf=test_dataf,\\n                                           target_col=\\\"target\\\",\\n                                           pred_cols=[\\\"prediction\\\", \\\"prediction_random\\\"],\\n                                           example_col=\\\"prediction_random\\\"\\n                                           )\";\n                var nbb_formatted_code = \"evaluator = NumeraiClassicEvaluator(fast_mode=True)\\nval_stats_fast = evaluator.full_evaluation(\\n    dataf=test_dataf,\\n    target_col=\\\"target\\\",\\n    pred_cols=[\\\"prediction\\\", \\\"prediction_random\\\"],\\n    example_col=\\\"prediction_random\\\",\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# skip\n",
    "evaluator = NumeraiClassicEvaluator(fast_mode=True)\n",
    "val_stats_fast = evaluator.full_evaluation(dataf=test_dataf,\n",
    "                                           target_col=\"target\",\n",
    "                                           pred_cols=[\"prediction\", \"prediction_random\"],\n",
    "                                           example_col=\"prediction_random\"\n",
    "                                           )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "                   target      mean       std    sharpe  max_drawdown  \\\nprediction         target  0.025453  0.026586  0.957381     -0.082849   \nprediction_random  target  0.001232  0.013302  0.092622     -0.107464   \n\n                          apy  mmc_mean   mmc_std  mmc_sharpe  \\\nprediction         228.846183  0.019524  0.020418    0.956887   \nprediction_random    5.639523  0.000019  0.000115    0.094133   \n\n                   corr_with_example_preds  \nprediction                       -0.000807  \nprediction_random                 0.999930  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>sharpe</th>\n      <th>max_drawdown</th>\n      <th>apy</th>\n      <th>mmc_mean</th>\n      <th>mmc_std</th>\n      <th>mmc_sharpe</th>\n      <th>corr_with_example_preds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>prediction</th>\n      <td>target</td>\n      <td>0.025453</td>\n      <td>0.026586</td>\n      <td>0.957381</td>\n      <td>-0.082849</td>\n      <td>228.846183</td>\n      <td>0.019524</td>\n      <td>0.020418</td>\n      <td>0.956887</td>\n      <td>-0.000807</td>\n    </tr>\n    <tr>\n      <th>prediction_random</th>\n      <td>target</td>\n      <td>0.001232</td>\n      <td>0.013302</td>\n      <td>0.092622</td>\n      <td>-0.107464</td>\n      <td>5.639523</td>\n      <td>0.000019</td>\n      <td>0.000115</td>\n      <td>0.094133</td>\n      <td>0.999930</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 13;\n                var nbb_unformatted_code = \"val_stats_fast\";\n                var nbb_formatted_code = \"val_stats_fast\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_stats_fast"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": " \u001B[31mDeleting directory for \u001B[0m\u001B[31m'NumeraiClassicDownloader\u001B[0m\u001B[32m'\u001B[0m \nPath: \u001B[32m'/Users/clepelaars/Desktop/crowdcent/repositories/numerai-blocks/nbs/eval_test_1234321'\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> <span style=\"color: #800000; text-decoration-color: #800000\">Deleting directory for </span><span style=\"color: #800000; text-decoration-color: #800000\">'NumeraiClassicDownloader</span><span style=\"color: #008000; text-decoration-color: #008000\">'</span> \nPath: <span style=\"color: #008000; text-decoration-color: #008000\">'/Users/clepelaars/Desktop/crowdcent/repositories/numerai-blocks/nbs/eval_test_1234321'</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 14;\n                var nbb_unformatted_code = \"# Clean up environment\\ndownloader.remove_base_directory()\";\n                var nbb_formatted_code = \"# Clean up environment\\ndownloader.remove_base_directory()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean up environment\n",
    "downloader.remove_base_directory()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "--------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_download.ipynb.\n",
      "Converted 02_numerframe.ipynb.\n",
      "Converted 03_preprocessing.ipynb.\n",
      "Converted 04_model.ipynb.\n",
      "Converted 05_postprocessing.ipynb.\n",
      "Converted 06_modelpipeline.ipynb.\n",
      "Converted 07_evaluation.ipynb.\n",
      "Converted 08_key.ipynb.\n",
      "Converted 09_submission.ipynb.\n",
      "Converted 10_staking.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 15;\n                var nbb_unformatted_code = \"# hide\\n# Run this cell to sync all changes with library\\nfrom nbdev.export import notebook2script\\n\\nnotebook2script()\";\n                var nbb_formatted_code = \"# hide\\n# Run this cell to sync all changes with library\\nfrom nbdev.export import notebook2script\\n\\nnotebook2script()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "# Run this cell to sync all changes with library\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 15;\n                var nbb_unformatted_code = \"# hide\\n# Run this cell to sync all changes with library\\nfrom nbdev.export import notebook2script\\n\\nnotebook2script()\";\n                var nbb_formatted_code = \"# hide\\n# Run this cell to sync all changes with library\\nfrom nbdev.export import notebook2script\\n\\nnotebook2script()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}