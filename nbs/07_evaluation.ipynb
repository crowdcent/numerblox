{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 1;\n                var nbb_unformatted_code = \"# hide\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\\n%load_ext lab_black\";\n                var nbb_formatted_code = \"# hide\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\\n%load_ext lab_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 2;\n                var nbb_unformatted_code = \"# default_exp evaluation\";\n                var nbb_formatted_code = \"# default_exp evaluation\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default_exp evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "> Compute evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This section provides evaluation schemes for both Numerai Classic and Signals.\n",
    "The `Evaluator` takes a `NumerFrame` as input and returns a Pandas DataFrame containing metrics for each given prediction column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 3;\n                var nbb_unformatted_code = \"# export\\nimport json\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom tqdm.auto import tqdm\\nfrom typing import Tuple, Union\\n\\nfrom numerblox.numerframe import NumerFrame, create_numerframe\\nfrom numerblox.postprocessing import FeatureNeutralizer\";\n                var nbb_formatted_code = \"# export\\nimport json\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom tqdm.auto import tqdm\\nfrom typing import Tuple, Union\\n\\nfrom numerblox.numerframe import NumerFrame, create_numerframe\\nfrom numerblox.postprocessing import FeatureNeutralizer\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Union\n",
    "\n",
    "from numerblox.numerframe import NumerFrame, create_numerframe\n",
    "from numerblox.postprocessing import FeatureNeutralizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BaseEvaluator` implements all the evaluation logic that is common for Numerai Classic and Signals. This includes:\n",
    "- Mean, Standard Deviation and Sharpe for era returns.\n",
    "- Max drawdown\n",
    "- Annual Percentage Yield (APY)\n",
    "- Mean, Standard deviation and Sharpe for [MMC (Meta Model Contribution)](https://docs.numer.ai/tournament/metamodel-contribution) returns.\n",
    "- Correlation with example predictions\n",
    "- Max [feature exposure](https://forum.numer.ai/t/model-diagnostics-feature-exposure/899)\n",
    "- [Feature Neutral Mean (FNC)](https://docs.numer.ai/tournament/feature-neutral-correlation), Standard deviation and Sharpe\n",
    "- [Exposure Dissimilarity](https://forum.numer.ai/t/true-contribution-details/5128/4)\n",
    "- Mean, Standard Deviation and Sharpe for TB200 (Buy top 200 stocks and sell bottom 200 stocks).\n",
    "- Mean, Standard Deviation and Sharpe for TB500 (Buy top 500 stocks and sell bottom 500 stocks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 4;\n                var nbb_unformatted_code = \"# export\\nclass BaseEvaluator:\\n    \\\"\\\"\\\"\\n    Evaluation functionality that is relevant for both\\n    Numerai Classic and Numerai Signals.\\n\\n    :param era_col: Column name pointing to eras. \\\\n\\n    Most commonly \\\"era\\\" for Numerai Classic and \\\"friday_date\\\" for Numerai Signals. \\\\n\\n    :param fast_mode: Will skip compute intensive metrics if set to True,\\n    namely max_exposure, feature neutral mean, TB200 and TB500.\\n    \\\"\\\"\\\"\\n    def __init__(self, era_col: str = \\\"era\\\", fast_mode=False):\\n        self.era_col = era_col\\n        self.fast_mode = fast_mode\\n\\n    def full_evaluation(\\n        self,\\n        dataf: NumerFrame,\\n        example_col: str,\\n        pred_cols: list = None,\\n        target_col: str = \\\"target\\\",\\n    ) -> pd.DataFrame:\\n        \\\"\\\"\\\"\\n        Perform evaluation for each prediction column in the NumerFrame\\n        against give target and example prediction column.\\n        \\\"\\\"\\\"\\n        val_stats = pd.DataFrame()\\n        dataf = dataf.fillna(0.5)\\n        pred_cols = dataf.prediction_cols if not pred_cols else pred_cols\\n        for col in tqdm(pred_cols, desc=\\\"Evaluation: \\\"):\\n            col_stats = self.evaluation_one_col(\\n                dataf=dataf,\\n                pred_col=col,\\n                target_col=target_col,\\n                example_col=example_col,\\n            )\\n            val_stats = pd.concat([val_stats, col_stats], axis=0)\\n        return val_stats\\n\\n    def evaluation_one_col(\\n        self,\\n        dataf: Union[pd.DataFrame, NumerFrame],\\n        pred_col: str,\\n        target_col: str,\\n        example_col: str,\\n    ):\\n        \\\"\\\"\\\"\\n        Perform evaluation for one prediction column\\n        against given target and example prediction column.\\n        \\\"\\\"\\\"\\n        col_stats = pd.DataFrame()\\n        # Compute stats\\n        val_corrs = self.per_era_corrs(\\n            dataf=dataf, pred_col=pred_col, target_col=target_col\\n        )\\n        mean, std, sharpe = self.mean_std_sharpe(era_corrs=val_corrs)\\n        max_drawdown = self.max_drawdown(era_corrs=val_corrs)\\n        apy = self.apy(era_corrs=val_corrs)\\n        example_corr = self.example_correlation(\\n            dataf=dataf, pred_col=pred_col, example_col=example_col\\n        )\\n        mmc_mean, mmc_std, mmc_sharpe = self.mmc(\\n            dataf=dataf,\\n            pred_col=pred_col,\\n            target_col=target_col,\\n            example_col=example_col,\\n        )\\n        ex_diss = self.exposure_dissimilarity(\\n            dataf=dataf, pred_col=pred_col, example_col=example_col\\n        )\\n\\n        col_stats.loc[pred_col, \\\"target\\\"] = target_col\\n        col_stats.loc[pred_col, \\\"mean\\\"] = mean\\n        col_stats.loc[pred_col, \\\"std\\\"] = std\\n        col_stats.loc[pred_col, \\\"sharpe\\\"] = sharpe\\n        col_stats.loc[pred_col, \\\"max_drawdown\\\"] = max_drawdown\\n        col_stats.loc[pred_col, \\\"apy\\\"] = apy\\n        col_stats.loc[pred_col, \\\"mmc_mean\\\"] = mmc_mean\\n        col_stats.loc[pred_col, \\\"mmc_std\\\"] = mmc_std\\n        col_stats.loc[pred_col, \\\"mmc_sharpe\\\"] = mmc_sharpe\\n        col_stats.loc[pred_col, \\\"corr_with_example_preds\\\"] = example_corr\\n        col_stats.loc[pred_col, \\\"exposure_dissimilarity\\\"] = ex_diss\\n\\n        # Compute intensive stats\\n        if not self.fast_mode:\\n            max_feature_exposure = self.max_feature_exposure(\\n                dataf=dataf, pred_col=pred_col\\n            )\\n            fn_mean, fn_std, fn_sharpe = self.feature_neutral_mean_std_sharpe(\\n                dataf=dataf, pred_col=pred_col, target_col=target_col\\n            )\\n            tb200_mean, tb200_std, tb200_sharpe = self.tbx_mean_std_sharpe(\\n                dataf=dataf, pred_col=pred_col, target_col=target_col, tb=200\\n            )\\n            tb500_mean, tb500_std, tb500_sharpe = self.tbx_mean_std_sharpe(\\n                dataf=dataf, pred_col=pred_col, target_col=target_col, tb=500\\n            )\\n\\n            col_stats.loc[pred_col, \\\"max_feature_exposure\\\"] = max_feature_exposure\\n            col_stats.loc[pred_col, \\\"feature_neutral_mean\\\"] = fn_mean\\n            col_stats.loc[pred_col, \\\"feature_neutral_std\\\"] = fn_std\\n            col_stats.loc[pred_col, \\\"feature_neutral_sharpe\\\"] = fn_sharpe\\n            col_stats.loc[pred_col, \\\"tb200_mean\\\"] = tb200_mean\\n            col_stats.loc[pred_col, \\\"tb200_std\\\"] = tb200_std\\n            col_stats.loc[pred_col, \\\"tb200_sharpe\\\"] = tb200_sharpe\\n            col_stats.loc[pred_col, \\\"tb500_mean\\\"] = tb500_mean\\n            col_stats.loc[pred_col, \\\"tb500_std\\\"] = tb500_std\\n            col_stats.loc[pred_col, \\\"tb500_sharpe\\\"] = tb500_sharpe\\n        return col_stats\\n\\n    def per_era_corrs(\\n        self, dataf: pd.DataFrame, pred_col: str, target_col: str\\n    ) -> pd.Series:\\n        \\\"\\\"\\\"Correlation between prediction and target for each era.\\\"\\\"\\\"\\n        return dataf.groupby(dataf[self.era_col]).apply(\\n            lambda d: self._normalize_uniform(d[pred_col].fillna(0.5)).corr(\\n                d[target_col]\\n            )\\n        )\\n\\n    def mean_std_sharpe(\\n        self, era_corrs: pd.Series\\n    ) -> Tuple[np.float64, np.float64, np.float64]:\\n        \\\"\\\"\\\"\\n        Average, standard deviation and Sharpe ratio for\\n        correlations per era.\\n        \\\"\\\"\\\"\\n        mean = pd.Series(era_corrs.mean()).item()\\n        std = pd.Series(era_corrs.std(ddof=0)).item()\\n        sharpe = mean / std\\n        return mean, std, sharpe\\n\\n    @staticmethod\\n    def max_drawdown(era_corrs: pd.Series) -> np.float64:\\n        \\\"\\\"\\\"Maximum drawdown per era.\\\"\\\"\\\"\\n        # Arbitrarily large window\\n        rolling_max = (\\n            (era_corrs + 1).cumprod().rolling(window=9000, min_periods=1).max()\\n        )\\n        daily_value = (era_corrs + 1).cumprod()\\n        max_drawdown = -((rolling_max - daily_value) / rolling_max).max()\\n        return max_drawdown\\n\\n    @staticmethod\\n    def apy(era_corrs: pd.Series, stake_compounding_lag: int = 4) -> np.float64:\\n        \\\"\\\"\\\"\\n        Annual percentage yield.\\n        :param era_corrs: Correlation scores by era\\n        :param stake_compounding_lag: Compounding lag for Numerai rounds (4 for Numerai Classic)\\n        \\\"\\\"\\\"\\n        payout_scores = era_corrs.clip(-0.25, 0.25)\\n        payout_daily_value = (payout_scores + 1).cumprod()\\n        apy = (\\n            ((payout_daily_value.dropna().iloc[-1]) ** (1 / len(payout_scores)))\\n            ** (\\n                52 - stake_compounding_lag\\n            )  # 52 weeks of compounding minus n for stake compounding lag\\n            - 1\\n        ) * 100\\n        return apy\\n\\n    def example_correlation(\\n        self, dataf: Union[pd.DataFrame, NumerFrame], pred_col: str, example_col: str\\n    ):\\n        \\\"\\\"\\\"Correlations with example predictions.\\\"\\\"\\\"\\n        return self.per_era_corrs(\\n            dataf=dataf,\\n            pred_col=pred_col,\\n            target_col=example_col,\\n        ).mean()\\n\\n    def max_feature_exposure(\\n        self, dataf: Union[pd.DataFrame, NumerFrame], pred_col: str\\n    ) -> np.float64:\\n        \\\"\\\"\\\"Maximum exposure over all features.\\\"\\\"\\\"\\n        max_per_era = dataf.groupby(self.era_col).apply(\\n            lambda d: d[dataf.feature_cols].corrwith(d[pred_col]).abs().max()\\n        )\\n        max_feature_exposure = max_per_era.mean(skipna=True)\\n        return max_feature_exposure\\n\\n    def feature_neutral_mean_std_sharpe(\\n        self, dataf: Union[pd.DataFrame, NumerFrame], pred_col: str, target_col: str, feature_names: list = None\\n    ) -> Tuple[np.float64, np.float64, np.float64]:\\n        \\\"\\\"\\\"\\n        Feature neutralized mean performance.\\n        More info: https://docs.numer.ai/tournament/feature-neutral-correlation\\n        \\\"\\\"\\\"\\n        fn = FeatureNeutralizer(pred_name=pred_col,\\n                                feature_names=feature_names,\\n                                proportion=1.0)\\n        neutralized_dataf = fn(dataf=dataf)\\n        neutral_corrs = self.per_era_corrs(\\n            dataf=neutralized_dataf,\\n            pred_col=f\\\"{pred_col}_neutralized_1.0\\\",\\n            target_col=target_col,\\n        )\\n        mean, std, sharpe = self.mean_std_sharpe(era_corrs=neutral_corrs)\\n        return mean, std, sharpe\\n\\n    def tbx_mean_std_sharpe(\\n        self, dataf: pd.DataFrame, pred_col: str, target_col: str, tb: int = 200\\n    ) -> Tuple[np.float64, np.float64, np.float64]:\\n        \\\"\\\"\\\"\\n        Calculate Mean, Standard deviation and Sharpe ratio\\n        when we focus on the x top and x bottom predictions.\\n        :param tb: How many of top and bottom predictions to focus on.\\n        TB200 and TB500 are the most common situations.\\n        \\\"\\\"\\\"\\n        tb_val_corrs = self._score_by_date(\\n            dataf=dataf, columns=[pred_col], target=target_col, tb=tb\\n        )\\n        return self.mean_std_sharpe(era_corrs=tb_val_corrs)\\n\\n    def mmc(\\n        self, dataf: pd.DataFrame, pred_col: str, target_col: str, example_col: str\\n    ) -> Tuple[np.float64, np.float64, np.float64]:\\n        \\\"\\\"\\\"\\n        MMC Mean, standard deviation and Sharpe ratio.\\n        More info: https://docs.numer.ai/tournament/metamodel-contribution\\n        \\\"\\\"\\\"\\n        mmc_scores = []\\n        corr_scores = []\\n        for _, x in dataf.groupby(self.era_col):\\n            series = self._neutralize_series(\\n                self._normalize_uniform(x[pred_col]), (x[example_col])\\n            )\\n            mmc_scores.append(np.cov(series, x[target_col])[0, 1] / (0.29 ** 2))\\n            corr_scores.append(self._normalize_uniform(x[pred_col]).corr(x[target_col]))\\n\\n        val_mmc_mean = np.mean(mmc_scores)\\n        val_mmc_std = np.std(mmc_scores)\\n        corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\\n        corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\\n        return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe\\n\\n    def exposure_dissimilarity(self, dataf: pd.DataFrame, pred_col: str, example_col: str) -> np.float32:\\n        \\\"\\\"\\\"\\n        Model pattern of feature exposure to the example column.\\n        See TC details forum post: https://forum.numer.ai/t/true-contribution-details/5128/4\\n\\n       1. Calculate the correlation of a user\\u2019s prediction and the example prediction with each of the features to form two vectors U and E.\\n       2. Take the dot product of U and E divided by the dot product of E with E. This measures how similar the pattern of exposures are and is normalized to be 1 if U is identical to E.\\n       3. Subtract from 1 to form a dissimilarity metric where 0 means the same exposure pattern as example predictions, positive values indicate differing patterns of exposure and negative values indicate similar patterns but even higher exposures. Note that models with 0 feature exposure will have a dissimilarity value of 1.\\n        Exposure Dissimilarity: 1 - U\\u2022E/E\\u2022E\\n        \\\"\\\"\\\"\\n        U = []\\n        E = []\\n        for feat in tqdm(dataf.feature_cols, desc=\\\"Exposure Dissimilarity correlations\\\"):\\n            corr_pred = dataf[feat].corr(dataf[pred_col], method='spearman')\\n            corr_example = dataf[feat].corr(dataf[example_col], method='spearman')\\n            U.append(corr_pred)\\n            E.append(corr_example)\\n        exp_dis = 1 - (U @ E) / (E @ E)\\n        assert 0. <= exp_dis <= 1., \\\"Check formula. Exposure dissimilarity should be between 0 and 1.\\\"\\n        return exp_dis\\n\\n\\n    @staticmethod\\n    def _neutralize_series(series, by, proportion=1.0):\\n        scores = series.values.reshape(-1, 1)\\n        exposures = by.values.reshape(-1, 1)\\n\\n        # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\\n        exposures = np.hstack(\\n            (exposures, np.array([np.mean(series)] * len(exposures)).reshape(-1, 1))\\n        )\\n\\n        correction = proportion * (\\n            exposures.dot(np.linalg.lstsq(exposures, scores, rcond=None)[0])\\n        )\\n        corrected_scores = scores - correction\\n        neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\\n        return neutralized\\n\\n    def _score_by_date(\\n        self, dataf: pd.DataFrame, columns: list, target: str, tb: int = None\\n    ):\\n        \\\"\\\"\\\"\\n        Get era correlation based on given TB (x top and bottom predictions).\\n        :param tb: How many of top and bottom predictions to focus on.\\n        TB200 is the most common situation.\\n        \\\"\\\"\\\"\\n        unique_eras = dataf[self.era_col].unique()\\n        computed = []\\n        for u in unique_eras:\\n            df_era = dataf[dataf[self.era_col] == u]\\n            era_pred = np.float64(df_era[columns].values.T)\\n            era_target = np.float64(df_era[target].values.T)\\n\\n            if tb is None:\\n                ccs = np.corrcoef(era_target, era_pred)[0, 1:]\\n            else:\\n                tbidx = np.argsort(era_pred, axis=1)\\n                tbidx = np.concatenate([tbidx[:, :tb], tbidx[:, -tb:]], axis=1)\\n                ccs = [\\n                    np.corrcoef(era_target[idx], pred[idx])[0, 1]\\n                    for idx, pred in zip(tbidx, era_pred)\\n                ]\\n                ccs = np.array(ccs)\\n            computed.append(ccs)\\n        return pd.DataFrame(\\n            np.array(computed), columns=columns, index=dataf[self.era_col].unique()\\n        )\\n\\n    @staticmethod\\n    def _normalize_uniform(df: pd.DataFrame) -> pd.Series:\\n        \\\"\\\"\\\"Normalize predictions uniformly using ranks.\\\"\\\"\\\"\\n        x = (df.rank(method=\\\"first\\\") - 0.5) / len(df)\\n        return pd.Series(x, index=df.index)\\n\\n    def plot_correlations(\\n        self,\\n        dataf: NumerFrame,\\n        pred_cols: list = None,\\n        target_col: str = \\\"target\\\",\\n        roll_mean: int = 20,\\n    ):\\n        \\\"\\\"\\\"\\n        Plot per era correlations over time.\\n        :param roll_mean: How many eras should be averaged to compute a rolling score.\\n        \\\"\\\"\\\"\\n        validation_by_eras = pd.DataFrame()\\n        pred_cols = dataf.prediction_cols if not pred_cols else pred_cols\\n        for pred_col in pred_cols:\\n            per_era_corrs = self.per_era_corrs(\\n                dataf, pred_col=pred_col, target_col=target_col\\n            )\\n            validation_by_eras.loc[:, pred_col] = per_era_corrs\\n\\n        validation_by_eras.rolling(roll_mean).mean().plot(\\n            kind=\\\"line\\\",\\n            marker=\\\"o\\\",\\n            ms=4,\\n            title=f\\\"Rolling Per Era Correlation Mean (rolling window size: {roll_mean})\\\",\\n            figsize=(15, 5),\\n        )\\n        plt.legend(\\n            loc=\\\"upper center\\\",\\n            bbox_to_anchor=(0.5, -0.05),\\n            fancybox=True,\\n            shadow=True,\\n            ncol=1,\\n        )\\n        plt.axhline(y=0.0, color=\\\"r\\\", linestyle=\\\"--\\\")\\n        plt.show()\\n\\n        validation_by_eras.cumsum().plot(\\n            title=\\\"Cumulative Sum of Era Correlations\\\", figsize=(15, 5)\\n        )\\n        plt.legend(\\n            loc=\\\"upper center\\\",\\n            bbox_to_anchor=(0.5, -0.05),\\n            fancybox=True,\\n            shadow=True,\\n            ncol=1,\\n        )\\n        plt.axhline(y=0.0, color=\\\"r\\\", linestyle=\\\"--\\\")\\n        plt.show()\\n        return None\";\n                var nbb_formatted_code = \"# export\\nclass BaseEvaluator:\\n    \\\"\\\"\\\"\\n    Evaluation functionality that is relevant for both\\n    Numerai Classic and Numerai Signals.\\n\\n    :param era_col: Column name pointing to eras. \\\\n\\n    Most commonly \\\"era\\\" for Numerai Classic and \\\"friday_date\\\" for Numerai Signals. \\\\n\\n    :param fast_mode: Will skip compute intensive metrics if set to True,\\n    namely max_exposure, feature neutral mean, TB200 and TB500.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, era_col: str = \\\"era\\\", fast_mode=False):\\n        self.era_col = era_col\\n        self.fast_mode = fast_mode\\n\\n    def full_evaluation(\\n        self,\\n        dataf: NumerFrame,\\n        example_col: str,\\n        pred_cols: list = None,\\n        target_col: str = \\\"target\\\",\\n    ) -> pd.DataFrame:\\n        \\\"\\\"\\\"\\n        Perform evaluation for each prediction column in the NumerFrame\\n        against give target and example prediction column.\\n        \\\"\\\"\\\"\\n        val_stats = pd.DataFrame()\\n        dataf = dataf.fillna(0.5)\\n        pred_cols = dataf.prediction_cols if not pred_cols else pred_cols\\n        for col in tqdm(pred_cols, desc=\\\"Evaluation: \\\"):\\n            col_stats = self.evaluation_one_col(\\n                dataf=dataf,\\n                pred_col=col,\\n                target_col=target_col,\\n                example_col=example_col,\\n            )\\n            val_stats = pd.concat([val_stats, col_stats], axis=0)\\n        return val_stats\\n\\n    def evaluation_one_col(\\n        self,\\n        dataf: Union[pd.DataFrame, NumerFrame],\\n        pred_col: str,\\n        target_col: str,\\n        example_col: str,\\n    ):\\n        \\\"\\\"\\\"\\n        Perform evaluation for one prediction column\\n        against given target and example prediction column.\\n        \\\"\\\"\\\"\\n        col_stats = pd.DataFrame()\\n        # Compute stats\\n        val_corrs = self.per_era_corrs(\\n            dataf=dataf, pred_col=pred_col, target_col=target_col\\n        )\\n        mean, std, sharpe = self.mean_std_sharpe(era_corrs=val_corrs)\\n        max_drawdown = self.max_drawdown(era_corrs=val_corrs)\\n        apy = self.apy(era_corrs=val_corrs)\\n        example_corr = self.example_correlation(\\n            dataf=dataf, pred_col=pred_col, example_col=example_col\\n        )\\n        mmc_mean, mmc_std, mmc_sharpe = self.mmc(\\n            dataf=dataf,\\n            pred_col=pred_col,\\n            target_col=target_col,\\n            example_col=example_col,\\n        )\\n        ex_diss = self.exposure_dissimilarity(\\n            dataf=dataf, pred_col=pred_col, example_col=example_col\\n        )\\n\\n        col_stats.loc[pred_col, \\\"target\\\"] = target_col\\n        col_stats.loc[pred_col, \\\"mean\\\"] = mean\\n        col_stats.loc[pred_col, \\\"std\\\"] = std\\n        col_stats.loc[pred_col, \\\"sharpe\\\"] = sharpe\\n        col_stats.loc[pred_col, \\\"max_drawdown\\\"] = max_drawdown\\n        col_stats.loc[pred_col, \\\"apy\\\"] = apy\\n        col_stats.loc[pred_col, \\\"mmc_mean\\\"] = mmc_mean\\n        col_stats.loc[pred_col, \\\"mmc_std\\\"] = mmc_std\\n        col_stats.loc[pred_col, \\\"mmc_sharpe\\\"] = mmc_sharpe\\n        col_stats.loc[pred_col, \\\"corr_with_example_preds\\\"] = example_corr\\n        col_stats.loc[pred_col, \\\"exposure_dissimilarity\\\"] = ex_diss\\n\\n        # Compute intensive stats\\n        if not self.fast_mode:\\n            max_feature_exposure = self.max_feature_exposure(\\n                dataf=dataf, pred_col=pred_col\\n            )\\n            fn_mean, fn_std, fn_sharpe = self.feature_neutral_mean_std_sharpe(\\n                dataf=dataf, pred_col=pred_col, target_col=target_col\\n            )\\n            tb200_mean, tb200_std, tb200_sharpe = self.tbx_mean_std_sharpe(\\n                dataf=dataf, pred_col=pred_col, target_col=target_col, tb=200\\n            )\\n            tb500_mean, tb500_std, tb500_sharpe = self.tbx_mean_std_sharpe(\\n                dataf=dataf, pred_col=pred_col, target_col=target_col, tb=500\\n            )\\n\\n            col_stats.loc[pred_col, \\\"max_feature_exposure\\\"] = max_feature_exposure\\n            col_stats.loc[pred_col, \\\"feature_neutral_mean\\\"] = fn_mean\\n            col_stats.loc[pred_col, \\\"feature_neutral_std\\\"] = fn_std\\n            col_stats.loc[pred_col, \\\"feature_neutral_sharpe\\\"] = fn_sharpe\\n            col_stats.loc[pred_col, \\\"tb200_mean\\\"] = tb200_mean\\n            col_stats.loc[pred_col, \\\"tb200_std\\\"] = tb200_std\\n            col_stats.loc[pred_col, \\\"tb200_sharpe\\\"] = tb200_sharpe\\n            col_stats.loc[pred_col, \\\"tb500_mean\\\"] = tb500_mean\\n            col_stats.loc[pred_col, \\\"tb500_std\\\"] = tb500_std\\n            col_stats.loc[pred_col, \\\"tb500_sharpe\\\"] = tb500_sharpe\\n        return col_stats\\n\\n    def per_era_corrs(\\n        self, dataf: pd.DataFrame, pred_col: str, target_col: str\\n    ) -> pd.Series:\\n        \\\"\\\"\\\"Correlation between prediction and target for each era.\\\"\\\"\\\"\\n        return dataf.groupby(dataf[self.era_col]).apply(\\n            lambda d: self._normalize_uniform(d[pred_col].fillna(0.5)).corr(\\n                d[target_col]\\n            )\\n        )\\n\\n    def mean_std_sharpe(\\n        self, era_corrs: pd.Series\\n    ) -> Tuple[np.float64, np.float64, np.float64]:\\n        \\\"\\\"\\\"\\n        Average, standard deviation and Sharpe ratio for\\n        correlations per era.\\n        \\\"\\\"\\\"\\n        mean = pd.Series(era_corrs.mean()).item()\\n        std = pd.Series(era_corrs.std(ddof=0)).item()\\n        sharpe = mean / std\\n        return mean, std, sharpe\\n\\n    @staticmethod\\n    def max_drawdown(era_corrs: pd.Series) -> np.float64:\\n        \\\"\\\"\\\"Maximum drawdown per era.\\\"\\\"\\\"\\n        # Arbitrarily large window\\n        rolling_max = (\\n            (era_corrs + 1).cumprod().rolling(window=9000, min_periods=1).max()\\n        )\\n        daily_value = (era_corrs + 1).cumprod()\\n        max_drawdown = -((rolling_max - daily_value) / rolling_max).max()\\n        return max_drawdown\\n\\n    @staticmethod\\n    def apy(era_corrs: pd.Series, stake_compounding_lag: int = 4) -> np.float64:\\n        \\\"\\\"\\\"\\n        Annual percentage yield.\\n        :param era_corrs: Correlation scores by era\\n        :param stake_compounding_lag: Compounding lag for Numerai rounds (4 for Numerai Classic)\\n        \\\"\\\"\\\"\\n        payout_scores = era_corrs.clip(-0.25, 0.25)\\n        payout_daily_value = (payout_scores + 1).cumprod()\\n        apy = (\\n            ((payout_daily_value.dropna().iloc[-1]) ** (1 / len(payout_scores)))\\n            ** (\\n                52 - stake_compounding_lag\\n            )  # 52 weeks of compounding minus n for stake compounding lag\\n            - 1\\n        ) * 100\\n        return apy\\n\\n    def example_correlation(\\n        self, dataf: Union[pd.DataFrame, NumerFrame], pred_col: str, example_col: str\\n    ):\\n        \\\"\\\"\\\"Correlations with example predictions.\\\"\\\"\\\"\\n        return self.per_era_corrs(\\n            dataf=dataf,\\n            pred_col=pred_col,\\n            target_col=example_col,\\n        ).mean()\\n\\n    def max_feature_exposure(\\n        self, dataf: Union[pd.DataFrame, NumerFrame], pred_col: str\\n    ) -> np.float64:\\n        \\\"\\\"\\\"Maximum exposure over all features.\\\"\\\"\\\"\\n        max_per_era = dataf.groupby(self.era_col).apply(\\n            lambda d: d[dataf.feature_cols].corrwith(d[pred_col]).abs().max()\\n        )\\n        max_feature_exposure = max_per_era.mean(skipna=True)\\n        return max_feature_exposure\\n\\n    def feature_neutral_mean_std_sharpe(\\n        self,\\n        dataf: Union[pd.DataFrame, NumerFrame],\\n        pred_col: str,\\n        target_col: str,\\n        feature_names: list = None,\\n    ) -> Tuple[np.float64, np.float64, np.float64]:\\n        \\\"\\\"\\\"\\n        Feature neutralized mean performance.\\n        More info: https://docs.numer.ai/tournament/feature-neutral-correlation\\n        \\\"\\\"\\\"\\n        fn = FeatureNeutralizer(\\n            pred_name=pred_col, feature_names=feature_names, proportion=1.0\\n        )\\n        neutralized_dataf = fn(dataf=dataf)\\n        neutral_corrs = self.per_era_corrs(\\n            dataf=neutralized_dataf,\\n            pred_col=f\\\"{pred_col}_neutralized_1.0\\\",\\n            target_col=target_col,\\n        )\\n        mean, std, sharpe = self.mean_std_sharpe(era_corrs=neutral_corrs)\\n        return mean, std, sharpe\\n\\n    def tbx_mean_std_sharpe(\\n        self, dataf: pd.DataFrame, pred_col: str, target_col: str, tb: int = 200\\n    ) -> Tuple[np.float64, np.float64, np.float64]:\\n        \\\"\\\"\\\"\\n        Calculate Mean, Standard deviation and Sharpe ratio\\n        when we focus on the x top and x bottom predictions.\\n        :param tb: How many of top and bottom predictions to focus on.\\n        TB200 and TB500 are the most common situations.\\n        \\\"\\\"\\\"\\n        tb_val_corrs = self._score_by_date(\\n            dataf=dataf, columns=[pred_col], target=target_col, tb=tb\\n        )\\n        return self.mean_std_sharpe(era_corrs=tb_val_corrs)\\n\\n    def mmc(\\n        self, dataf: pd.DataFrame, pred_col: str, target_col: str, example_col: str\\n    ) -> Tuple[np.float64, np.float64, np.float64]:\\n        \\\"\\\"\\\"\\n        MMC Mean, standard deviation and Sharpe ratio.\\n        More info: https://docs.numer.ai/tournament/metamodel-contribution\\n        \\\"\\\"\\\"\\n        mmc_scores = []\\n        corr_scores = []\\n        for _, x in dataf.groupby(self.era_col):\\n            series = self._neutralize_series(\\n                self._normalize_uniform(x[pred_col]), (x[example_col])\\n            )\\n            mmc_scores.append(np.cov(series, x[target_col])[0, 1] / (0.29 ** 2))\\n            corr_scores.append(self._normalize_uniform(x[pred_col]).corr(x[target_col]))\\n\\n        val_mmc_mean = np.mean(mmc_scores)\\n        val_mmc_std = np.std(mmc_scores)\\n        corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\\n        corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\\n        return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe\\n\\n    def exposure_dissimilarity(\\n        self, dataf: pd.DataFrame, pred_col: str, example_col: str\\n    ) -> np.float32:\\n        \\\"\\\"\\\"\\n         Model pattern of feature exposure to the example column.\\n         See TC details forum post: https://forum.numer.ai/t/true-contribution-details/5128/4\\n\\n        1. Calculate the correlation of a user\\u2019s prediction and the example prediction with each of the features to form two vectors U and E.\\n        2. Take the dot product of U and E divided by the dot product of E with E. This measures how similar the pattern of exposures are and is normalized to be 1 if U is identical to E.\\n        3. Subtract from 1 to form a dissimilarity metric where 0 means the same exposure pattern as example predictions, positive values indicate differing patterns of exposure and negative values indicate similar patterns but even higher exposures. Note that models with 0 feature exposure will have a dissimilarity value of 1.\\n         Exposure Dissimilarity: 1 - U\\u2022E/E\\u2022E\\n        \\\"\\\"\\\"\\n        U = []\\n        E = []\\n        for feat in tqdm(\\n            dataf.feature_cols, desc=\\\"Exposure Dissimilarity correlations\\\"\\n        ):\\n            corr_pred = dataf[feat].corr(dataf[pred_col], method=\\\"spearman\\\")\\n            corr_example = dataf[feat].corr(dataf[example_col], method=\\\"spearman\\\")\\n            U.append(corr_pred)\\n            E.append(corr_example)\\n        exp_dis = 1 - (U @ E) / (E @ E)\\n        assert (\\n            0.0 <= exp_dis <= 1.0\\n        ), \\\"Check formula. Exposure dissimilarity should be between 0 and 1.\\\"\\n        return exp_dis\\n\\n    @staticmethod\\n    def _neutralize_series(series, by, proportion=1.0):\\n        scores = series.values.reshape(-1, 1)\\n        exposures = by.values.reshape(-1, 1)\\n\\n        # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\\n        exposures = np.hstack(\\n            (exposures, np.array([np.mean(series)] * len(exposures)).reshape(-1, 1))\\n        )\\n\\n        correction = proportion * (\\n            exposures.dot(np.linalg.lstsq(exposures, scores, rcond=None)[0])\\n        )\\n        corrected_scores = scores - correction\\n        neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\\n        return neutralized\\n\\n    def _score_by_date(\\n        self, dataf: pd.DataFrame, columns: list, target: str, tb: int = None\\n    ):\\n        \\\"\\\"\\\"\\n        Get era correlation based on given TB (x top and bottom predictions).\\n        :param tb: How many of top and bottom predictions to focus on.\\n        TB200 is the most common situation.\\n        \\\"\\\"\\\"\\n        unique_eras = dataf[self.era_col].unique()\\n        computed = []\\n        for u in unique_eras:\\n            df_era = dataf[dataf[self.era_col] == u]\\n            era_pred = np.float64(df_era[columns].values.T)\\n            era_target = np.float64(df_era[target].values.T)\\n\\n            if tb is None:\\n                ccs = np.corrcoef(era_target, era_pred)[0, 1:]\\n            else:\\n                tbidx = np.argsort(era_pred, axis=1)\\n                tbidx = np.concatenate([tbidx[:, :tb], tbidx[:, -tb:]], axis=1)\\n                ccs = [\\n                    np.corrcoef(era_target[idx], pred[idx])[0, 1]\\n                    for idx, pred in zip(tbidx, era_pred)\\n                ]\\n                ccs = np.array(ccs)\\n            computed.append(ccs)\\n        return pd.DataFrame(\\n            np.array(computed), columns=columns, index=dataf[self.era_col].unique()\\n        )\\n\\n    @staticmethod\\n    def _normalize_uniform(df: pd.DataFrame) -> pd.Series:\\n        \\\"\\\"\\\"Normalize predictions uniformly using ranks.\\\"\\\"\\\"\\n        x = (df.rank(method=\\\"first\\\") - 0.5) / len(df)\\n        return pd.Series(x, index=df.index)\\n\\n    def plot_correlations(\\n        self,\\n        dataf: NumerFrame,\\n        pred_cols: list = None,\\n        target_col: str = \\\"target\\\",\\n        roll_mean: int = 20,\\n    ):\\n        \\\"\\\"\\\"\\n        Plot per era correlations over time.\\n        :param roll_mean: How many eras should be averaged to compute a rolling score.\\n        \\\"\\\"\\\"\\n        validation_by_eras = pd.DataFrame()\\n        pred_cols = dataf.prediction_cols if not pred_cols else pred_cols\\n        for pred_col in pred_cols:\\n            per_era_corrs = self.per_era_corrs(\\n                dataf, pred_col=pred_col, target_col=target_col\\n            )\\n            validation_by_eras.loc[:, pred_col] = per_era_corrs\\n\\n        validation_by_eras.rolling(roll_mean).mean().plot(\\n            kind=\\\"line\\\",\\n            marker=\\\"o\\\",\\n            ms=4,\\n            title=f\\\"Rolling Per Era Correlation Mean (rolling window size: {roll_mean})\\\",\\n            figsize=(15, 5),\\n        )\\n        plt.legend(\\n            loc=\\\"upper center\\\",\\n            bbox_to_anchor=(0.5, -0.05),\\n            fancybox=True,\\n            shadow=True,\\n            ncol=1,\\n        )\\n        plt.axhline(y=0.0, color=\\\"r\\\", linestyle=\\\"--\\\")\\n        plt.show()\\n\\n        validation_by_eras.cumsum().plot(\\n            title=\\\"Cumulative Sum of Era Correlations\\\", figsize=(15, 5)\\n        )\\n        plt.legend(\\n            loc=\\\"upper center\\\",\\n            bbox_to_anchor=(0.5, -0.05),\\n            fancybox=True,\\n            shadow=True,\\n            ncol=1,\\n        )\\n        plt.axhline(y=0.0, color=\\\"r\\\", linestyle=\\\"--\\\")\\n        plt.show()\\n        return None\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class BaseEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluation functionality that is relevant for both\n",
    "    Numerai Classic and Numerai Signals.\n",
    "\n",
    "    :param era_col: Column name pointing to eras. \\n\n",
    "    Most commonly \"era\" for Numerai Classic and \"friday_date\" for Numerai Signals. \\n\n",
    "    :param fast_mode: Will skip compute intensive metrics if set to True,\n",
    "    namely max_exposure, feature neutral mean, TB200 and TB500.\n",
    "    \"\"\"\n",
    "    def __init__(self, era_col: str = \"era\", fast_mode=False):\n",
    "        self.era_col = era_col\n",
    "        self.fast_mode = fast_mode\n",
    "\n",
    "    def full_evaluation(\n",
    "        self,\n",
    "        dataf: NumerFrame,\n",
    "        example_col: str,\n",
    "        pred_cols: list = None,\n",
    "        target_col: str = \"target\",\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform evaluation for each prediction column in the NumerFrame\n",
    "        against give target and example prediction column.\n",
    "        \"\"\"\n",
    "        val_stats = pd.DataFrame()\n",
    "        dataf = dataf.fillna(0.5)\n",
    "        pred_cols = dataf.prediction_cols if not pred_cols else pred_cols\n",
    "        for col in tqdm(pred_cols, desc=\"Evaluation: \"):\n",
    "            col_stats = self.evaluation_one_col(\n",
    "                dataf=dataf,\n",
    "                pred_col=col,\n",
    "                target_col=target_col,\n",
    "                example_col=example_col,\n",
    "            )\n",
    "            val_stats = pd.concat([val_stats, col_stats], axis=0)\n",
    "        return val_stats\n",
    "\n",
    "    def evaluation_one_col(\n",
    "        self,\n",
    "        dataf: Union[pd.DataFrame, NumerFrame],\n",
    "        pred_col: str,\n",
    "        target_col: str,\n",
    "        example_col: str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Perform evaluation for one prediction column\n",
    "        against given target and example prediction column.\n",
    "        \"\"\"\n",
    "        col_stats = pd.DataFrame()\n",
    "        # Compute stats\n",
    "        val_corrs = self.per_era_corrs(\n",
    "            dataf=dataf, pred_col=pred_col, target_col=target_col\n",
    "        )\n",
    "        mean, std, sharpe = self.mean_std_sharpe(era_corrs=val_corrs)\n",
    "        max_drawdown = self.max_drawdown(era_corrs=val_corrs)\n",
    "        apy = self.apy(era_corrs=val_corrs)\n",
    "        example_corr = self.example_correlation(\n",
    "            dataf=dataf, pred_col=pred_col, example_col=example_col\n",
    "        )\n",
    "        mmc_mean, mmc_std, mmc_sharpe = self.mmc(\n",
    "            dataf=dataf,\n",
    "            pred_col=pred_col,\n",
    "            target_col=target_col,\n",
    "            example_col=example_col,\n",
    "        )\n",
    "        ex_diss = self.exposure_dissimilarity(\n",
    "            dataf=dataf, pred_col=pred_col, example_col=example_col\n",
    "        )\n",
    "\n",
    "        col_stats.loc[pred_col, \"target\"] = target_col\n",
    "        col_stats.loc[pred_col, \"mean\"] = mean\n",
    "        col_stats.loc[pred_col, \"std\"] = std\n",
    "        col_stats.loc[pred_col, \"sharpe\"] = sharpe\n",
    "        col_stats.loc[pred_col, \"max_drawdown\"] = max_drawdown\n",
    "        col_stats.loc[pred_col, \"apy\"] = apy\n",
    "        col_stats.loc[pred_col, \"mmc_mean\"] = mmc_mean\n",
    "        col_stats.loc[pred_col, \"mmc_std\"] = mmc_std\n",
    "        col_stats.loc[pred_col, \"mmc_sharpe\"] = mmc_sharpe\n",
    "        col_stats.loc[pred_col, \"corr_with_example_preds\"] = example_corr\n",
    "        col_stats.loc[pred_col, \"exposure_dissimilarity\"] = ex_diss\n",
    "\n",
    "        # Compute intensive stats\n",
    "        if not self.fast_mode:\n",
    "            max_feature_exposure = self.max_feature_exposure(\n",
    "                dataf=dataf, pred_col=pred_col\n",
    "            )\n",
    "            fn_mean, fn_std, fn_sharpe = self.feature_neutral_mean_std_sharpe(\n",
    "                dataf=dataf, pred_col=pred_col, target_col=target_col\n",
    "            )\n",
    "            tb200_mean, tb200_std, tb200_sharpe = self.tbx_mean_std_sharpe(\n",
    "                dataf=dataf, pred_col=pred_col, target_col=target_col, tb=200\n",
    "            )\n",
    "            tb500_mean, tb500_std, tb500_sharpe = self.tbx_mean_std_sharpe(\n",
    "                dataf=dataf, pred_col=pred_col, target_col=target_col, tb=500\n",
    "            )\n",
    "\n",
    "            col_stats.loc[pred_col, \"max_feature_exposure\"] = max_feature_exposure\n",
    "            col_stats.loc[pred_col, \"feature_neutral_mean\"] = fn_mean\n",
    "            col_stats.loc[pred_col, \"feature_neutral_std\"] = fn_std\n",
    "            col_stats.loc[pred_col, \"feature_neutral_sharpe\"] = fn_sharpe\n",
    "            col_stats.loc[pred_col, \"tb200_mean\"] = tb200_mean\n",
    "            col_stats.loc[pred_col, \"tb200_std\"] = tb200_std\n",
    "            col_stats.loc[pred_col, \"tb200_sharpe\"] = tb200_sharpe\n",
    "            col_stats.loc[pred_col, \"tb500_mean\"] = tb500_mean\n",
    "            col_stats.loc[pred_col, \"tb500_std\"] = tb500_std\n",
    "            col_stats.loc[pred_col, \"tb500_sharpe\"] = tb500_sharpe\n",
    "        return col_stats\n",
    "\n",
    "    def per_era_corrs(\n",
    "        self, dataf: pd.DataFrame, pred_col: str, target_col: str\n",
    "    ) -> pd.Series:\n",
    "        \"\"\"Correlation between prediction and target for each era.\"\"\"\n",
    "        return dataf.groupby(dataf[self.era_col]).apply(\n",
    "            lambda d: self._normalize_uniform(d[pred_col].fillna(0.5)).corr(\n",
    "                d[target_col]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def mean_std_sharpe(\n",
    "        self, era_corrs: pd.Series\n",
    "    ) -> Tuple[np.float64, np.float64, np.float64]:\n",
    "        \"\"\"\n",
    "        Average, standard deviation and Sharpe ratio for\n",
    "        correlations per era.\n",
    "        \"\"\"\n",
    "        mean = pd.Series(era_corrs.mean()).item()\n",
    "        std = pd.Series(era_corrs.std(ddof=0)).item()\n",
    "        sharpe = mean / std\n",
    "        return mean, std, sharpe\n",
    "\n",
    "    @staticmethod\n",
    "    def max_drawdown(era_corrs: pd.Series) -> np.float64:\n",
    "        \"\"\"Maximum drawdown per era.\"\"\"\n",
    "        # Arbitrarily large window\n",
    "        rolling_max = (\n",
    "            (era_corrs + 1).cumprod().rolling(window=9000, min_periods=1).max()\n",
    "        )\n",
    "        daily_value = (era_corrs + 1).cumprod()\n",
    "        max_drawdown = -((rolling_max - daily_value) / rolling_max).max()\n",
    "        return max_drawdown\n",
    "\n",
    "    @staticmethod\n",
    "    def apy(era_corrs: pd.Series, stake_compounding_lag: int = 4) -> np.float64:\n",
    "        \"\"\"\n",
    "        Annual percentage yield.\n",
    "        :param era_corrs: Correlation scores by era\n",
    "        :param stake_compounding_lag: Compounding lag for Numerai rounds (4 for Numerai Classic)\n",
    "        \"\"\"\n",
    "        payout_scores = era_corrs.clip(-0.25, 0.25)\n",
    "        payout_daily_value = (payout_scores + 1).cumprod()\n",
    "        apy = (\n",
    "            ((payout_daily_value.dropna().iloc[-1]) ** (1 / len(payout_scores)))\n",
    "            ** (\n",
    "                52 - stake_compounding_lag\n",
    "            )  # 52 weeks of compounding minus n for stake compounding lag\n",
    "            - 1\n",
    "        ) * 100\n",
    "        return apy\n",
    "\n",
    "    def example_correlation(\n",
    "        self, dataf: Union[pd.DataFrame, NumerFrame], pred_col: str, example_col: str\n",
    "    ):\n",
    "        \"\"\"Correlations with example predictions.\"\"\"\n",
    "        return self.per_era_corrs(\n",
    "            dataf=dataf,\n",
    "            pred_col=pred_col,\n",
    "            target_col=example_col,\n",
    "        ).mean()\n",
    "\n",
    "    def max_feature_exposure(\n",
    "        self, dataf: Union[pd.DataFrame, NumerFrame], pred_col: str\n",
    "    ) -> np.float64:\n",
    "        \"\"\"Maximum exposure over all features.\"\"\"\n",
    "        max_per_era = dataf.groupby(self.era_col).apply(\n",
    "            lambda d: d[dataf.feature_cols].corrwith(d[pred_col]).abs().max()\n",
    "        )\n",
    "        max_feature_exposure = max_per_era.mean(skipna=True)\n",
    "        return max_feature_exposure\n",
    "\n",
    "    def feature_neutral_mean_std_sharpe(\n",
    "        self, dataf: Union[pd.DataFrame, NumerFrame], pred_col: str, target_col: str, feature_names: list = None\n",
    "    ) -> Tuple[np.float64, np.float64, np.float64]:\n",
    "        \"\"\"\n",
    "        Feature neutralized mean performance.\n",
    "        More info: https://docs.numer.ai/tournament/feature-neutral-correlation\n",
    "        \"\"\"\n",
    "        fn = FeatureNeutralizer(pred_name=pred_col,\n",
    "                                feature_names=feature_names,\n",
    "                                proportion=1.0)\n",
    "        neutralized_dataf = fn(dataf=dataf)\n",
    "        neutral_corrs = self.per_era_corrs(\n",
    "            dataf=neutralized_dataf,\n",
    "            pred_col=f\"{pred_col}_neutralized_1.0\",\n",
    "            target_col=target_col,\n",
    "        )\n",
    "        mean, std, sharpe = self.mean_std_sharpe(era_corrs=neutral_corrs)\n",
    "        return mean, std, sharpe\n",
    "\n",
    "    def tbx_mean_std_sharpe(\n",
    "        self, dataf: pd.DataFrame, pred_col: str, target_col: str, tb: int = 200\n",
    "    ) -> Tuple[np.float64, np.float64, np.float64]:\n",
    "        \"\"\"\n",
    "        Calculate Mean, Standard deviation and Sharpe ratio\n",
    "        when we focus on the x top and x bottom predictions.\n",
    "        :param tb: How many of top and bottom predictions to focus on.\n",
    "        TB200 and TB500 are the most common situations.\n",
    "        \"\"\"\n",
    "        tb_val_corrs = self._score_by_date(\n",
    "            dataf=dataf, columns=[pred_col], target=target_col, tb=tb\n",
    "        )\n",
    "        return self.mean_std_sharpe(era_corrs=tb_val_corrs)\n",
    "\n",
    "    def mmc(\n",
    "        self, dataf: pd.DataFrame, pred_col: str, target_col: str, example_col: str\n",
    "    ) -> Tuple[np.float64, np.float64, np.float64]:\n",
    "        \"\"\"\n",
    "        MMC Mean, standard deviation and Sharpe ratio.\n",
    "        More info: https://docs.numer.ai/tournament/metamodel-contribution\n",
    "        \"\"\"\n",
    "        mmc_scores = []\n",
    "        corr_scores = []\n",
    "        for _, x in dataf.groupby(self.era_col):\n",
    "            series = self._neutralize_series(\n",
    "                self._normalize_uniform(x[pred_col]), (x[example_col])\n",
    "            )\n",
    "            mmc_scores.append(np.cov(series, x[target_col])[0, 1] / (0.29 ** 2))\n",
    "            corr_scores.append(self._normalize_uniform(x[pred_col]).corr(x[target_col]))\n",
    "\n",
    "        val_mmc_mean = np.mean(mmc_scores)\n",
    "        val_mmc_std = np.std(mmc_scores)\n",
    "        corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
    "        corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
    "        return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe\n",
    "\n",
    "    def exposure_dissimilarity(self, dataf: pd.DataFrame, pred_col: str, example_col: str) -> np.float32:\n",
    "        \"\"\"\n",
    "        Model pattern of feature exposure to the example column.\n",
    "        See TC details forum post: https://forum.numer.ai/t/true-contribution-details/5128/4\n",
    "        \"\"\"\n",
    "        U = []\n",
    "        E = []\n",
    "        for feat in tqdm(dataf.feature_cols, desc=\"Exposure Dissimilarity correlations\"):\n",
    "            corr_pred = dataf[feat].corr(dataf[pred_col], method='spearman')\n",
    "            corr_example = dataf[feat].corr(dataf[example_col], method='spearman')\n",
    "            U.append(corr_pred)\n",
    "            E.append(corr_example)\n",
    "        exp_dis = 1 - (U @ E) / (E @ E)\n",
    "        assert 0. <= exp_dis <= 1., \"Check formula. Exposure dissimilarity should be between 0 and 1.\"\n",
    "        return exp_dis\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _neutralize_series(series, by, proportion=1.0):\n",
    "        scores = series.values.reshape(-1, 1)\n",
    "        exposures = by.values.reshape(-1, 1)\n",
    "\n",
    "        # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
    "        exposures = np.hstack(\n",
    "            (exposures, np.array([np.mean(series)] * len(exposures)).reshape(-1, 1))\n",
    "        )\n",
    "\n",
    "        correction = proportion * (\n",
    "            exposures.dot(np.linalg.lstsq(exposures, scores, rcond=None)[0])\n",
    "        )\n",
    "        corrected_scores = scores - correction\n",
    "        neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
    "        return neutralized\n",
    "\n",
    "    def _score_by_date(\n",
    "        self, dataf: pd.DataFrame, columns: list, target: str, tb: int = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Get era correlation based on given TB (x top and bottom predictions).\n",
    "        :param tb: How many of top and bottom predictions to focus on.\n",
    "        TB200 is the most common situation.\n",
    "        \"\"\"\n",
    "        unique_eras = dataf[self.era_col].unique()\n",
    "        computed = []\n",
    "        for u in unique_eras:\n",
    "            df_era = dataf[dataf[self.era_col] == u]\n",
    "            era_pred = np.float64(df_era[columns].values.T)\n",
    "            era_target = np.float64(df_era[target].values.T)\n",
    "\n",
    "            if tb is None:\n",
    "                ccs = np.corrcoef(era_target, era_pred)[0, 1:]\n",
    "            else:\n",
    "                tbidx = np.argsort(era_pred, axis=1)\n",
    "                tbidx = np.concatenate([tbidx[:, :tb], tbidx[:, -tb:]], axis=1)\n",
    "                ccs = [\n",
    "                    np.corrcoef(era_target[idx], pred[idx])[0, 1]\n",
    "                    for idx, pred in zip(tbidx, era_pred)\n",
    "                ]\n",
    "                ccs = np.array(ccs)\n",
    "            computed.append(ccs)\n",
    "        return pd.DataFrame(\n",
    "            np.array(computed), columns=columns, index=dataf[self.era_col].unique()\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_uniform(df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Normalize predictions uniformly using ranks.\"\"\"\n",
    "        x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
    "        return pd.Series(x, index=df.index)\n",
    "\n",
    "    def plot_correlations(\n",
    "        self,\n",
    "        dataf: NumerFrame,\n",
    "        pred_cols: list = None,\n",
    "        target_col: str = \"target\",\n",
    "        roll_mean: int = 20,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plot per era correlations over time.\n",
    "        :param roll_mean: How many eras should be averaged to compute a rolling score.\n",
    "        \"\"\"\n",
    "        validation_by_eras = pd.DataFrame()\n",
    "        pred_cols = dataf.prediction_cols if not pred_cols else pred_cols\n",
    "        for pred_col in pred_cols:\n",
    "            per_era_corrs = self.per_era_corrs(\n",
    "                dataf, pred_col=pred_col, target_col=target_col\n",
    "            )\n",
    "            validation_by_eras.loc[:, pred_col] = per_era_corrs\n",
    "\n",
    "        validation_by_eras.rolling(roll_mean).mean().plot(\n",
    "            kind=\"line\",\n",
    "            marker=\"o\",\n",
    "            ms=4,\n",
    "            title=f\"Rolling Per Era Correlation Mean (rolling window size: {roll_mean})\",\n",
    "            figsize=(15, 5),\n",
    "        )\n",
    "        plt.legend(\n",
    "            loc=\"upper center\",\n",
    "            bbox_to_anchor=(0.5, -0.05),\n",
    "            fancybox=True,\n",
    "            shadow=True,\n",
    "            ncol=1,\n",
    "        )\n",
    "        plt.axhline(y=0.0, color=\"r\", linestyle=\"--\")\n",
    "        plt.show()\n",
    "\n",
    "        validation_by_eras.cumsum().plot(\n",
    "            title=\"Cumulative Sum of Era Correlations\", figsize=(15, 5)\n",
    "        )\n",
    "        plt.legend(\n",
    "            loc=\"upper center\",\n",
    "            bbox_to_anchor=(0.5, -0.05),\n",
    "            fancybox=True,\n",
    "            shadow=True,\n",
    "            ncol=1,\n",
    "        )\n",
    "        plt.axhline(y=0.0, color=\"r\", linestyle=\"--\")\n",
    "        plt.show()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Numerai Classic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NumeraiClassicEvaluator` extends the base evaluation scheme with metrics specific to Numerai Classic.\n",
    "\n",
    "Additional metrics specific to Numerai are:\n",
    "\n",
    "- [Feature Neutral Mean v3 (FNCv3)](https://forum.numer.ai/t/true-contribution-details/5128/4), Standard deviation v3 and Sharpe v3.\n",
    "- FNCv3 x Exposure Dissimilarity (Interaction between the two metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 5;\n                var nbb_unformatted_code = \"# export\\nclass NumeraiClassicEvaluator(BaseEvaluator):\\n    \\\"\\\"\\\"Evaluator for all metrics that are relevant in Numerai Classic.\\\"\\\"\\\"\\n    def __init__(self, era_col: str = \\\"era\\\", fast_mode=False):\\n        super().__init__(era_col=era_col, fast_mode=fast_mode)\\n        # 420 features in medium feature set for FNC v3 calculation\\n        self.v3_features = self.__load_json(\\\"assets/feature_sets/v3_features.json\\\")['medium']\\n\\n    def full_evaluation(\\n        self,\\n        dataf: NumerFrame,\\n        example_col: str,\\n        pred_cols: list = None,\\n        target_col: str = \\\"target\\\",\\n    ) -> pd.DataFrame:\\n        val_stats = pd.DataFrame()\\n        dataf = dataf.fillna(0.5)\\n        pred_cols = dataf.prediction_cols if not pred_cols else pred_cols\\n\\n        # V3 feature check\\n        valid_v3_features = set(self.v3_features).issubset(set(dataf.columns))\\n        if not valid_v3_features:\\n            print(\\\"WARNING: Not all features needed for v3 metrics are defined in DataFrame. Skipping calculation of v3 metrics.\\\")\\n        for col in tqdm(pred_cols, desc=\\\"Evaluation: \\\"):\\n            # Metrics that can be calculated for both Numerai Classic and Signals\\n            col_stats = self.evaluation_one_col(\\n                dataf=dataf,\\n                pred_col=col,\\n                target_col=target_col,\\n                example_col=example_col,\\n            )\\n            # Numerai Classic specific metrics\\n            if not self.fast_mode and valid_v3_features:\\n                fnc_v3, fn_std_v3, fn_sharpe_v3 = self.feature_neutral_mean_std_sharpe(\\n                    dataf=dataf, pred_col=col, target_col=target_col, feature_names=self.v3_features\\n                )\\n                col_stats.loc[col, \\\"feature_neutral_mean_v3\\\"] = fnc_v3\\n                col_stats.loc[col, \\\"feature_neutral_std_v3\\\"] = fn_std_v3\\n                col_stats.loc[col, \\\"feature_neutral_sharpe_v3\\\"] = fn_sharpe_v3\\n            val_stats = pd.concat([val_stats, col_stats], axis=0)\\n        return val_stats\\n\\n    def __load_json(self, json_path: str) -> dict:\\n        with open(json_path, 'r') as f:\\n            data = json.load(f)\\n        return data\";\n                var nbb_formatted_code = \"# export\\nclass NumeraiClassicEvaluator(BaseEvaluator):\\n    \\\"\\\"\\\"Evaluator for all metrics that are relevant in Numerai Classic.\\\"\\\"\\\"\\n\\n    def __init__(self, era_col: str = \\\"era\\\", fast_mode=False):\\n        super().__init__(era_col=era_col, fast_mode=fast_mode)\\n        # 420 features in medium feature set for FNC v3 calculation\\n        self.v3_features = self.__load_json(\\\"assets/feature_sets/v3_features.json\\\")[\\n            \\\"medium\\\"\\n        ]\\n\\n    def full_evaluation(\\n        self,\\n        dataf: NumerFrame,\\n        example_col: str,\\n        pred_cols: list = None,\\n        target_col: str = \\\"target\\\",\\n    ) -> pd.DataFrame:\\n        val_stats = pd.DataFrame()\\n        dataf = dataf.fillna(0.5)\\n        pred_cols = dataf.prediction_cols if not pred_cols else pred_cols\\n\\n        # V3 feature check\\n        valid_v3_features = set(self.v3_features).issubset(set(dataf.columns))\\n        if not valid_v3_features:\\n            print(\\n                \\\"WARNING: Not all features needed for v3 metrics are defined in DataFrame. Skipping calculation of v3 metrics.\\\"\\n            )\\n        for col in tqdm(pred_cols, desc=\\\"Evaluation: \\\"):\\n            # Metrics that can be calculated for both Numerai Classic and Signals\\n            col_stats = self.evaluation_one_col(\\n                dataf=dataf,\\n                pred_col=col,\\n                target_col=target_col,\\n                example_col=example_col,\\n            )\\n            # Numerai Classic specific metrics\\n            if not self.fast_mode and valid_v3_features:\\n                fnc_v3, fn_std_v3, fn_sharpe_v3 = self.feature_neutral_mean_std_sharpe(\\n                    dataf=dataf,\\n                    pred_col=col,\\n                    target_col=target_col,\\n                    feature_names=self.v3_features,\\n                )\\n                col_stats.loc[col, \\\"feature_neutral_mean_v3\\\"] = fnc_v3\\n                col_stats.loc[col, \\\"feature_neutral_std_v3\\\"] = fn_std_v3\\n                col_stats.loc[col, \\\"feature_neutral_sharpe_v3\\\"] = fn_sharpe_v3\\n            val_stats = pd.concat([val_stats, col_stats], axis=0)\\n        return val_stats\\n\\n    def __load_json(self, json_path: str) -> dict:\\n        with open(json_path, \\\"r\\\") as f:\\n            data = json.load(f)\\n        return data\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class NumeraiClassicEvaluator(BaseEvaluator):\n",
    "    \"\"\"Evaluator for all metrics that are relevant in Numerai Classic.\"\"\"\n",
    "    def __init__(self, era_col: str = \"era\", fast_mode=False):\n",
    "        super().__init__(era_col=era_col, fast_mode=fast_mode)\n",
    "        # 420 features in medium feature set for FNC v3 calculation\n",
    "        self.v3_features = self.__load_json(\"assets/feature_sets/v3_features.json\")['medium']\n",
    "\n",
    "    def full_evaluation(\n",
    "        self,\n",
    "        dataf: NumerFrame,\n",
    "        example_col: str,\n",
    "        pred_cols: list = None,\n",
    "        target_col: str = \"target\",\n",
    "    ) -> pd.DataFrame:\n",
    "        val_stats = pd.DataFrame()\n",
    "        dataf = dataf.fillna(0.5)\n",
    "        pred_cols = dataf.prediction_cols if not pred_cols else pred_cols\n",
    "\n",
    "        # V3 feature check\n",
    "        valid_v3_features = set(self.v3_features).issubset(set(dataf.columns))\n",
    "        if not valid_v3_features:\n",
    "            print(\"WARNING: Not all features needed for v3 metrics are defined in DataFrame. Skipping calculation of v3 metrics.\")\n",
    "        for col in tqdm(pred_cols, desc=\"Evaluation: \"):\n",
    "            # Metrics that can be calculated for both Numerai Classic and Signals\n",
    "            col_stats = self.evaluation_one_col(\n",
    "                dataf=dataf,\n",
    "                pred_col=col,\n",
    "                target_col=target_col,\n",
    "                example_col=example_col,\n",
    "            )\n",
    "            # Numerai Classic specific metrics\n",
    "            if not self.fast_mode and valid_v3_features:\n",
    "                fnc_v3, fn_std_v3, fn_sharpe_v3 = self.feature_neutral_mean_std_sharpe(\n",
    "                    dataf=dataf, pred_col=col, target_col=target_col, feature_names=self.v3_features\n",
    "                )\n",
    "                col_stats.loc[col, \"feature_neutral_mean_v3\"] = fnc_v3\n",
    "                col_stats.loc[col, \"feature_neutral_std_v3\"] = fn_std_v3\n",
    "                col_stats.loc[col, \"feature_neutral_sharpe_v3\"] = fn_sharpe_v3\n",
    "            val_stats = pd.concat([val_stats, col_stats], axis=0)\n",
    "        return val_stats\n",
    "\n",
    "    def __load_json(self, json_path: str) -> dict:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Numerai Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NumeraiSignalsEvaluator` extends the base evaluation scheme with metrics specific to Numerai Signals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 6;\n                var nbb_unformatted_code = \"# export\\nclass NumeraiSignalsEvaluator(BaseEvaluator):\\n    \\\"\\\"\\\"Evaluator for all metrics that are relevant in Numerai Signals.\\\"\\\"\\\"\\n    def __init__(self, era_col: str = \\\"friday_date\\\", fast_mode=False):\\n        super().__init__(era_col=era_col, fast_mode=fast_mode)\";\n                var nbb_formatted_code = \"# export\\nclass NumeraiSignalsEvaluator(BaseEvaluator):\\n    \\\"\\\"\\\"Evaluator for all metrics that are relevant in Numerai Signals.\\\"\\\"\\\"\\n\\n    def __init__(self, era_col: str = \\\"friday_date\\\", fast_mode=False):\\n        super().__init__(era_col=era_col, fast_mode=fast_mode)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class NumeraiSignalsEvaluator(BaseEvaluator):\n",
    "    \"\"\"Evaluator for all metrics that are relevant in Numerai Signals.\"\"\"\n",
    "    def __init__(self, era_col: str = \"friday_date\", fast_mode=False):\n",
    "        super().__init__(era_col=era_col, fast_mode=fast_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test `NumeraiClassicEvaluator` on version 2 evaluation data with example predictions. The baseline reference (`example_col`) will be random predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "No existing directory found at \u001B[32m'\u001B[0m\u001B[34meval_test_1234321\u001B[0m\u001B[32m'\u001B[0m. Creating directory\u001B[33m...\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">No existing directory found at <span style=\"color: #008000; text-decoration-color: #008000\">'</span><span style=\"color: #000080; text-decoration-color: #000080\">eval_test_1234321</span><span style=\"color: #008000; text-decoration-color: #008000\">'</span>. Creating directory<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": " \u001B[32mDownloading\u001B[0m \u001B[32m'v3/example_validation_predictions.parquet'\u001B[0m \n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> <span style=\"color: #008000; text-decoration-color: #008000\">Downloading</span> <span style=\"color: #008000; text-decoration-color: #008000\">'v3/example_validation_predictions.parquet'</span> \n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 12:37:29,552 INFO numerapi.utils: starting download\n",
      "eval_test_1234321/example_validation_predictions.parquet: 13.0MB [00:02, 6.28MB/s]                            \n"
     ]
    },
    {
     "data": {
      "text/plain": " \u001B[32mDownloading\u001B[0m \u001B[32m'v3/numerai_validation_data.parquet'\u001B[0m \n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> <span style=\"color: #008000; text-decoration-color: #008000\">Downloading</span> <span style=\"color: #008000; text-decoration-color: #008000\">'v3/numerai_validation_data.parquet'</span> \n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 12:37:33,329 INFO numerapi.utils: starting download\n",
      "eval_test_1234321/numerai_validation_data.parquet: 228MB [00:46, 4.85MB/s]                            \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 7;\n                var nbb_unformatted_code = \"from numerblox.download import NumeraiClassicDownloader\\n\\ndirectory = \\\"eval_test_1234321/\\\"\\ndownloader = NumeraiClassicDownloader(directory_path=directory)\\ndownloader.download_single_dataset(filename=\\\"v3/example_validation_predictions.parquet\\\",\\n                                   dest_path=directory + \\\"example_validation_predictions.parquet\\\")\\ndownloader.download_single_dataset(filename=\\\"v3/numerai_validation_data.parquet\\\",\\n                                   dest_path=directory + \\\"numerai_validation_data.parquet\\\")\";\n                var nbb_formatted_code = \"from numerblox.download import NumeraiClassicDownloader\\n\\ndirectory = \\\"eval_test_1234321/\\\"\\ndownloader = NumeraiClassicDownloader(directory_path=directory)\\ndownloader.download_single_dataset(\\n    filename=\\\"v3/example_validation_predictions.parquet\\\",\\n    dest_path=directory + \\\"example_validation_predictions.parquet\\\",\\n)\\ndownloader.download_single_dataset(\\n    filename=\\\"v3/numerai_validation_data.parquet\\\",\\n    dest_path=directory + \\\"numerai_validation_data.parquet\\\",\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numerblox.download import NumeraiClassicDownloader\n",
    "\n",
    "directory = \"eval_test_1234321/\"\n",
    "downloader = NumeraiClassicDownloader(directory_path=directory)\n",
    "downloader.download_single_dataset(filename=\"v3/example_validation_predictions.parquet\",\n",
    "                                   dest_path=directory + \"example_validation_predictions.parquet\")\n",
    "downloader.download_single_dataset(filename=\"v3/numerai_validation_data.parquet\",\n",
    "                                   dest_path=directory + \"numerai_validation_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                   era   data_type  feature_dichasial_hammier_spawner  \\\nid                                                                      \nn000777698096000  0857  validation                               0.50   \nn0009793a3b91c27  0857  validation                               0.75   \n\n                  feature_rheumy_epistemic_prancer  \\\nid                                                   \nn000777698096000                              0.50   \nn0009793a3b91c27                              0.25   \n\n                  feature_pert_performative_hormuz  \\\nid                                                   \nn000777698096000                              0.25   \nn0009793a3b91c27                              0.50   \n\n                  feature_hillier_unpitied_theobromine  \\\nid                                                       \nn000777698096000                                  0.25   \nn0009793a3b91c27                                  0.75   \n\n                  feature_perigean_bewitching_thruster  \\\nid                                                       \nn000777698096000                                   0.0   \nn0009793a3b91c27                                   1.0   \n\n                  feature_renegade_undomestic_milord  \\\nid                                                     \nn000777698096000                                 0.0   \nn0009793a3b91c27                                 0.0   \n\n                  feature_koranic_rude_corf  \\\nid                                            \nn000777698096000                       0.00   \nn0009793a3b91c27                       0.25   \n\n                  feature_demisable_expiring_millepede  ...  target_george_20  \\\nid                                                      ...                     \nn000777698096000                                  0.75  ...              0.25   \nn0009793a3b91c27                                  0.25  ...              0.50   \n\n                  target_george_60  target_william_20  target_william_60  \\\nid                                                                         \nn000777698096000               0.5           0.166667           0.500000   \nn0009793a3b91c27               0.5           0.666667           0.833333   \n\n                  target_arthur_20  target_arthur_60  target_thomas_20  \\\nid                                                                       \nn000777698096000          0.166667          0.500000          0.166667   \nn0009793a3b91c27          0.666667          0.833333          0.500000   \n\n                  target_thomas_60  prediction  prediction_random  \nid                                                                 \nn000777698096000               0.5    0.228263           0.191519  \nn0009793a3b91c27               0.5    0.731198           0.622109  \n\n[2 rows x 1075 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>era</th>\n      <th>data_type</th>\n      <th>feature_dichasial_hammier_spawner</th>\n      <th>feature_rheumy_epistemic_prancer</th>\n      <th>feature_pert_performative_hormuz</th>\n      <th>feature_hillier_unpitied_theobromine</th>\n      <th>feature_perigean_bewitching_thruster</th>\n      <th>feature_renegade_undomestic_milord</th>\n      <th>feature_koranic_rude_corf</th>\n      <th>feature_demisable_expiring_millepede</th>\n      <th>...</th>\n      <th>target_george_20</th>\n      <th>target_george_60</th>\n      <th>target_william_20</th>\n      <th>target_william_60</th>\n      <th>target_arthur_20</th>\n      <th>target_arthur_60</th>\n      <th>target_thomas_20</th>\n      <th>target_thomas_60</th>\n      <th>prediction</th>\n      <th>prediction_random</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>n000777698096000</th>\n      <td>0857</td>\n      <td>validation</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.75</td>\n      <td>...</td>\n      <td>0.25</td>\n      <td>0.5</td>\n      <td>0.166667</td>\n      <td>0.500000</td>\n      <td>0.166667</td>\n      <td>0.500000</td>\n      <td>0.166667</td>\n      <td>0.5</td>\n      <td>0.228263</td>\n      <td>0.191519</td>\n    </tr>\n    <tr>\n      <th>n0009793a3b91c27</th>\n      <td>0857</td>\n      <td>validation</td>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>...</td>\n      <td>0.50</td>\n      <td>0.5</td>\n      <td>0.666667</td>\n      <td>0.833333</td>\n      <td>0.666667</td>\n      <td>0.833333</td>\n      <td>0.500000</td>\n      <td>0.5</td>\n      <td>0.731198</td>\n      <td>0.622109</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows  1075 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 8;\n                var nbb_unformatted_code = \"np.random.seed(1234)\\ntest_dataf = create_numerframe(directory + \\\"numerai_validation_data.parquet\\\")\\nexample_preds = pd.read_parquet(directory + \\\"example_validation_predictions.parquet\\\")\\ntest_dataf = test_dataf.merge(example_preds, on=\\\"id\\\", how=\\\"left\\\")\\n\\ntest_dataf.loc[:, \\\"prediction_random\\\"] = np.random.uniform(size=len(test_dataf))\\ntest_dataf.head(2)\";\n                var nbb_formatted_code = \"np.random.seed(1234)\\ntest_dataf = create_numerframe(directory + \\\"numerai_validation_data.parquet\\\")\\nexample_preds = pd.read_parquet(directory + \\\"example_validation_predictions.parquet\\\")\\ntest_dataf = test_dataf.merge(example_preds, on=\\\"id\\\", how=\\\"left\\\")\\n\\ntest_dataf.loc[:, \\\"prediction_random\\\"] = np.random.uniform(size=len(test_dataf))\\ntest_dataf.head(2)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "test_dataf = create_numerframe(directory + \"numerai_validation_data.parquet\")\n",
    "example_preds = pd.read_parquet(directory + \"example_validation_predictions.parquet\")\n",
    "test_dataf = test_dataf.merge(example_preds, on=\"id\", how=\"left\")\n",
    "\n",
    "test_dataf.loc[:, \"prediction_random\"] = np.random.uniform(size=len(test_dataf))\n",
    "test_dataf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                   era   data_type  feature_dichasial_hammier_spawner  \\\nid                                                                      \nn000777698096000  0857  validation                               0.50   \nn0009793a3b91c27  0857  validation                               0.75   \nn00099ccd6698ab0  0857  validation                               0.25   \nn0019e36bbb8702b  0857  validation                               0.50   \nn0028cb874439df8  0857  validation                               0.00   \n\n                  feature_rheumy_epistemic_prancer  \\\nid                                                   \nn000777698096000                              0.50   \nn0009793a3b91c27                              0.25   \nn00099ccd6698ab0                              0.75   \nn0019e36bbb8702b                              1.00   \nn0028cb874439df8                              0.75   \n\n                  feature_pert_performative_hormuz  \\\nid                                                   \nn000777698096000                              0.25   \nn0009793a3b91c27                              0.50   \nn00099ccd6698ab0                              0.00   \nn0019e36bbb8702b                              0.25   \nn0028cb874439df8                              0.00   \n\n                  feature_hillier_unpitied_theobromine  \\\nid                                                       \nn000777698096000                                  0.25   \nn0009793a3b91c27                                  0.75   \nn00099ccd6698ab0                                  0.75   \nn0019e36bbb8702b                                  0.75   \nn0028cb874439df8                                  0.25   \n\n                  feature_perigean_bewitching_thruster  \\\nid                                                       \nn000777698096000                                  0.00   \nn0009793a3b91c27                                  1.00   \nn00099ccd6698ab0                                  1.00   \nn0019e36bbb8702b                                  0.75   \nn0028cb874439df8                                  1.00   \n\n                  feature_renegade_undomestic_milord  \\\nid                                                     \nn000777698096000                                0.00   \nn0009793a3b91c27                                0.00   \nn00099ccd6698ab0                                0.75   \nn0019e36bbb8702b                                0.50   \nn0028cb874439df8                                0.50   \n\n                  feature_koranic_rude_corf  \\\nid                                            \nn000777698096000                       0.00   \nn0009793a3b91c27                       0.25   \nn00099ccd6698ab0                       1.00   \nn0019e36bbb8702b                       0.00   \nn0028cb874439df8                       0.00   \n\n                  feature_demisable_expiring_millepede  ...  target_george_20  \\\nid                                                      ...                     \nn000777698096000                                  0.75  ...              0.25   \nn0009793a3b91c27                                  0.25  ...              0.50   \nn00099ccd6698ab0                                  1.00  ...              0.25   \nn0019e36bbb8702b                                  0.75  ...              0.50   \nn0028cb874439df8                                  0.00  ...              0.50   \n\n                  target_george_60  target_william_20  target_william_60  \\\nid                                                                         \nn000777698096000              0.50           0.166667           0.500000   \nn0009793a3b91c27              0.50           0.666667           0.833333   \nn00099ccd6698ab0              0.25           0.166667           0.333333   \nn0019e36bbb8702b              0.50           0.666667           0.500000   \nn0028cb874439df8              0.75           0.333333           0.500000   \n\n                  target_arthur_20  target_arthur_60  target_thomas_20  \\\nid                                                                       \nn000777698096000          0.166667          0.500000          0.166667   \nn0009793a3b91c27          0.666667          0.833333          0.500000   \nn00099ccd6698ab0          0.166667          0.333333          0.166667   \nn0019e36bbb8702b          0.666667          0.500000          0.500000   \nn0028cb874439df8          0.333333          0.500000          0.500000   \n\n                  target_thomas_60  prediction  prediction_random  \nid                                                                 \nn000777698096000          0.500000    0.228263           0.191519  \nn0009793a3b91c27          0.500000    0.731198           0.622109  \nn00099ccd6698ab0          0.166667    0.846885           0.437728  \nn0019e36bbb8702b          0.500000    0.849709           0.785359  \nn0028cb874439df8          0.666667    0.175037           0.779976  \n\n[5 rows x 1075 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>era</th>\n      <th>data_type</th>\n      <th>feature_dichasial_hammier_spawner</th>\n      <th>feature_rheumy_epistemic_prancer</th>\n      <th>feature_pert_performative_hormuz</th>\n      <th>feature_hillier_unpitied_theobromine</th>\n      <th>feature_perigean_bewitching_thruster</th>\n      <th>feature_renegade_undomestic_milord</th>\n      <th>feature_koranic_rude_corf</th>\n      <th>feature_demisable_expiring_millepede</th>\n      <th>...</th>\n      <th>target_george_20</th>\n      <th>target_george_60</th>\n      <th>target_william_20</th>\n      <th>target_william_60</th>\n      <th>target_arthur_20</th>\n      <th>target_arthur_60</th>\n      <th>target_thomas_20</th>\n      <th>target_thomas_60</th>\n      <th>prediction</th>\n      <th>prediction_random</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>n000777698096000</th>\n      <td>0857</td>\n      <td>validation</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.75</td>\n      <td>...</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.166667</td>\n      <td>0.500000</td>\n      <td>0.166667</td>\n      <td>0.500000</td>\n      <td>0.166667</td>\n      <td>0.500000</td>\n      <td>0.228263</td>\n      <td>0.191519</td>\n    </tr>\n    <tr>\n      <th>n0009793a3b91c27</th>\n      <td>0857</td>\n      <td>validation</td>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>...</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.666667</td>\n      <td>0.833333</td>\n      <td>0.666667</td>\n      <td>0.833333</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.731198</td>\n      <td>0.622109</td>\n    </tr>\n    <tr>\n      <th>n00099ccd6698ab0</th>\n      <td>0857</td>\n      <td>validation</td>\n      <td>0.25</td>\n      <td>0.75</td>\n      <td>0.00</td>\n      <td>0.75</td>\n      <td>1.00</td>\n      <td>0.75</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.166667</td>\n      <td>0.333333</td>\n      <td>0.166667</td>\n      <td>0.333333</td>\n      <td>0.166667</td>\n      <td>0.166667</td>\n      <td>0.846885</td>\n      <td>0.437728</td>\n    </tr>\n    <tr>\n      <th>n0019e36bbb8702b</th>\n      <td>0857</td>\n      <td>validation</td>\n      <td>0.50</td>\n      <td>1.00</td>\n      <td>0.25</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>0.00</td>\n      <td>0.75</td>\n      <td>...</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.849709</td>\n      <td>0.785359</td>\n    </tr>\n    <tr>\n      <th>n0028cb874439df8</th>\n      <td>0857</td>\n      <td>validation</td>\n      <td>0.00</td>\n      <td>0.75</td>\n      <td>0.00</td>\n      <td>0.25</td>\n      <td>1.00</td>\n      <td>0.50</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.333333</td>\n      <td>0.500000</td>\n      <td>0.333333</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.666667</td>\n      <td>0.175037</td>\n      <td>0.779976</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows  1075 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 9;\n                var nbb_unformatted_code = \"test_dataf.head()\";\n                var nbb_formatted_code = \"test_dataf.head()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Evaluation:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06686b7efca64da7bba09480c0370a6c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 12:38:30,204 INFO numexpr.utils: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-03-29 12:38:30,205 INFO numexpr.utils: NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Exposure Dissimilarity correlations:   0%|          | 0/1050 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d16b57ca4e124bd190c1f8f24d35f2fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/d5/1qd2ftjn7ts_5_k6w2k2g4c00000gn/T/ipykernel_3472/2276270935.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mtarget_col\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"target\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0mpred_cols\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"prediction\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"prediction_random\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m     \u001B[0mexample_col\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"prediction_random\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m )\n\u001B[1;32m      9\u001B[0m \u001B[0mval_stats\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/d5/1qd2ftjn7ts_5_k6w2k2g4c00000gn/T/ipykernel_3472/1009285181.py\u001B[0m in \u001B[0;36mfull_evaluation\u001B[0;34m(self, dataf, example_col, pred_cols, target_col)\u001B[0m\n\u001B[1;32m     28\u001B[0m                 \u001B[0mpred_col\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m                 \u001B[0mtarget_col\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtarget_col\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m                 \u001B[0mexample_col\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mexample_col\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m             )\n\u001B[1;32m     32\u001B[0m             \u001B[0;31m# Numerai Classic specific metrics\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/d5/1qd2ftjn7ts_5_k6w2k2g4c00000gn/T/ipykernel_3472/981690484.py\u001B[0m in \u001B[0;36mevaluation_one_col\u001B[0;34m(self, dataf, pred_col, target_col, example_col)\u001B[0m\n\u001B[1;32m     67\u001B[0m         )\n\u001B[1;32m     68\u001B[0m         ex_diss = self.exposure_dissimilarity(\n\u001B[0;32m---> 69\u001B[0;31m             \u001B[0mdataf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdataf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpred_col\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpred_col\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexample_col\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mexample_col\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     70\u001B[0m         )\n\u001B[1;32m     71\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/d5/1qd2ftjn7ts_5_k6w2k2g4c00000gn/T/ipykernel_3472/981690484.py\u001B[0m in \u001B[0;36mexposure_dissimilarity\u001B[0;34m(self, dataf, pred_col, example_col)\u001B[0m\n\u001B[1;32m    248\u001B[0m         \u001B[0mE\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    249\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mfeat\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeature_cols\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdesc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"Exposure Dissimilarity correlations\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 250\u001B[0;31m             \u001B[0mcorr_pred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdataf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfeat\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcorr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpred_col\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'spearman'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    251\u001B[0m             \u001B[0mcorr_example\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdataf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfeat\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcorr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mexample_col\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'spearman'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    252\u001B[0m             \u001B[0mU\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcorr_pred\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/numerai-blocks37/lib/python3.7/site-packages/pandas/core/series.py\u001B[0m in \u001B[0;36mcorr\u001B[0;34m(self, other, method, min_periods)\u001B[0m\n\u001B[1;32m   2297\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmethod\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"pearson\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"spearman\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"kendall\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mcallable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2298\u001B[0m             return nanops.nancorr(\n\u001B[0;32m-> 2299\u001B[0;31m                 \u001B[0mthis\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mother\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmethod\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmin_periods\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmin_periods\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2300\u001B[0m             )\n\u001B[1;32m   2301\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/numerai-blocks37/lib/python3.7/site-packages/pandas/core/nanops.py\u001B[0m in \u001B[0;36m_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     69\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     70\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merrstate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minvalid\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"ignore\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 71\u001B[0;31m                     \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     72\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mValueError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m                 \u001B[0;31m# we want to transform an object array\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/numerai-blocks37/lib/python3.7/site-packages/pandas/core/nanops.py\u001B[0m in \u001B[0;36mnancorr\u001B[0;34m(a, b, method, min_periods)\u001B[0m\n\u001B[1;32m   1457\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1458\u001B[0m     \u001B[0mf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_corr_func\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1459\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1460\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1461\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/numerai-blocks37/lib/python3.7/site-packages/pandas/core/nanops.py\u001B[0m in \u001B[0;36mfunc\u001B[0;34m(a, b)\u001B[0m\n\u001B[1;32m   1472\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1473\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1474\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mspearmanr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1475\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1476\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/numerai-blocks37/lib/python3.7/site-packages/scipy/stats/stats.py\u001B[0m in \u001B[0;36mspearmanr\u001B[0;34m(a, b, axis, nan_policy, alternative)\u001B[0m\n\u001B[1;32m   4505\u001B[0m                 \u001B[0mvariable_has_nan\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misnan\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0many\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxisout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4506\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4507\u001B[0;31m     \u001B[0ma_ranked\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply_along_axis\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrankdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxisout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ma\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4508\u001B[0m     \u001B[0mrs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcorrcoef\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma_ranked\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrowvar\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxisout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4509\u001B[0m     \u001B[0mdof\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mn_obs\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m2\u001B[0m  \u001B[0;31m# degrees of freedom\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mapply_along_axis\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/numerai-blocks37/lib/python3.7/site-packages/numpy/lib/shape_base.py\u001B[0m in \u001B[0;36mapply_along_axis\u001B[0;34m(func1d, axis, arr, *args, **kwargs)\u001B[0m\n\u001B[1;32m    400\u001B[0m     \u001B[0mbuff\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mind0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mres\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    401\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mind\u001B[0m \u001B[0;32min\u001B[0m \u001B[0minds\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 402\u001B[0;31m         \u001B[0mbuff\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mind\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0masanyarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc1d\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minarr_view\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mind\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    403\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    404\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mres\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmatrix\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/numerai-blocks37/lib/python3.7/site-packages/scipy/stats/stats.py\u001B[0m in \u001B[0;36mrankdata\u001B[0;34m(a, method, axis)\u001B[0m\n\u001B[1;32m   8710\u001B[0m     \u001B[0marr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mravel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   8711\u001B[0m     \u001B[0malgo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'mergesort'\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mmethod\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'ordinal'\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m'quicksort'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 8712\u001B[0;31m     \u001B[0msorter\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margsort\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkind\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0malgo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   8713\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   8714\u001B[0m     \u001B[0minv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mempty\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msorter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mintp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<__array_function__ internals>\u001B[0m in \u001B[0;36margsort\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/numerai-blocks37/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001B[0m in \u001B[0;36margsort\u001B[0;34m(a, axis, kind, order)\u001B[0m\n\u001B[1;32m   1110\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1111\u001B[0m     \"\"\"\n\u001B[0;32m-> 1112\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_wrapfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'argsort'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkind\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkind\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1113\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1114\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/numerai-blocks37/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001B[0m in \u001B[0;36m_wrapfunc\u001B[0;34m(obj, method, *args, **kwds)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 58\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mbound\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     59\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m         \u001B[0;31m# A TypeError occurs if the object does have such a method in its\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 10;\n                var nbb_unformatted_code = \"# slow\\nevaluator = NumeraiClassicEvaluator()\\nval_stats = evaluator.full_evaluation(\\n    dataf=test_dataf,\\n    target_col=\\\"target\\\",\\n    pred_cols=[\\\"prediction\\\", \\\"prediction_random\\\"],\\n    example_col=\\\"prediction_random\\\",\\n)\\nval_stats\";\n                var nbb_formatted_code = \"# slow\\nevaluator = NumeraiClassicEvaluator()\\nval_stats = evaluator.full_evaluation(\\n    dataf=test_dataf,\\n    target_col=\\\"target\\\",\\n    pred_cols=[\\\"prediction\\\", \\\"prediction_random\\\"],\\n    example_col=\\\"prediction_random\\\",\\n)\\nval_stats\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "evaluator = NumeraiClassicEvaluator()\n",
    "val_stats = evaluator.full_evaluation(\n",
    "    dataf=test_dataf,\n",
    "    target_col=\"target\",\n",
    "    pred_cols=[\"prediction\", \"prediction_random\"],\n",
    "    example_col=\"prediction_random\",\n",
    ")\n",
    "val_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Evaluator` returns a Pandas DataFrame containing metrics for each prediction column defined.\n",
    "Note that any column can be used as example prediction. For practical use cases we recommend using proper example predictions (provided by Numerai) instead of random predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fast evaluation\n",
    "\n",
    "`fast_mode` skips max. feature exposure, feature neutral mean, FNCv3, TB200 and TB500 calculations, which can take a while to compute on full Numerai datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = NumeraiClassicEvaluator(fast_mode=True)\n",
    "val_stats_fast = evaluator.full_evaluation(\n",
    "    dataf=test_dataf,\n",
    "    target_col=\"target\",\n",
    "    pred_cols=[\"prediction\", \"prediction_random\"],\n",
    "    example_col=\"prediction_random\",\n",
    ")\n",
    "val_stats_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plot_correlations` method will use matplotlib to plot per era correlation scores over time. The plots default to a rolling window of 20 eras in order to best align with repuation scores as measured on the Numerai leaderboards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.plot_correlations(\n",
    "    test_dataf, pred_cols=[\"prediction\", \"prediction_random\"], roll_mean=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up environment\n",
    "downloader.remove_base_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# Run this cell to sync all changes with library\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}