{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from numerblox.misc import Key\n",
    "from numerblox.download import NumeraiClassicDownloader\n",
    "from numerblox.evaluation import NumeraiClassicEvaluator\n",
    "from numerblox.numerframe import create_numerframe\n",
    "from numerblox.prediction_loaders import ExamplePredictions\n",
    "from numerblox.submission import NumeraiClassicSubmitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NumeraiClassicDownloader` allows you to download training and inference data with a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "downloader = NumeraiClassicDownloader(\"data\")\n",
    "# Training and validation data\n",
    "downloader.download_training_data(\"train_val\", version=\"5.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a custom Pandas DataFrame data structure called `NumerFrame` with `create_numerframe` here to easily parse the Numerai data. The usage of `NumerFrame` is completely optional, but greatly simplify the building of Numerai pipelines and experimentation with Numerai data.\n",
    "\n",
    "We then fit a simple XGBoost regressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_numerframe(\"data/train_val/train.parquet\")\n",
    "X, y = df.sample(100).get_feature_target_pair(multi_target=False)\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X.values, y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NumeraiClassicEvaluator` will calculate all relevant Numerai metrics. \n",
    "\n",
    "`ExamplePredictions` is a NumerBlox class that handles downloading of example predictions for you. This object like all other NumerBlox processors can also used end to end in a scikit-learn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = create_numerframe(\"data/train_val/validation.parquet\")[:100]\n",
    "val_df[\"prediction\"] = xgb.predict(val_df.get_feature_data)\n",
    "val_df[\"example_preds\"] = ExamplePredictions(\"v5.0/validation_example_preds.parquet\").fit_transform(None)[\"prediction\"].values[:100]\n",
    "evaluator = NumeraiClassicEvaluator()\n",
    "metrics = evaluator.full_evaluation(val_df, example_col=\"example_preds\", pred_cols=[\"prediction\"], target_col=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again `NumeraiClassicDownloader` and `NumerFrame` are leveraged to simplify inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader.download_inference_data(\"current_round\", version=\"5.0\")\n",
    "live_df = create_numerframe(file_path=\"data/current_round/live.parquet\")\n",
    "live_X, live_y = live_df.get_feature_target_pair(multi_target=False)\n",
    "preds = xgb.predict(live_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NumeraiClassicSubmitter` takes care of data integrity checks and submission to Numerai for you. Credentials are conveniently initialized with a `Key` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit\n",
    "NUMERAI_PUBLIC_ID = \"YOUR_PUBLIC_ID\"\n",
    "NUMERAI_SECRET_KEY = \"YOUR_SECRET_KEY\"\n",
    "key = Key(pub_id=NUMERAI_PUBLIC_ID, secret_key=NUMERAI_SECRET_KEY)\n",
    "submitter = NumeraiClassicSubmitter(directory_path=\"sub_current_round\", key=key)\n",
    "# Your prediction file with 'id' as index and defined 'cols' below.\n",
    "pred_dataf = pd.DataFrame(preds, index=live_df.index, columns=[\"prediction\"])\n",
    "# Only works with valid key credentials and model_name\n",
    "# submitter.full_submission(dataf=pred_dataf,\n",
    "#                           cols=\"prediction\",\n",
    "#                           file_name=\"submission.csv\",\n",
    "#                           model_name=\"MY_MODEL_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Clean up environment (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All downloader and submitter have functionality to remove themselver. This is especially convenient if you are running a daily inference pipeline on your server or a cloud VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader.remove_base_directory()\n",
    "submitter.remove_base_directory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classic_prod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
